{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "43bdced2-2f27-46db-ae08-4f85ff96215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_namespace_from_service_account():\n",
    "    \"\"\"\n",
    "    Reads the Kubernetes namespace from the service account mount point.\n",
    "    Returns 'default' if not running in a Kubernetes pod or if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    namespace_file = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n",
    "    try:\n",
    "        with open(namespace_file, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except IOError:\n",
    "        return 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ebab2171-4f8b-45c7-a0fb-088fed7a8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token successfully refreshed.\n"
     ]
    }
   ],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35431717-376b-4c1c-ae09-7c484202ddcb",
   "metadata": {},
   "source": [
    "## Agent Configuration  \n",
    "\n",
    "This section covers the configuration of the agent, including:  \n",
    "* Defining the data context that the agent will interact with  \n",
    "* Setting up the routine the agent will follow as a system prompt (embedding the data context)  \n",
    "* Establishing the list of tools available for the agent to complete its tasks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8d3d4d36-171e-4136-b4d3-e3a42be61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent Configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "import json\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import kserve\n",
    "from kserve import ModelServer\n",
    "from kserve.model import Model\n",
    "from kserve.utils.utils import get_predict_input, get_predict_response\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client import ApiClient\n",
    "import yaml\n",
    "from kserve import KServeClient\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import utils\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1ModelSpec\n",
    "from kserve import V1beta1ModelFormat\n",
    "from kubernetes.client import V1ResourceRequirements\n",
    "from kubernetes.client.models import V1ObjectMeta  \n",
    "import hashlib\n",
    "import re\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a484676f-41fa-4889-8ecd-91d5cefb80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Kubernetes and Auth Setup (Same as other agents)\n",
    "def get_namespace_from_service_account():\n",
    "    namespace_file = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n",
    "    try:\n",
    "        with open(namespace_file, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except IOError:\n",
    "        return 'default'\n",
    "\n",
    "def get_nvidia_auth_token():\n",
    "    token_path = Path(\"/etc/secrets/ezua/.auth_token\")\n",
    "    if token_path.exists():\n",
    "        with open(token_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    raise ValueError(\"NVIDIA auth token not found\")\n",
    "\n",
    "NAMESPACE = get_namespace_from_service_account()\n",
    "nvidia_api_key = get_nvidia_auth_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "23760081-2dd4-42a9-9f64-0f041f418f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET https://llama-3-1-8b-6efc4543-predictor-ezai-services.hpepcai-ingress.pcai.hpecic.net/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# 2. LLM Configuration (Same as other agents)\n",
    "llm = NVIDIA(\n",
    "    base_url=\"https://llama-3-1-8b-6efc4543-predictor-ezai-services.hpepcai-ingress.pcai.hpecic.net/v1\",\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    api_key=nvidia_api_key,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5f00183c-5347-44b7-8239-5567f2544cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PostgreSQL Connection (Same as other agents)\n",
    "def get_db_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=f\"vince-retail-postgres.{NAMESPACE}.svc.cluster.local\",\n",
    "        database=\"vince-retail\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\",\n",
    "        port=\"5432\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "00d487c8-16ea-429a-950e-0e3124c4582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enhanced MLflow Setup with Error Handling\n",
    "def setup_mlflow():\n",
    "    mlflow_uri = \"http://mlflow.mlflow.svc.cluster.local:5000\"\n",
    "    s3_uri = 'http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
    "    experiment_name = 'vince-retail'\n",
    "    \n",
    "    mlflow.set_tracking_uri(mlflow_uri)\n",
    "    \n",
    "    # Test connection with retries\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        # Create experiment if not exists\n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            if experiment is None:\n",
    "                print(\"Creating new MLflow experiment...\")\n",
    "                mlflow.create_experiment(experiment_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not verify experiment - {str(e)}\")\n",
    "        \n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0b07341c-0ca6-42eb-bbda-7b802c0b48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. System Prompt\n",
    "DATA_SCIENTIST_SYSTEM_PROMPT = f\"\"\"\n",
    "You are a senior data scientist working with retail data. Your responsibilities include:\n",
    "\n",
    "1. Predictive Modeling:\n",
    "   - Analyze business requirements and identify prediction opportunities\n",
    "   - Select appropriate algorithms based on data characteristics\n",
    "   - Train, evaluate, and deploy models using MLflow\n",
    "   - Serve models via KServe for production use\n",
    "\n",
    "2. Data Preparation:\n",
    "   - Work with Data Engineer agent to ensure clean data\n",
    "   - Feature engineering for predictive tasks\n",
    "   - Handle temporal splits for time-series data\n",
    "\n",
    "3. Model Operations:\n",
    "   - Version all models with MLflow\n",
    "   - Monitor model performance\n",
    "   - Retrain models when data drift detected\n",
    "\n",
    "Available Prediction Tasks:\n",
    "- Demand forecasting\n",
    "- Customer lifetime value\n",
    "- Churn prediction\n",
    "- Recommendation systems\n",
    "- Price optimization\n",
    "\n",
    "Always:\n",
    "- Explain your modeling approach\n",
    "- Validate data suitability\n",
    "- Track all experiments\n",
    "- Document feature importance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "582e21a2-b6b0-4466-903a-542f076739c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Core Prediction Tools\n",
    "class PredictiveTools:\n",
    "    def __init__(self):\n",
    "        self.conn = get_db_connection()\n",
    "        try:\n",
    "            setup_mlflow()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: MLflow not available - {str(e)}\")\n",
    "            self.mlflow_enabled = False\n",
    "        else:\n",
    "            self.mlflow_enabled = True\n",
    "\n",
    "    def get_data_for_prediction(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute SQL query and return as DataFrame\"\"\"\n",
    "        with self.conn.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    def _log_to_mlflow(self, model, metrics, artifact_path):\n",
    "        \"\"\"Safe logging with MLflow availability check\"\"\"\n",
    "        if not self.mlflow_enabled:\n",
    "            print(\"MLflow not available - skipping logging\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            with mlflow.start_run():\n",
    "                for name, value in metrics.items():\n",
    "                    mlflow.log_metric(name, value)\n",
    "                model_info = mlflow.sklearn.log_model(model, artifact_path)\n",
    "                return model_info.model_uri\n",
    "        except Exception as e:\n",
    "            print(f\"MLflow logging failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def train_demand_forecast_model(self, product_id: str = None):\n",
    "        \"\"\"Train demand forecast model using correct schema\"\"\"\n",
    "        # Build query with proper column names\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            o.order_date as date,\n",
    "            op.product_id,\n",
    "            op.product_quantity as quantity,\n",
    "            c.price_cents/100.0 as price  -- Convert cents to dollars\n",
    "        FROM source_order_products op\n",
    "        JOIN source_orders o ON op.order_id = o.order_id\n",
    "        JOIN source_catalog c ON op.product_id = c.product_id\n",
    "        \"\"\"\n",
    "        \n",
    "        if product_id:\n",
    "            query += f\" WHERE op.product_id = '{product_id}'\"\n",
    "        \n",
    "        try:\n",
    "            df = self.get_data_for_prediction(query)\n",
    "            \n",
    "            if df.empty:\n",
    "                return {\"status\": \"error\", \"message\": \"No data found for training\"}\n",
    "            \n",
    "            # Feature engineering\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df.set_index('date').sort_index()\n",
    "            df['day_of_week'] = df.index.dayofweek\n",
    "            df['month'] = df.index.month\n",
    "            df['year'] = df.index.year\n",
    "            \n",
    "            # Model training\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            from sklearn.metrics import mean_absolute_error\n",
    "            \n",
    "            X = df[['day_of_week', 'month', 'year', 'price']]\n",
    "            y = df['quantity']\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "            \n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluation\n",
    "            predictions = model.predict(X_test)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            \n",
    "            # MLflow logging\n",
    "            model_uri = self._log_to_mlflow(\n",
    "                model=model,\n",
    "                metrics={\"mae\": mae},\n",
    "                artifact_path=\"demand-forecast-model\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"metrics\": {\"mae\": mae},\n",
    "                \"model_uri\": model_uri,\n",
    "                \"features_used\": list(X.columns),\n",
    "                \"training_samples\": len(X_train)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "    def get_production_model_uri(self, model_name: str) -> str:\n",
    "        \"\"\"Get the proper S3 URI for a production model\"\"\"\n",
    "        try:\n",
    "            client = MlflowClient()\n",
    "            \n",
    "            # 1. Get production model\n",
    "            latest_versions = client.get_latest_versions(\n",
    "                name=model_name, \n",
    "                stages=[\"Production\"]\n",
    "            )\n",
    "            \n",
    "            if not latest_versions:\n",
    "                raise ValueError(f\"No production model found for {model_name}\")\n",
    "            \n",
    "            # 2. Convert to proper S3 URI\n",
    "            source_uri = latest_versions[0].source\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to get model URI: {str(e)}\")\n",
    "\n",
    "    def deploy_model_to_kserve(self, model_name: str) -> Dict:\n",
    "        \"\"\"Deploy with proper S3 URI handling\"\"\"\n",
    "        try:\n",
    "            # 1. Get production model URI\n",
    "            model_uri = self.get_production_model_uri(model_name)\n",
    "            print(f\"Resolved model URI: {model_uri}\")\n",
    "            \n",
    "            # 2. Generate compliant name\n",
    "            deploy_name = self._generate_compliant_name(model_name)\n",
    "            \n",
    "            # 3. Create KServe client and specs\n",
    "            kserve_client = KServeClient()\n",
    "            model_spec = V1beta1ModelSpec(\n",
    "                model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "                storage_uri=model_uri,\n",
    "                runtime=\"kserve-mlserver\",\n",
    "                resources=V1ResourceRequirements(\n",
    "                    requests={\"cpu\": \"1\", \"memory\": \"2Gi\"},\n",
    "                    limits={\"cpu\": \"1\", \"memory\": \"2Gi\"}\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # 4. Deploy\n",
    "            isvc = V1beta1InferenceService(\n",
    "                api_version=\"serving.kserve.io/v1beta1\",\n",
    "                kind=\"InferenceService\",\n",
    "                metadata=V1ObjectMeta(name=deploy_name, namespace=NAMESPACE),\n",
    "                spec=V1beta1InferenceServiceSpec(\n",
    "                    predictor=V1beta1PredictorSpec(model=model_spec)\n",
    "                )\n",
    "            )\n",
    "            kserve_client.create(isvc)\n",
    "            \n",
    "            # 5. Wait for deployment\n",
    "            kserve_client.wait_isvc_ready(deploy_name, namespace=NAMESPACE, timeout_seconds=300)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"deployed\",\n",
    "                \"deployment_name\": deploy_name,\n",
    "                \"model_name\": model_name,\n",
    "                \"model_uri\": model_uri,\n",
    "                \"message\": f\"Deployed {model_name} from {model_uri}\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2cd66406-b861-4683-aabe-f5c0a376ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. KServe Model Wrapper\n",
    "class RetailerModel(Model):\n",
    "    def __init__(self, name: str):\n",
    "        super().__init__(name)\n",
    "        self.model = None\n",
    "        self.ready = False\n",
    "\n",
    "    def load(self):\n",
    "        # Load model from MLflow\n",
    "        model_uri = f\"models:/{self.name}/latest\"\n",
    "        self.model = mlflow.pyfunc.load_model(model_uri)\n",
    "        self.ready = True\n",
    "        return self.ready\n",
    "\n",
    "    def predict(self, payload: Dict, headers: Dict[str, str] = None) -> Dict:\n",
    "        input_data = get_predict_input(payload)\n",
    "        prediction = self.model.predict(input_data)\n",
    "        return get_predict_response(payload, prediction, self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bee9f69c-4f1b-463f-9761-11e95e1a2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScientistAgent:\n",
    "    def __init__(self):\n",
    "        self.tools = PredictiveTools()\n",
    "        self.deployed_models = {}\n",
    "    \n",
    "    def run_prediction_pipeline(self, task: str, params: Dict):\n",
    "        \"\"\"End-to-end prediction workflow with proper error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting {task} prediction pipeline...\")\n",
    "            \n",
    "            if task == \"demand_forecast\":\n",
    "                # 1. Train model\n",
    "                train_result = self.tools.train_demand_forecast_model(\n",
    "                    product_id=params.get(\"product_id\")\n",
    "                )\n",
    "                \n",
    "                if train_result.get(\"status\") != \"success\":\n",
    "                    return train_result\n",
    "                \n",
    "                # 2. Deploy model\n",
    "                model_name = f\"demand-forecast-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "                deploy_result = self.tools.deploy_model_to_kserve(\n",
    "                    train_result[\"model_uri\"],\n",
    "                    model_name\n",
    "                )\n",
    "\n",
    "                # 3. Verify and store deployment\n",
    "                if deploy_result.get(\"status\") == \"deployed\":\n",
    "                    ready = self._wait_for_deployment(model_name)\n",
    "                    if not ready:\n",
    "                        return {\n",
    "                            \"status\": \"error\",\n",
    "                            \"message\": \"Deployment timeout\",\n",
    "                            \"model_name\": model_name\n",
    "                        }\n",
    "                    \n",
    "                    # Get the real endpoint URL\n",
    "                    deploy_result[\"endpoint\"] = self._get_kserve_url(model_name)\n",
    "                    self.deployed_models[model_name] = {\n",
    "                        \"train_metrics\": train_result[\"metrics\"],\n",
    "                        \"deploy_info\": deploy_result,\n",
    "                        \"deploy_time\": datetime.now().isoformat()\n",
    "                    }\n",
    "                \n",
    "                return deploy_result\n",
    "            \n",
    "            return {\"status\": \"error\", \"message\": \"Unsupported task\"}\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "    def _get_kserve_url(self, model_name: str) -> str:\n",
    "        \"\"\"Get the actual KServe endpoint URL\"\"\"\n",
    "        config.load_incluster_config()\n",
    "        api = client.CustomObjectsApi()\n",
    "        \n",
    "        isvc = api.get_namespaced_custom_object(\n",
    "            group=\"serving.kserve.io\",\n",
    "            version=\"v1beta1\",\n",
    "            namespace=NAMESPACE,\n",
    "            plural=\"inferenceservices\",\n",
    "            name=model_name\n",
    "        )\n",
    "        \n",
    "        if 'status' in isvc and 'url' in isvc['status']:\n",
    "            return isvc['status']['url']\n",
    "        return f\"http://{model_name}-predictor-default.{NAMESPACE}.svc.cluster.local\"\n",
    "\n",
    "    def _wait_for_deployment(self, model_name: str, timeout_sec: int = 300) -> bool:\n",
    "        \"\"\"Wait for KServe deployment to be ready\"\"\"\n",
    "        config.load_incluster_config()\n",
    "        api = client.CustomObjectsApi()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout_sec:\n",
    "            try:\n",
    "                isvc = api.get_namespaced_custom_object(\n",
    "                    group=\"serving.kserve.io\",\n",
    "                    version=\"v1beta1\",\n",
    "                    namespace=NAMESPACE,\n",
    "                    plural=\"inferenceservices\",\n",
    "                    name=model_name\n",
    "                )\n",
    "                \n",
    "                if 'status' in isvc and 'conditions' in isvc['status']:\n",
    "                    for condition in isvc['status']['conditions']:\n",
    "                        if condition['type'] == 'Ready' and condition['status'] == 'True':\n",
    "                            return True\n",
    "                \n",
    "                time.sleep(5)\n",
    "            except Exception:\n",
    "                time.sleep(5)\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cd842adb-7672-4286-83e8-d7ecfeae3600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Data Scientist Agent (KServe Integrated)\n",
      "    --------------------------------------\n",
      "    Available commands:\n",
      "    - forecast <product_id>: Run demand forecast pipeline\n",
      "    - list_models: Show deployed models\n",
      "    - delete <model_name>: Delete a deployed model\n",
      "    - exit: Quit\n",
      "    \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Command:  forecast 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting demand_forecast prediction pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/02 11:47:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/04/02 11:47:51 INFO mlflow.tracking._tracking_service.client: üèÉ View run classy-gnu-247 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/22/runs/de312a1b81ea4b8abffd77d09041efeb.\n",
      "2025/04/02 11:47:51 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/22.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Deployment failed: Unknown error\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Command:  exit\n"
     ]
    }
   ],
   "source": [
    "# 9. Interactive Interface\n",
    "def run_data_scientist_agent():\n",
    "    agent = DataScientistAgent()\n",
    "    \n",
    "    print(\"\"\"\n",
    "    Data Scientist Agent (KServe Integrated)\n",
    "    --------------------------------------\n",
    "    Available commands:\n",
    "    - forecast <product_id>: Run demand forecast pipeline\n",
    "    - list_models: Show deployed models\n",
    "    - delete <model_name>: Delete a deployed model\n",
    "    - exit: Quit\n",
    "    \"\"\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            command = input(\"\\nCommand: \").strip().lower()\n",
    "            \n",
    "            if command.startswith(\"forecast\"):\n",
    "                product_id = command.split()[1] if len(command.split()) > 1 else None\n",
    "                result = agent.run_prediction_pipeline(\n",
    "                    \"demand_forecast\",\n",
    "                    {\"product_id\": product_id}\n",
    "                )\n",
    "                \n",
    "                if result.get(\"status\") == \"deployed\":\n",
    "                    print(f\"\\nüéâ Model deployed successfully!\")\n",
    "                    print(f\"Name: {result['model_name']}\")\n",
    "                    print(f\"Endpoint: {result['endpoint']}\")\n",
    "                    print(f\"Test with: curl -X POST {result['endpoint']}/v1/models/{result['model_name']}:predict\")\n",
    "                else:\n",
    "                    print(f\"\\n‚ùå Deployment failed: {result.get('error', 'Unknown error')}\")\n",
    "                \n",
    "            elif command == \"list_models\":\n",
    "                if not agent.deployed_models:\n",
    "                    print(\"No models currently deployed\")\n",
    "                else:\n",
    "                    print(\"\\nDeployed Models:\")\n",
    "                    for name, info in agent.deployed_models.items():\n",
    "                        print(f\"\\nüîπ {name}\")\n",
    "                        print(f\"Deployed: {info['deploy_time']}\")\n",
    "                        print(f\"Endpoint: {info['deploy_info']['endpoint']}\")\n",
    "                        print(f\"Metrics: {info['train_metrics']}\")\n",
    "                        \n",
    "            elif command.startswith(\"delete \"):\n",
    "                model_name = command.split()[1]\n",
    "                # Add delete functionality here\n",
    "                print(f\"Delete functionality for {model_name} would be implemented here\")\n",
    "                \n",
    "            elif command in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"Invalid command. Try 'forecast <product_id>', 'list_models', or 'exit'\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_scientist_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c89fbd91-9f8e-41de-9225-e14a1949103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token successfully refreshed.\n"
     ]
    }
   ],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c86a28-77ba-4ab2-96da-d3808522623d",
   "metadata": {},
   "source": [
    "##### Agent Runtime\n",
    "This section covers the code executed while the agent is in action, including:\n",
    "* Preparing the tools for use by the agent\n",
    "* The agent's runtime function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f310e2-7e3b-42e8-bd52-13a8a57896c7",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "### Sample Questions:\n",
    "1. What are our top-selling products by revenue and quantity sold?\n",
    "2. Who are our top 10 customers by total spend and order frequency?\n",
    "3. Which products have the lowest stock levels relative to their sales velocity?\n",
    "4. Which product categories generate the highest profit margins?\n",
    "5. What is our order fulfillment rate and average time to fulfill orders?\n",
    "6. How has our customer base grown over time?\n",
    "7. What are the seasonal trends in our product categories?\n",
    "8. What products are frequently purchased together?\n",
    "9. What percentage of customers make repeat purchases?\n",
    "10. Which customer segments are most profitable when considering acquisition cost and lifetime value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a302d7f-d098-421e-a434-0f4d3c52acf5",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae613a63-b596-43ab-b06c-13dc7314ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No resources found in vincent-charbon-ed7eea62 namespace.\n"
     ]
    }
   ],
   "source": [
    "!kubectl get isvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb43629-0654-4821-80bf-85ae4fe51234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
