{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be01229-0819-4586-8d10-32c2a0513ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7ab9d-1899-4fee-a27d-7d6f1d6ccc9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> This exercise requires the completion of <a href=\"./01.prepare_the_data.ipynb\" <b>Exercise 1:</b> Prepare the Data</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806b008-9c4f-42f5-838a-5b42b0d5188e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 2:** Connect and Query Data Sources with EzPresto\n",
    "\n",
    "This exercise will be introduce **EzPresto**, HPE's supercharged Presto distribution on **HPE AI Essentials**. EzPresto is an SQL query engine based on the open-source query engine PrestoDB that is optimized to run federated queries across various data sources. Enterprise applications and data processing engines (such as Spark!) can leverage EzPresto for rapid query performance and prompt insights through federated data access. \n",
    "\n",
    "EzPresto is the underlying driver behind all of what you will learn in this exercise, in which you will:\n",
    "\n",
    "- Connect Data Sources to HPE Private Cloud AI.\n",
    "- Run SQL queries using the Query Editor.\n",
    "- Learn about HPE's custom Jupyter Magic commands.\n",
    "- Run SQL queries on connected Data Sources directly from within a notebook cell.\n",
    "\n",
    "By the end of this exercise, you will be proficient in directly or indirectly leveraging EzPresto to streamline your data analytics workflows. \n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f71c84-02cf-4d3e-9f4f-c47e6ac2dbb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Connecting a Data Source in AI Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0fe25-896d-4c44-a64b-2ae98df81012",
   "metadata": {
    "tags": []
   },
   "source": [
    "**HPE Private Cloud AI** allows users to connect multiple types of internal and external data sources - from SQL servers to Snowflake, Terradata and Oracle databases - and make the files, objects and tables within them available to any tool or application running on PCAI.\n",
    "\n",
    "In this section, you will learn how to make a data connection using the Delta Tables you created in Exercise 1. \n",
    "\n",
    "### Connect Delta Tables as Data Source using Apache Hive.\n",
    "\n",
    "Let's take those Delta Tables you created in Exercise 1 and make them available to other applications in AI Essentials by connecting them as an **Data Source**. Apache Hive gives an SQL-like interface to query data stored in various databases and file systems, like the delta tables you created in Exercise 1. This will allow you to use EzPresto to turn them into datasets later in the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55feb0-5392-496c-8dcb-17299d9c3efb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Set your <b>Username</b> here !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e8788a-205f-42ff-9222-7c18efd8c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME=\"vince\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66903125-7e78-41ea-b13b-4807d834082f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We start by defining a function to read the Kubernetes namespace from the service account mount point, allowing us to deploy resources in the correct namespace. If not running in a Kubernetes environment, it defaults to the \"default\" namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3522928-47f7-4ced-95c0-68157b8251fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_namespace_from_service_account():\n",
    "    \"\"\"\n",
    "    Reads the Kubernetes namespace from the service account mount point.\n",
    "    Returns 'default' if not running in a Kubernetes pod or if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    namespace_file = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n",
    "    try:\n",
    "        with open(namespace_file, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except IOError:\n",
    "        return 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032655fa-81db-419f-9993-08e2b09cb1df",
   "metadata": {
    "tags": []
   },
   "source": [
    "Set the Global variables required to run the exercise smootly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001da5c6-ff48-475d-a270-9defbf6cd3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE: vincent-charbon-01f47b03\n",
      "POSTGRES_PASSWORD: postgres\n",
      "PG_SERVICE_NAME: vince-retail-postgres\n",
      "PG_DATABASE_NAME: vince-retail\n"
     ]
    }
   ],
   "source": [
    "# Global configuration\n",
    "NAMESPACE = get_namespace_from_service_account()\n",
    "POSTGRES_PASSWORD = \"postgres\"\n",
    "PG_SERVICE_NAME = f\"{USERNAME}-retail-postgres\"\n",
    "PG_DATABASE_NAME = f\"{USERNAME}-retail\"\n",
    "\n",
    "# Print the result\n",
    "print(\"NAMESPACE:\", NAMESPACE)\n",
    "print(\"POSTGRES_PASSWORD:\", POSTGRES_PASSWORD)\n",
    "print(\"PG_SERVICE_NAME:\", PG_SERVICE_NAME)\n",
    "print(\"PG_DATABASE_NAME:\", PG_DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ff8b7-8429-4f88-9d52-484dde25ff5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "First let's get your details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20f2f04-8686-45c9-97bc-8d030b074fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PostgreSQL Connection Details:\n",
      "========================================\n",
      "               Name : retailvince\n",
      "     Connection Url : jdbc:postgresql://vince-retail-postgres.vincent-charbon-01f47b03.svc.cluster.local:5432/vince-retail\n",
      "    Connection User : postgres\n",
      "Connection Password : postgres\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_db_connection_details():\n",
    "    \"\"\"Display database connection details without connecting\"\"\"\n",
    "    connection_details = {\n",
    "        \"Name\": f\"retail{USERNAME}\",\n",
    "        \"Connection Url\": f\"jdbc:postgresql://{PG_SERVICE_NAME}.{NAMESPACE}.svc.cluster.local:5432/{PG_DATABASE_NAME}\",\n",
    "        \"Connection User\": \"postgres\",\n",
    "        \"Connection Password\": POSTGRES_PASSWORD,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPostgreSQL Connection Details:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    max_key_length = max(len(key) for key in connection_details.keys())\n",
    "    \n",
    "    for key, value in connection_details.items():\n",
    "        print(f\"{key.title():>{max_key_length}} : {value}\")\n",
    "    \n",
    "    print(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "display_db_connection_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d9b1-1b1e-4170-8590-6410c8ce5c22",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Navigate back to the AI Essentials dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Engineering` > `Data Sources`.\n",
    "1. Under `Structured Data`, click `Add New Data Source`.\n",
    "1. Under **PostgreSQL**, click `Create Connection`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eef0ef-596b-49d8-aaea-0fb356b214d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/connect-dl.png\" alt=\"Drawing\" style=\"width: 25%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81c4e6-e737-4a72-be42-b2797b4f0864",
   "metadata": {
    "tags": []
   },
   "source": [
    "6. Under the `Structured Data` tab, you will now see your connected data source. \n",
    "\n",
    "### Viewing and Querying Data from Data Sources\n",
    "\n",
    "Now that our Delta Tables are available via a Data Source, we can leverage the native data tools within AI Essentials to run queries and create datasets.\n",
    "\n",
    "1. Click on the three dots in the top right hand corner of the newly created data source.\n",
    "1. Click `Change to public access`. In the dialog box that appears, click `Proceed`.\n",
    "1. Next, select `Query using Data Catalog`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f721c4a-07a1-4c2f-b26a-8f0b9a0da9a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/title.png\" alt=\"Drawing\" style=\"width: 25%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03eaf7-2b56-487b-9853-01edbdfcaedd",
   "metadata": {
    "tags": []
   },
   "source": [
    "4. Under `Connected Data Sources`, look for the `retail` group.\n",
    "5. Under the `retail` group, check the `default` box. \n",
    "6. Select the datasets for all three countries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48139022-3815-40ea-bb82-a0ab84b1fc0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/datacatalog.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355dcac6-8f33-4c6d-813c-357c0afb1f69",
   "metadata": {
    "tags": []
   },
   "source": [
    "7. Click `Selected Datasets` in the top left corner.\n",
    "8. Click `Query Editor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a74bc-bb74-4e46-9f63-1cebc4990d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, you are introduced to the the **Query Editor** where you can directly query data sources from specific datasets and data tables - all from within the HPE AI Essentials user interface! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de14a6-4cad-4d43-adf0-d6d5452c3429",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/QueryEditor.png\" alt=\"Drawing\" style=\"width: 75%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f9fe6-6057-4d7b-ab9b-a940f6b3f8fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "9. Next, we're going to run an SQL query which will combine the data from our three tables (czech, germany, and swiss) in the retail.default schema. This will merge all columns from the czech and germany tables, and select specific columns from the swiss table whilst also applying a transformation to the country column. We'll also limit our final result set to 1000 rows. And all in a nice UI!\n",
    "\n",
    "    Paste the following SQL Query into the `SQL Query` field, update the table names in the `FROM` to reflect with your environment and \n",
    "\n",
    "   Click `Run`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13bcea-7bb7-4f4f-a216-c0a42f4f6d1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "```sql\n",
    "SELECT \n",
    "  sc.customer_id,\n",
    "  sc.customer_name,\n",
    "  sc.customer_email,\n",
    "  COUNT(DISTINCT so.order_id) AS order_frequency,\n",
    "  SUM(sop.product_quantity * sca.price_cents) AS total_spend\n",
    "FROM \n",
    "  retailvince.public.source_orders so\n",
    "  JOIN retailvince.public.source_order_products sop ON so.order_id = sop.order_id\n",
    "  JOIN retailvince.public.source_catalog sca ON sop.product_id = sca.product_id\n",
    "  JOIN retailvince.public.source_customers sc ON so.customer_id = sc.customer_id\n",
    "GROUP BY \n",
    "  sc.customer_id, sc.customer_name, sc.customer_email\n",
    "ORDER BY \n",
    "  total_spend DESC, order_frequency DESC\n",
    "LIMIT 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d258af-ff6e-47b6-9fcb-adb98593bb69",
   "metadata": {
    "tags": []
   },
   "source": [
    "10. Expand the resulting query to visually validate it.\n",
    "11. Under `Actions` (top-left corner of the table), click `Save as View`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d945a-e833-4e5a-8d65-dd00989aa680",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/query-results.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cdd3d-f65a-4cec-a360-b52e6b93b377",
   "metadata": {
    "tags": []
   },
   "source": [
    "12. Name the View `retail`. \n",
    "13. We'll want to save the schema of this new table as a Custom Schema. Under `Schema`, select `+ Add new schema` and name it `retailschema`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785fbea-7da1-498a-abcd-75a10f07472b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/save-as-view.png\" alt=\"Drawing\" style=\"width: 45%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a892513-16c2-4467-b14a-afa866c6b591",
   "metadata": {
    "tags": []
   },
   "source": [
    "You have now saved the dataset resulting from your SQL query as an **Asset**. Assets are made available to other applications via EzPresto, as you will explore in Exercise 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93394c5a-e892-4908-a14f-77cabd545b96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Cached Assets in HPE AI Essentials**\n",
    "\n",
    "To view our saved Assets, including the one that we just created:\n",
    "\n",
    "1. In AIE, under the sidebar navigation menu, select `Data Engineering` > `Cached Assets`. \n",
    "1. You should see an asset with Name `retail`. \n",
    "1. Click the three dots associated with the `retail` asset.\n",
    "\n",
    "<img src=\"./images/exercise2/cachedasset.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "\n",
    "4. Click `View Columns`. \n",
    "5. Validate there are eleven fields. \n",
    "\n",
    "Should you ever wish to run futher queries on a Cached Asset in the future, check the box next to an asset and click `Query Editor`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc431cae-33d3-4f9f-bedb-d61b4b43b166",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Jupyter Magic Commands on HPE AI Essentials**\n",
    "\n",
    "Jupyter Notebooks Magic functions, also known as magic commands or magics, are commands that you can execute within a code cell.   \n",
    "Magics are not code of any language, but are shortcuts that extends the capabilities of a notebook. \n",
    "\n",
    "There are two types of magic commands - **Line** and **Cell** magic commands:\n",
    "\n",
    "**Line magic** commands do not require a cell body and start with a single % character.  \n",
    "**Cell magic** commands start with %% and require additional lines of input (a cell body). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05b8ca-96c8-4232-b1c2-b66b3d197a7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HPE AI Essentials Magic Commands \n",
    "**HPE AI Essentials** supports both Line and Cell magic commands and includes custom commands that allow for users to interact with other tools native to AIE directly within notebooks.\n",
    "\n",
    "We can check out the full list of custom HPE magic commands by running `%command`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb82bf08-370c-4d5f-b866-e34c1c28623e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e25e767ce74f7f8feb53abd3040408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n    <table style=\"border-collapse: collapse; border: 1px solid black; width: 100%;\">\\n        <t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The %commands command lists the magic commands and SDKs that are customized by Hewlett Packard Enterprise and are available in this notebook.\n",
    "%commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d19dd-815b-43ca-92bb-b2bf74ce12a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Updating the Cached JWT Token\n",
    "\n",
    "In a Jupyter notebook, a JWT token (JSON Web Token) is a compact and URL-safe means of representing authentication information to be transferred between other notebook servers or external applications. It is commonly used for securely authenticating and authorizing users within Jupyter environments, allowing them to access resources and execute code while ensuring their identity and permissions are properly validated.\n",
    "\n",
    "When working in Jupyter notebooks for long durations, particularly when making calls to other applications, the JWT token can expire and result in an error when attempting to make calls. This is particularly relevant for working on a Jupyter notebook within **HPE AI Essentials**, which provides users to leverage a plethora of external tools within the notebook (Such as Spark, Livy and Presto).\n",
    "\n",
    "**If you encounter a JWT token expiration error while running cells in a Jupyter notebook**, you can resolve it by running the `%update_token` magic command.  \n",
    "This function updates the JWT in environment variables and any other locations where the token is utilized. \n",
    "  \n",
    "Ideally, it is good practice to refresh the token prior to making external connections. Some examples relevant to the Smart Retail Experience demo include:\n",
    "\n",
    "- Authentication when establishing a connection with PrestoDB.\n",
    "- Authentication with local s3 minio object storage.  \n",
    "- Authentication with KServe external API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe7b6e8-d5a4-4244-9edf-9d9425355d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token successfully refreshed.\n"
     ]
    }
   ],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b59107-9c22-430e-b49e-a41e29245916",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Directly interacting with connected SQL databases using the SQL Magic Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c053e-e835-4527-b02c-769fc26b65c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using the `%sql` magic command, you can directly query SQL databases from Data Sources you have connected to **HPE AI Essentials** from within Jupyter notebook cells! When you run the notebook cell containing `%sql` and your SQL query, the magic command sends the query to the database, runs the query, and retrieves the result.\n",
    "\n",
    "This is made possible by the native integration of EzPresto into AIE. **However, the Data Source must be made publicly available.**\n",
    "\n",
    "To change the access of a Data Source from `private` to `public`:\n",
    "\n",
    "1. Navigate back to the AIE dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Engineering` > `Data Sources`.\n",
    "1. Under the three dots in the top corner of the Data Source of interest, click `Change to public access`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341b8e2-56f7-415f-b88a-cfa64c6b32f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> Wait until a confirmation message appears stating that the source is publicly available.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c2cd9-c591-433e-9150-ef4d83edf3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's try the `%sql` magic command to interact with our Delta Tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f552d93-de1e-4af8-9c2c-5d0ac2c960cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;presto://ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local:8081&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'presto://ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local:8081'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SELECT * FROM retailvince.public.source_customers limit 10\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>customer_id</th>\n",
       "            <th>customer_name</th>\n",
       "            <th>customer_surname</th>\n",
       "            <th>customer_email</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>None</td>\n",
       "            <td>Brown</td>\n",
       "            <td>liam.williams@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>Ava</td>\n",
       "            <td>Brown</td>\n",
       "            <td>noah.jones@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>None</td>\n",
       "            <td>Johnson</td>\n",
       "            <td>emma.johnson@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4</td>\n",
       "            <td>Olivia</td>\n",
       "            <td>None</td>\n",
       "            <td>unknown.smith@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5</td>\n",
       "            <td>Olivia</td>\n",
       "            <td>Smith</td>\n",
       "            <td>unknown.williams@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6</td>\n",
       "            <td>Olivia</td>\n",
       "            <td>Van Helsing</td>\n",
       "            <td>unknown.smith@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7</td>\n",
       "            <td>None</td>\n",
       "            <td>Brown</td>\n",
       "            <td>invalid.email</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8</td>\n",
       "            <td>Noah</td>\n",
       "            <td>O'Brien</td>\n",
       "            <td>liam.jones@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9</td>\n",
       "            <td>Olivia</td>\n",
       "            <td>Johnson</td>\n",
       "            <td>noah.brown@example.com</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>10</td>\n",
       "            <td>Ava</td>\n",
       "            <td>Williams</td>\n",
       "            <td>emma.smith@example.com</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<span style='font-style:italic;font-size:11px'><code>ResultSet</code> : to convert to pandas, call <a href='https://jupysql.ploomber.io/en/latest/integrations/pandas.html'><code>.DataFrame()</code></a> or to polars, call <a href='https://jupysql.ploomber.io/en/latest/integrations/polars.html'><code>.PolarsDataFrame()</code></a></span><br>\n",
       "<span style=\"font-style:italic;text-align:center;\">Truncated to displaylimit of 10</span><br><span style=\"font-style:italic;text-align:center;\">If you want to see more, please visit <a href=\"https://jupysql.ploomber.io/en/latest/api/configuration.html#displaylimit\">displaylimit</a> configuration</span>"
      ],
      "text/plain": [
       "+-------------+---------------+------------------+------------------------------+\n",
       "| customer_id | customer_name | customer_surname |        customer_email        |\n",
       "+-------------+---------------+------------------+------------------------------+\n",
       "|      1      |      None     |      Brown       |  liam.williams@example.com   |\n",
       "|      2      |      Ava      |      Brown       |    noah.jones@example.com    |\n",
       "|      3      |      None     |     Johnson      |   emma.johnson@example.com   |\n",
       "|      4      |     Olivia    |       None       |  unknown.smith@example.com   |\n",
       "|      5      |     Olivia    |      Smith       | unknown.williams@example.com |\n",
       "|      6      |     Olivia    |   Van Helsing    |  unknown.smith@example.com   |\n",
       "|      7      |      None     |      Brown       |        invalid.email         |\n",
       "|      8      |      Noah     |     O'Brien      |    liam.jones@example.com    |\n",
       "|      9      |     Olivia    |     Johnson      |    noah.brown@example.com    |\n",
       "|      10     |      Ava      |     Williams     |    emma.smith@example.com    |\n",
       "+-------------+---------------+------------------+------------------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM retailvince.public.source_customers limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fc7b3-da34-43bd-8a6c-5c23ddbb377b",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also save the output of our command as a Python variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cece6e3-372a-414f-accb-8dd82b4841ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;presto://ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local:8081&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'presto://ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local:8081'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SELECT * FROM retailvince.public.source_customers limit 10\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+------------------+------------------------------+\n",
      "| customer_id | customer_name | customer_surname |        customer_email        |\n",
      "+-------------+---------------+------------------+------------------------------+\n",
      "|      1      |      None     |      Brown       |  liam.williams@example.com   |\n",
      "|      2      |      Ava      |      Brown       |    noah.jones@example.com    |\n",
      "|      3      |      None     |     Johnson      |   emma.johnson@example.com   |\n",
      "|      4      |     Olivia    |       None       |  unknown.smith@example.com   |\n",
      "|      5      |     Olivia    |      Smith       | unknown.williams@example.com |\n",
      "|      6      |     Olivia    |   Van Helsing    |  unknown.smith@example.com   |\n",
      "|      7      |      None     |      Brown       |        invalid.email         |\n",
      "|      8      |      Noah     |     O'Brien      |    liam.jones@example.com    |\n",
      "|      9      |     Olivia    |     Johnson      |    noah.brown@example.com    |\n",
      "|      10     |      Ava      |     Williams     |    emma.smith@example.com    |\n",
      "+-------------+---------------+------------------+------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ezpresto-sts-mst-0.ezpresto-svc-hdl.ezpresto.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = %sql SELECT * FROM retailvince.public.source_customers limit 10\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfac0-f760-45ed-9c7f-ecc7d86535a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Conclusion**\n",
    "\n",
    "In this exercise, you learned how **EzPresto** on **HPE AI Essentials** makes connecting internal and external data sources to your applications, such this notebook hosted on AIE, a snap. You learned how to leverage the data engineering tools available within the AIE interface, including the Query Editor, to create datasets from your data sources that could then be shared and quered using HPE Magic Commands inside AIE-hosted Jupyter Notebooks.\n",
    "\n",
    "If you look at the results of the SQL query, you can see some inconsistencies. Therefore, in the next exercise, you will learn how to transform this data using **Apache Spark** to prepare it for AI!\n",
    "\n",
    "Next: <a href=\"./03.explore_data_with_spark.ipynb\" style=\"color: black\"><b style=\"color: #01a982;\">Exercise 3:</b> Explore Retail Data with Apache Spark</a>\n",
    "\n",
    "Previous: <a href=\"./01.prepare_the_data.ipynb\" style=\"color: black\"><b style=\"color: #01a982;\">Exercise 1:</b> Prepare the Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c2019-25ff-4a45-97d3-9d1e24adcf64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
