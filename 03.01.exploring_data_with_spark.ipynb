{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801896dd-7519-4344-8cac-2a9e6ce22a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bac2dc-3599-441e-ab23-d952d84d1e24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 3:** Exploring Retail Data with Apache Spark\n",
    "\n",
    "This exercise will introduce **Apache Spark on HPE AI Essentials**. We'll leverage Spark's powerful distributed processing capabilities to analyze and fix the sales information.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Set up a Spark session for interacting with data.\n",
    "- Generate sample sales data for different countries and currencies.\n",
    "- Explore techniques for data loading, transformation, and analysis using Spark SQL and DataFrames.\n",
    "- Create Delta Tables and perform version control.\n",
    "\n",
    "Feel free to modify and extend the code examples to suit your specific data analysis needs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf107c4-4241-4b59-968e-c0cd1ea13f8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Prerequisites:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9744912-6776-4d76-a074-770fe2be4750",
   "metadata": {
    "tags": []
   },
   "source": [
    "As instructed in the [Introductory notebook](./00.introduction.ipynb), ensure that you have run `pip install -r requirements.txt` in a Terminal window, located in the same working directory, prior to running this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34b061-c46d-4864-ad93-36367fd76695",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Make sure you selected <b>PySpark</b> for your notebook kernel - check the top right corner!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbeea39-73eb-4ed7-9ab8-c6bd3c5aa8c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Create Spark Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366089e-a3b4-4c9e-ba53-fcbad54f0034",
   "metadata": {
    "tags": []
   },
   "source": [
    "Think about the most recent Excel spreadsheet you edited. It probably had tens or even hundreds of rows across tens of columns. When you run an Excel command, such as a *SUM()* or a *VLOOKUP()*, you may have noticed that it took a far bit of time to process. Maybe, even the fans of your laptop sped up a bit as your computer worked to crunch the numbers. \n",
    "\n",
    "Now, scale that same command out to a spreadsheet with tens of **millions** of rows across **thousands** of columns. That is the Big Data that companies must work with on a daily basis, and no single PC is going to run any *VLOOKUP* command on data of that size.\n",
    "\n",
    "Instead of spreadsheets, the enterprise world is largely built upon **tables** in a variety of formats. To query these tables to retrieve certain data takes a **mammoth** amount of compute. It makes no sense to have a single **compute server** executing these queries - it would be far faster to parallelize queries across several computers. Enter **Apache Spark**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce431084-dbf6-46c0-9378-d31139a83d2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction to Apache Spark on HPE AI Essentials\n",
    "\n",
    "Apache Spark is a popular open-source big data framework that **distributes the computations** required to perform queries on large sets of data. This distribution, along with working with data in-memory rather than directly from storage disks, drastically brings down the time usually taken to query and index data. The combination of speed, versatility, and ease of use made Spark the go-to framework when working with big data. \n",
    "\n",
    "Apache Spark comes pre-installed with **HPE Ezmeral AI Essentials** and can leverage as much or as little of the compute available in a AIE cluster as a user desired. The core components of an Apache Spark deployment include:\n",
    "\n",
    "<img src=\"./images/exercise1/spark_archi.PNG\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "\n",
    "**Driver:** The driver program coordinates the execution of Spark jobs. It submits tasks to executors, schedules operations, and manages communication between various components.\n",
    "\n",
    "**Workers:** These are machines in the Spark cluster that manage executors. Each worker runs one or more executors. When running Spark on a HPE AI Essentials deployment, Spark Workers are Kubernetes pods distributed among worker nodes of the AIE cluster, allowing them to scale across multiple machines as required. \n",
    "\n",
    "**Executors:** Executors reside on worker nodes and carry out the actual computations assigned by the driver program. They partition and distribute the workload across machines in the cluster.\n",
    "\n",
    "**JVM:**  Spark utilizes the Java Virtual Machine (JVM) on each worker node to execute executors.\n",
    "\n",
    "On **HPE AI Essentials**, you will use Apache Spark to analyze large datasets at high speed with a unified platform for batch processing, streaming, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be59379-6d08-426b-b759-80942d845b76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Spark Interactive Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07e337-f157-4cc2-9160-6f7cbaa69aaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's begin using Spark! Here, you use HPE AI Essentials' native integration of **Apache Livy** to create and manage an interactive Spark session. Livy is an open-source REST service that enables remote and interactive analytics on Apache Spark clusters. It provides a way to interact with Spark clusters programmatically using a REST API, allowing you to submit Spark jobs, run interactive queries, and manage Sparksessions from web applications without the need for a specific Spark client. As a result, multiple AIE users can interact with your Spark cluster concurrently and reliably!\n",
    "\n",
    "First, let's connect to the Livy endpoint and create a new Spark interactive session. The Spark interactive\n",
    "session is particularly useful for exploratory data analysis, prototyping, and iterative development. It allows you to\n",
    "interactively work with large datasets, perform transformations, apply analytical operations, and build ML models using\n",
    "Spark's distributed computing capabilities. \n",
    "\n",
    "To communicate with Livy and manage your sessions you use Sparkmagic, an open-source tool that provides a Jupyter kernel\n",
    "extension. Sparkmagic integrates with Livy, to provide the underlying communication layer between the Jupyter kernel and\n",
    "the Spark cluster.\n",
    "\n",
    "**Execute the cell below**, then:\n",
    "\n",
    "1. Select the `Add Endpoint` tab.\n",
    "1. Select `Single Sign-on` and ensure there is a Livy address in the `Address` field. \n",
    "1. Click `Add Endpoint`.\n",
    "1. Select the `Create Session` tab.\n",
    "1. Provide a name (e.g. `retail-demo`).\n",
    "1. Select `python` under the Language field.\n",
    "1. Click `Create Session` (right side).\n",
    "\n",
    "The session will take a few minutes for your session to initialize. \n",
    "\n",
    "Once ready, the Manage Sessions pane will activate, displaying\n",
    "your session ID. When the session state turns to idle, you're all set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182b30ae-c564-42d3-8872-3559afa39b84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9993336996a54b998929b06bb9a9fc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='No sessions yet.'))), CreateSessi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1544dabe9fb54aab942e2d3413aa10d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [],
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf2946-32f2-40b6-88cc-0af78fed48f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's check the status of the session.\n",
    "\n",
    "1. Navigate back to the AIE dashboard.\n",
    "1. In the sidebar navigation menu, select `Spark Interactive Sessions`.\n",
    "\n",
    "![image.png](./images/exercise1/menu.PNG)\n",
    "\n",
    "3. Here, you can check the status of your session. It will take 2-3 minutes to start. When the `State` says `Idle`, the session is ready. \n",
    "\n",
    "![image.png](./images/exercise1/session.PNG)\n",
    "\n",
    "4. Scroll back up to the Notebook cell of the session (%manage_spark command). Confirm under the `Manage Sessions` tab that the session should now be visible as `Idle` too. \n",
    "\n",
    "![image.png](./images/exercise1/session2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172e937-fb30-4e50-ac72-c94d4c66ba5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Spark Interactive Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1b1ae-2d62-420d-bf6d-f5dc43a63111",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Run the `%config_spark` magic command.\n",
    "2. Leave the settings as they are. Click `Submit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89506a-93b1-4673-8064-3f51470ce265",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Ignore the resulting message and <b>do not</b> restart the kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43012d3a-51e1-40ba-8827-c1ef21175533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79c95e-d669-4575-adba-c2ce7553a120",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, let's import the required libraries for working with Spark in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06db1cc-a4d9-4b51-9052-7677547c7bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"file:///mounts/shared-volume/shared/retail-data/delta-tables/vince\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba716488-8cc3-4917-9b92-4a8470afd682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailDataPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "delta_path = \"file:///mounts/shared-volume/shared/retail-data/delta-tables/vince/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44180fb8-24d9-431d-ba8d-e9ddf91196e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: source_catalog\n",
      "Successfully saved source_catalog to Delta format\n",
      "Processing table: source_customers\n",
      "Successfully saved source_customers to Delta format\n",
      "Processing table: source_orders\n",
      "Successfully saved source_orders to Delta format\n",
      "Processing table: source_order_products\n",
      "Successfully saved source_order_products to Delta format\n",
      "Processing table: source_stock\n",
      "Successfully saved source_stock to Delta format\n",
      "All tables processed"
     ]
    }
   ],
   "source": [
    "# List of tables to extract\n",
    "tables = [\n",
    "    \"source_catalog\",\n",
    "    \"source_customers\",\n",
    "    \"source_orders\",\n",
    "    \"source_order_products\",\n",
    "    \"source_stock\"\n",
    "]\n",
    "\n",
    "def extract_and_save_table(table_name):\n",
    "    \"\"\"Extract a single table from Presto and save to Delta\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "        \n",
    "        # Presto connection configuration\n",
    "        uri = f\"jdbc:presto://ezpresto.{DOMAIN}:443/{catalog}/{schema}\"\n",
    "        query = f\"SELECT * FROM {catalog}.{schema}.{table_name}\"\n",
    "        \n",
    "        # Read from Presto\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"driver\", \"com.facebook.presto.jdbc.PrestoDriver\") \\\n",
    "            .option(\"url\", uri) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"SSL\", \"true\") \\\n",
    "            .option(\"IgnoreSSLChecks\", \"true\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Write to Delta format\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"{delta_path}{table_name}\")\n",
    "            \n",
    "        print(f\"Successfully saved {table_name} to Delta format\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Process all tables\n",
    "for table in tables:\n",
    "    success = extract_and_save_table(table)\n",
    "    if not success:\n",
    "        print(f\"Failed to process table {table}\")\n",
    "        # Continue with next table or break based on your requirements\n",
    "\n",
    "print(\"All tables processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4518a59b-ce54-4efa-9549-d6719864a158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, when, lit\n",
    "\n",
    "# Clean product names and categories\n",
    "cleaned_catalog = spark.read.format(\"delta\").load(f\"{delta_path}source_catalog\") \\\n",
    "    .withColumn(\"product_name\", trim(col(\"product_name\"))) \\\n",
    "    .withColumn(\"product_category\", \n",
    "        when(col(\"product_category\") == \"Toyz\", \"Toys\")\n",
    "        .when(col(\"product_category\") == \"Clothng\", \"Clothing\")\n",
    "        .when(col(\"product_category\") == \"Eletronics\", \"Electronics\")\n",
    "        .otherwise(col(\"product_category\"))) \\\n",
    "    .filter(col(\"product_id\").isNotNull()) \\\n",
    "    .filter(col(\"price_cents\") > 0)  # Remove negative prices\n",
    "\n",
    "cleaned_catalog.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c826182-adfa-4ba4-9f24-d869b91c4c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean customer data\n",
    "cleaned_customers = spark.read.format(\"delta\").load(f\"{delta_path}source_customers\") \\\n",
    "    .withColumn(\"customer_name\", trim(col(\"customer_name\"))) \\\n",
    "    .withColumn(\"customer_surname\", trim(col(\"customer_surname\"))) \\\n",
    "    .withColumn(\"customer_email\",\n",
    "        when(\n",
    "            (col(\"customer_email\").contains(\"@\")) & \n",
    "            (col(\"customer_email\").contains(\".\")),\n",
    "            col(\"customer_email\")\n",
    "        ).otherwise(lit(None))) \\\n",
    "    .filter(col(\"customer_id\").isNotNull())\n",
    "\n",
    "cleaned_customers.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ad5aeda-e5e8-4caf-b00f-0a8ac4684ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean stock data\n",
    "cleaned_stock = spark.read.format(\"delta\").load(f\"{delta_path}source_stock\") \\\n",
    "    .filter(col(\"product_quantity\") > 0) \\\n",
    "    .filter(col(\"entry_date\") <= lit(current_date())) \\\n",
    "    .filter(col(\"product_id\").isNotNull()) \\\n",
    "    .filter(col(\"purchase_price_cents\") > 0)\n",
    "\n",
    "cleaned_stock.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cba173c-e25a-4dbe-8189-83d409a60f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9d38954a7d4671a27ae7f1a55fdcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://livy-0.livy-svc.spark.svc.cluster.local:8998/sessions/28/statements/44 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Clean orders data\n",
    "cleaned_orders = spark.read.format(\"delta\").load(f\"{delta_path}source_orders\") \\\n",
    "    .filter(col(\"order_date\") <= current_date()) \\\n",
    "    .filter(col(\"customer_id\").isNotNull()) \\\n",
    "    .withColumn(\"order_status\",\n",
    "        when(col(\"order_status\").isin([\"completed\", \"pending\", \"cancelled\", \"shipped\"]),\n",
    "            col(\"order_status\")\n",
    "        ).otherwise(lit(\"pending\")))\n",
    "\n",
    "cleaned_orders.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dca8f5aa-a8f4-4920-9ce0-8c271ba0777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:157)\n",
      "\tat org.apache.spark.internal.Logging$.islog4j2DefaultConfigured(Logging.scala:258)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:133)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:114)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:108)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:76)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:84)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "ERROR StatusConsoleListener Could not create plugin of type class org.apache.logging.log4j.core.async.AsyncLoggerConfig for element AsyncLogger: java.lang.NoClassDefFoundError: com/lmax/disruptor/EventHandler\n",
      " java.lang.NoClassDefFoundError: com/lmax/disruptor/EventHandler\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.getAsyncLoggerConfigDelegate(AbstractConfiguration.java:214)\n",
      "\tat org.apache.logging.log4j.core.async.AsyncLoggerConfig.<init>(AsyncLoggerConfig.java:108)\n",
      "\tat org.apache.logging.log4j.core.async.AsyncLoggerConfig$Builder.build(AsyncLoggerConfig.java:88)\n",
      "\tat org.apache.logging.log4j.core.async.AsyncLoggerConfig$Builder.build(AsyncLoggerConfig.java:80)\n",
      "\tat org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1138)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1063)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1055)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:664)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:258)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:304)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:694)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:711)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)\n",
      "\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)\n",
      "\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)\n",
      "\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:157)\n",
      "\tat org.apache.spark.internal.Logging$.islog4j2DefaultConfigured(Logging.scala:258)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:133)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:114)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:108)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:76)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:84)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.lang.ClassNotFoundException: com.lmax.disruptor.EventHandler\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\t... 27 more\n",
      "ERROR StatusConsoleListener Unable to invoke factory method in class org.apache.logging.log4j.core.async.AsyncLoggerConfig for element AsyncLogger: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.async.AsyncLoggerConfig\n",
      " java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.async.AsyncLoggerConfig\n",
      "\tat org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:260)\n",
      "\tat org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:136)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1138)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1063)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1055)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:664)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:258)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:304)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:694)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:711)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)\n",
      "\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)\n",
      "\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)\n",
      "\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:157)\n",
      "\tat org.apache.spark.internal.Logging$.islog4j2DefaultConfigured(Logging.scala:258)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:133)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:114)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:108)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:76)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:84)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "ERROR StatusConsoleListener Could not create plugin of type class org.apache.logging.log4j.core.async.AsyncLoggerConfig for element AsyncLogger: java.lang.NoClassDefFoundError: com/lmax/disruptor/EventHandler\n",
      " java.lang.NoClassDefFoundError: com/lmax/disruptor/EventHandler\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.getAsyncLoggerConfigDelegate(AbstractConfiguration.java:214)\n",
      "\tat org.apache.logging.log4j.core.async.AsyncLoggerConfig.<init>(AsyncLoggerConfig.java:108)\n",
      "\tat org.apache.logging.log4j.core.async.AsyncLoggerConfig$Builder.build(AsyncLoggerConfig.java:88)\n",
      "\tat org.apache.logging.log4j.core.async.AsyncLoggerConfig$Builder.build(AsyncLoggerConfig.java:80)\n",
      "\tat org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1138)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1063)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1055)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:664)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:258)\n",
      "\tat org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:304)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:694)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:711)\n",
      "\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)\n",
      "\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)\n",
      "\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)\n",
      "\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:157)\n",
      "\tat org.apache.spark.internal.Logging$.islog4j2DefaultConfigured(Logging.scala:258)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:133)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:114)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:108)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:76)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:84)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1137)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1146)\n"
     ]
    }
   ],
   "source": [
    "# Clean order products data\n",
    "cleaned_order_products = spark.read.format(\"delta\").load(f\"{delta_path}source_order_products\") \\\n",
    "    .filter(col(\"product_quantity\") > 0) \\\n",
    "    .filter(col(\"order_id\").isNotNull()) \\\n",
    "    .filter(col(\"product_id\").isNotNull())\n",
    "\n",
    "cleaned_order_products.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_order_products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd13b2-7758-497b-a119-009a5e61229e",
   "metadata": {},
   "source": [
    "# Now add fancy text and go back to presto to add the delta table there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705997c-de93-4f36-914a-801cee03fca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8665cd0-f89e-4d5e-a18c-2308642f43ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de69e7-9661-405a-a922-af8c1a1e81c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e2e1c-2c16-4db1-b3bc-709f8c2c961c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8a03b-c1e9-4543-89be-d9fc0659808f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff7b6b-8f5d-4d12-a501-1118b8f2e447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e332a-998e-43dc-bb5d-e7f2626cfd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "756a9487-8523-4d17-ade8-ce55d560da78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hey, The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "%%sql \n",
    "SELECT * FROM retailvince.public.source_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d114e84e-c07e-46e6-9cdb-49d08dfc18f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+-----------+\n",
      "|product_id|product_name|product_category|price_cents|\n",
      "+----------+------------+----------------+-----------+\n",
      "|         1|Electronic 1|           Books|       3980|\n",
      "|         2|    Clothn 2|      Home Decor|       4471|\n",
      "|         3|       Toy 3|        Clothing|       5279|\n",
      "|         4| Eletronic 4|            Toyz|       3426|\n",
      "|         5|        NULL|         Clothng|       8863|\n",
      "|         6|   Clothin 6|            Toyz|       3410|\n",
      "|         7| Home Deco 7|        Clothing|       1195|\n",
      "|         8|    Clothn 8|            NULL|       3458|\n",
      "|         9|       Toy 9|        Clothing|       7435|\n",
      "|        10|  Clothin 10|            Toys|       6627|\n",
      "|        11|Eletronic 11|            Toys|       2758|\n",
      "|        13|        NULL|         Clothng|       8983|\n",
      "|        14|      Toy 14|            NULL|       7845|\n",
      "|        15|Eletronic 15|      Home Decor|       8619|\n",
      "|        16|      Toy 16|         Clothng|       9183|\n",
      "|        17|      Toy 17|      Eletronics|       6925|\n",
      "|        18|        NULL|         Clothng|       9494|\n",
      "|        19|Home Deco 19|      Home Decor|       NULL|\n",
      "|        20|     Book 20|     Electronics|       9596|\n",
      "|        21|  Clothin 21|            Toyz|       8885|\n",
      "+----------+------------+----------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "DOMAIN = \"hpepcai.ezmeral.demo.local\"\n",
    "user = \"admin-901d042c\"\n",
    "catalog = \"retailvince\"\n",
    "schema = \"public\"\n",
    "table = \"source_catalog\"\n",
    "uri = f\"jdbc:presto://ezpresto.{DOMAIN}:443/{catalog}/{table}\"\n",
    "query = f\"select * from {catalog}.{schema}.{table}\"\n",
    "df = spark.read.format(\"jdbc\"). \\\n",
    "      option(\"driver\", \"com.facebook.presto.jdbc.PrestoDriver\"). \\\n",
    "      option(\"url\", uri). \\\n",
    "      option(\"user\", user). \\\n",
    "      option(\"SSL\", \"true\"). \\\n",
    "      option(\"IgnoreSSLChecks\", \"true\"). \\\n",
    "      option(\"query\", query). \\\n",
    "      load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d221be-4c50-4aa1-8799-5c4eb6ef34e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o155.load.\n",
      ": com.ezsql.sparkconnector.utils.dal.DalException: DalClient failed to extract schema\n",
      "\tat com.ezsql.sparkconnector.utils.dal.DalClient.extractSchema(DalClient.java:41)\n",
      "\tat com.ezsql.sparkconnector.PrestoTable.<init>(PrestoTable.java:30)\n",
      "\tat com.ezsql.sparkconnector.Presto.getTable(Presto.java:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n",
      "\tat scala.Option.flatMap(Option.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.ezsql.sparkconnector.utils.dal.DalException: failed to get execute query\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.executeSchemaQuery(SchemaHandler.java:44)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.fetchSchemaDetail(SchemaHandler.java:30)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.getSchema(SchemaHandler.java:26)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.DalClient.extractSchema(DalClient.java:38)\n",
      "\t... 19 more\n",
      "Caused by: java.net.UnknownHostException: ezpresto-sts-mst-0.admin-901d042c.svc.cluster.local\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:305)\n",
      "\tat java.base/sun.security.ssl.BaseSSLSocketImpl.connect(BaseSSLSocketImpl.java:173)\n",
      "\tat java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:182)\n",
      "\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:509)\n",
      "\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:604)\n",
      "\tat java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)\n",
      "\tat java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)\n",
      "\tat java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:207)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1186)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1080)\n",
      "\tat java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:193)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1387)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1362)\n",
      "\tat java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:246)\n",
      "\tat com.ezsql.sparkconnector.utils.http.HttpUtility.sendRequest(HttpUtility.java:53)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.executeSchemaQuery(SchemaHandler.java:41)\n",
      "\t... 22 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/mapr/spark/spark-3.5.1/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 314, in load\n",
      "    return self._df(self._jreader.load())\n",
      "  File \"/opt/mapr/spark/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/mapr/spark/spark-3.5.1/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/mapr/spark/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o155.load.\n",
      ": com.ezsql.sparkconnector.utils.dal.DalException: DalClient failed to extract schema\n",
      "\tat com.ezsql.sparkconnector.utils.dal.DalClient.extractSchema(DalClient.java:41)\n",
      "\tat com.ezsql.sparkconnector.PrestoTable.<init>(PrestoTable.java:30)\n",
      "\tat com.ezsql.sparkconnector.Presto.getTable(Presto.java:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n",
      "\tat scala.Option.flatMap(Option.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.ezsql.sparkconnector.utils.dal.DalException: failed to get execute query\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.executeSchemaQuery(SchemaHandler.java:44)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.fetchSchemaDetail(SchemaHandler.java:30)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.getSchema(SchemaHandler.java:26)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.DalClient.extractSchema(DalClient.java:38)\n",
      "\t... 19 more\n",
      "Caused by: java.net.UnknownHostException: ezpresto-sts-mst-0.admin-901d042c.svc.cluster.local\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:305)\n",
      "\tat java.base/sun.security.ssl.BaseSSLSocketImpl.connect(BaseSSLSocketImpl.java:173)\n",
      "\tat java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:182)\n",
      "\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:509)\n",
      "\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:604)\n",
      "\tat java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)\n",
      "\tat java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)\n",
      "\tat java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:207)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1186)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1080)\n",
      "\tat java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:193)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1387)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1362)\n",
      "\tat java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:246)\n",
      "\tat com.ezsql.sparkconnector.utils.http.HttpUtility.sendRequest(HttpUtility.java:53)\n",
      "\tat com.ezsql.sparkconnector.utils.dal.SchemaHandler.executeSchemaQuery(SchemaHandler.java:41)\n",
      "\t... 22 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfReader = spark.read.format(\"EzPresto\")\n",
    "dfReader.option(\"presto_url\", \"https://ezpresto-sts-mst-0.admin-901d042c.svc.cluster.local:8081\")\n",
    "dfReader.option(\"dal_url\", \"https://ezpresto-sts-mst-0.admin-901d042c.svc.cluster.local:9090\")\n",
    "dfReader.option(\"ignore_ssl_check\", \"true\")\n",
    "dfReader.option(\"query\", \"SELECT * FROM retailvince.public.source_catalog\")\n",
    "\n",
    "df = dfReader.load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95ab73e-84f7-404d-aefb-7a6a9bfe6224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "invalid syntax (<stdin>, line 1)\n",
      "  File \"<stdin>\", line 1\n",
      "    !cat /opt/conda/envs/sparkmagic/lib/python3.11/site-packages/sparkmagic/magics/remotesparkmagics.py\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /opt/conda/envs/sparkmagic/lib/python3.11/site-packages/sparkmagic/magics/remotesparkmagics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490e194-9e93-4979-a2cf-3d19945cd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "presto_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:presto://ezpresto.hpepcai.ezmeral.demo.local:/retailvince/public\") \\\n",
    "    .option(\"dbtable\", \"source_catalog\") \\\n",
    "    .option(\"driver\", \"com.facebook.presto.jdbc.PrestoDriver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce65991f-da9a-4d4f-80d7-402f511afd87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c42bce-504f-46b0-a3d2-fb45db89ce21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43769c-aa66-4057-9491-599a956f3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract data from SQL tables using %sql commands\"\"\"\n",
    "print(\"Extracting data from SQL tables...\")\n",
    "\n",
    "# Extract each table and cache the DataFrames\n",
    "catalog_df = spark.sql(\"SELECT * FROM retailvince.public.source_catalog\")\n",
    "customers_df = spark.sql(\"SELECT * FROM retailvince.public.source_customers\")\n",
    "orders_df = spark.sql(\"SELECT * FROM retailvince.public.source_orders\")\n",
    "order_products_df = spark.sql(\"SELECT * FROM retailvince.public.source_order_products\")\n",
    "stock_df = spark.sql(\"SELECT * FROM retailvince.public.source_stock\")\n",
    "\n",
    "return {\n",
    "    \"catalog\": catalog_df,\n",
    "    \"customers\": customers_df,\n",
    "    \"orders\": orders_df,\n",
    "    \"order_products\": order_products_df,\n",
    "    \"stock\": stock_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a68109-8af3-4efa-bc97-12910b9634bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delta_tables(dataframes):\n",
    "    \"\"\"Create Delta tables from the extracted data\"\"\"\n",
    "    print(\"Creating Delta tables...\")\n",
    "    \n",
    "    # Write each DataFrame to Delta format\n",
    "    dataframes[\"catalog\"].write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_path}source_catalog\")\n",
    "    dataframes[\"customers\"].write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_path}source_customers\")\n",
    "    dataframes[\"orders\"].write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_path}source_orders\")\n",
    "    dataframes[\"order_products\"].write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_path}source_order_products\")\n",
    "    dataframes[\"stock\"].write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_path}source_stock\")\n",
    "    \n",
    "    print(\"Delta tables created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a714503-c57f-4c33-8a48-211e635e8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data():\n",
    "    \"\"\"Transform the data in Delta tables\"\"\"\n",
    "    print(\"Transforming data...\")\n",
    "    \n",
    "    # Example transformation: Clean product names in catalog\n",
    "    catalog_delta = DeltaTable.forPath(spark, f\"{delta_path}source_catalog\")\n",
    "    catalog_df = catalog_delta.toDF()\n",
    "    \n",
    "    # Apply transformations\n",
    "    cleaned_catalog = catalog_df.withColumn(\n",
    "        \"product_name_clean\", \n",
    "        trim(col(\"product_name\"))\n",
    "    \n",
    "    # Write transformed data back\n",
    "    cleaned_catalog.write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_path}source_catalog\")\n",
    "    \n",
    "    print(\"Data transformation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eea331-5cd1-47af-8cb8-3c42d602d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data():\n",
    "    \"\"\"Validate the data quality\"\"\"\n",
    "    print(\"Validating data...\")\n",
    "    \n",
    "    # Example validation: Check for null product names\n",
    "    null_names = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS null_names_count \n",
    "    FROM delta.`{delta_path}source_catalog` \n",
    "    WHERE product_name IS NULL OR TRIM(product_name) = ''\n",
    "    \"\"\").collect()[0][\"null_names_count\"]\n",
    "    \n",
    "    print(f\"Found {null_names} products with null or empty names\")\n",
    "    \n",
    "    # Add more validations as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868b5ae-891a-4c3f-a873-cfd06ebb04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main pipeline execution\"\"\"\n",
    "    try:\n",
    "        # Step 1: Extract data from SQL tables\n",
    "        dataframes = extract_sql_tables()\n",
    "        \n",
    "        # Step 2: Create Delta tables\n",
    "        create_delta_tables(dataframes)\n",
    "        \n",
    "        # Step 3: Transform data\n",
    "        transform_data()\n",
    "        \n",
    "        # Step 4: Validate data\n",
    "        validate_data()\n",
    "        \n",
    "        print(\"Pipeline executed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b7bac-15df-4904-815e-e9a13788d7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c9c36-ce99-4a34-815a-dce93707e6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced11b0-1ec0-4912-8272-cd7aa53aff3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb642c-ea85-4c26-8971-791e46408143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7caf631-8ccd-4b8e-97d0-fe2e5ed6c365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee671a8-62aa-4e50-aba1-c17f839dc66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3b0e2-e7d7-40c8-958a-37f88d5e7a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ec39f-2d01-4299-956e-e8746b41f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delta_tables():\n",
    "    \"\"\"Create empty Delta tables with proper schema\"\"\"\n",
    "    # Create products table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta.`{delta_path}source_catalog` (\n",
    "        product_id INT,\n",
    "        product_name STRING,\n",
    "        product_category STRING,\n",
    "        price_cents INT\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create customers table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta.`{delta_path}source_customers` (\n",
    "        customer_id INT,\n",
    "        customer_name STRING,\n",
    "        customer_surname STRING,\n",
    "        customer_email STRING\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create stock table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta.`{delta_path}source_stock` (\n",
    "        entry_id INT,\n",
    "        product_id INT,\n",
    "        product_quantity INT,\n",
    "        purchase_price_cents INT,\n",
    "        entry_date DATE\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create orders table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta.`{delta_path}source_orders` (\n",
    "        order_id INT,\n",
    "        customer_id INT,\n",
    "        order_status STRING,\n",
    "        order_date DATE\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create order products table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta.`{delta_path}source_order_products` (\n",
    "        transaction_id INT,\n",
    "        order_id INT,\n",
    "        product_id INT,\n",
    "        product_quantity INT\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Delta tables created successfully\")\n",
    "\n",
    "def add_constraints():\n",
    "    \"\"\"Add constraints and metadata after table creation\"\"\"\n",
    "    # For Delta Lake, we can add constraints as table properties or enforce them through application logic\n",
    "    # This is a placeholder for where you would implement your constraint logic\n",
    "    print(\"Constraints would be enforced here through application logic\")\n",
    "\n",
    "    # Example: You could create a function that validates data before writing\n",
    "    # or use Delta Lake's data validation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50181e5d-ce90-46af-b101-3356ff0c52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data_to_delta():\n",
    "    \"\"\"Load generated sample data to Delta tables\"\"\"\n",
    "    data = generate_sample_data()\n",
    "    \n",
    "    # Convert each dataset to Spark DataFrame and write to Delta\n",
    "    spark.createDataFrame(data[\"products\"]).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(f\"{delta_path}source_catalog\")\n",
    "    \n",
    "    spark.createDataFrame(data[\"customers\"]).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(f\"{delta_path}source_customers\")\n",
    "    \n",
    "    # Similarly for other tables...\n",
    "    print(\"Sample data loaded to Delta tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39b47d-9255-4913-ada8-f302e01e7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data():\n",
    "    \"\"\"Apply transformations to the raw data\"\"\"\n",
    "    # Example transformation: Clean product names\n",
    "    products_df = DeltaTable.forPath(spark, f\"{delta_path}source_catalog\").toDF()\n",
    "    \n",
    "    cleaned_products = products_df.withColumn(\n",
    "        \"product_name_clean\",\n",
    "        col(\"product_name\").rtrim().ltrim()\n",
    "    ).drop(\"product_name\").withColumnRenamed(\"product_name_clean\", \"product_name\")\n",
    "    \n",
    "    # Example transformation: Standardize categories\n",
    "    category_mapping = {\n",
    "        \"Toyz\": \"Toys\",\n",
    "        \"Clothng\": \"Clothing\",\n",
    "        \"Eletronics\": \"Electronics\"\n",
    "    }\n",
    "    \n",
    "    mapping_expr = \"CASE \"\n",
    "    for wrong, correct in category_mapping.items():\n",
    "        mapping_expr += f\"WHEN product_category = '{wrong}' THEN '{correct}' \"\n",
    "    mapping_expr += \"ELSE product_category END\"\n",
    "    \n",
    "    cleaned_products = cleaned_products.withColumn(\n",
    "        \"product_category_clean\",\n",
    "        expr(mapping_expr)\n",
    "    ).drop(\"product_category\").withColumnRenamed(\"product_category_clean\", \"product_category\")\n",
    "    \n",
    "    # Save transformed data to new Delta table\n",
    "    cleaned_products.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{delta_path}transformed_catalog\")\n",
    "    \n",
    "    print(\"Data transformations completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862581ee-2de8-4cb8-b590-a515d2de6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data():\n",
    "    \"\"\"Run data quality checks\"\"\"\n",
    "    # Example validation: Check for null product names\n",
    "    null_names = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS null_names_count \n",
    "    FROM delta.`{delta_path}source_catalog` \n",
    "    WHERE product_name IS NULL OR TRIM(product_name) = ''\n",
    "    \"\"\").collect()[0][\"null_names_count\"]\n",
    "    \n",
    "    print(f\"Found {null_names} products with null or empty names\")\n",
    "    \n",
    "    # Add more validations as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f836d-f944-4644-8f6e-aead30ad3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Orchestrate the data pipeline\"\"\"\n",
    "try:\n",
    "    # Step 1: Create Delta tables\n",
    "    create_delta_tables()\n",
    "    \n",
    "    # Step 2: Load raw data\n",
    "    load_raw_data_to_delta()\n",
    "    \n",
    "    # Step 3: Validate raw data\n",
    "    validate_data()\n",
    "    \n",
    "    # Step 4: Transform data\n",
    "    transform_data()\n",
    "    \n",
    "    # Step 5: Validate transformed data\n",
    "    validate_data()\n",
    "    \n",
    "    print(\"Pipeline executed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d03e4-d6ea-4a42-85cf-8f75731209d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef16f3c-7a82-4812-974f-239136c6c8fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will also define the paths for where Spark will pull files from and save files to. These paths are specific to the AIE directory structure and to be left as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087bdc8e-44d1-48ab-9471-cccb2b875e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_root = \"file:///mounts/shared-volume/shared/retail-data/raw-data\"\n",
    "delta_root = \"file:///mounts/shared-volume/shared/retail-data/delta-tables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68571ddd-0bbb-4112-8d86-f431d3b5d0be",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can now instantiate the Spark session. We'll add delta extensions to the configuration to be able to interact with the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6811656-d54e-466e-89a1-f7e9719d4b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCleaningWithSpark\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.local.dir\", \"/mnt/shared/end2end-main-exercises/exercises\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Pyspark session started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a7dbb-003b-424d-bb37-302ce6493c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Generating and Preparing Sales Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8d980-01a8-4536-a653-c0ec087d4fbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we are going to synthetically generate several years of sales data from our three retail stores located in three countries: Switzerland, Germany and the Czech Republic. This sales data will provide the basis for the remaining exercises, where we will learn to analyze, graph and build dashboards to gather insights between and across regions. \n",
    "\n",
    "**Optional:** To use `Data Sources` connected through AIE (such as MySQL, MariaDB and PostgresSQL databases), follow **this**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b5e7a-c316-482b-a980-975ffdce573b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generating Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9b9d8-c9b8-42eb-80db-04479c8e8d8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "A Python script has been provided which can generate the sales data for the three given country locations. \n",
    "\n",
    "The parameters for this script are:\n",
    "\n",
    "- cu: Currency, to account for conversions between stores.\n",
    "- s: Number of stores in that region.\n",
    "- sy: Start Year\n",
    "- ey: End Year\n",
    "- csv: Resulting File Name\n",
    "\n",
    "We'll see the first 10 rows of the newly created table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9ef1d-e4d4-4d42-acb6-c9abae749939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run resources/create_csv.py -c \"Germany\" -cu EUR -s 5 -sy 2019 -ey 2023 -csv \"germany_sales_data_2019_2023.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671a32f-fb8e-46b5-aa4d-52220cb0cb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run resources/create_csv.py -c \"Czech Republic\" -cu CZK -s 5 -sy 2019 -ey 2023 -csv \"czech_sales_data_2019_2023.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc93d56-0fd2-4c33-b77d-d709c228597a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run resources/create_csv.py -c \"Swiss\" -cu CHF -s 5 -sy 2019 -ey 2023 -csv \"swiss_sales_data_2019_2023.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf50a54e-8337-49bf-b692-21a6ed7dca49",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll ensure that our Spark Interactive session can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a51cc5-8f76-49ef-bf2d-793e4da95f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "data_path = file_root\n",
    "\n",
    "# List files in the directory\n",
    "files = spark.sparkContext.wholeTextFiles(data_path)\n",
    "\n",
    "# Display the list of files\n",
    "for file_path, _ in files.collect():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dfac2-2f13-4b17-9e01-cd7cee98f55a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Create Delta Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acafe4dd-7c94-4416-a505-fb25558a614b",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we will create Delta Tables from our CSV files that we can query using AIE. Delta Tables are a type of table that can be created in Delta Lake, which is an extension of Apache Parquet file format.\n",
    "\n",
    "### Define an ETL Pipeline to create Delta Tables \n",
    "\n",
    "First, let's define some functions that will:\n",
    "\n",
    "1. Load the data in from a CSV and return a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d95d3-5fa2-4eb4-a15c-1c1467f73713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def load_data(spark, country, data_path):\n",
    "    # Define the path to the CSV file\n",
    "    csv_path = f\"{data_path}/{country}_sales_data_2019_2023.csv\"\n",
    "\n",
    "    # Define the schema with specific data types\n",
    "    schema = StructType([\n",
    "        StructField(\"PRODUCTID\", IntegerType(), True),\n",
    "        StructField(\"PRODUCT\", StringType(), True),\n",
    "        StructField(\"TYPE\", StringType(), True),\n",
    "        StructField(\"UNITPRICE\", DoubleType(), True),\n",
    "        StructField(\"UNIT\", StringType(), True),\n",
    "        StructField(\"QTY\", IntegerType(), True),\n",
    "        StructField(\"TOTALSALES\", DoubleType(), True),\n",
    "        StructField(\"CURRENCY\", StringType(), True),\n",
    "        StructField(\"STORE\", StringType(), True),\n",
    "        StructField(\"COUNTRY\", StringType(), True),\n",
    "        StructField(\"YEAR\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read data from the CSV file with the specified schema\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(csv_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c9954-ac26-41a4-abd3-18f7b04b00ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Clean the data, in this case by ensuring the currency of each item is standardized in Euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb38a7-7fe3-4c0a-a467-4b45d9ec64f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df, spark, country):\n",
    "    # Define a UDF to convert currencies to EUR\n",
    "    convert_udf = udf(lambda currency, amount: amount / CZK_TO_EUR_RATE if currency == \"CZK\" else amount / CHF_TO_EUR_RATE if currency == \"CHF\" else amount, DoubleType())\n",
    "\n",
    "    # Apply the UDFs to the DataFrame\n",
    "    corrected_df = df.withColumn(\"totalsales\", convert_udf(col(\"currency\"), col(\"totalsales\"))) \\\n",
    "                     .withColumn(\"currency\", lit(\"EUR\"))\n",
    "\n",
    "    # Show the results\n",
    "    corrected_df.show()\n",
    "\n",
    "    return corrected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506a081-3198-4387-972b-d93770235000",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Save the data as parquet files (Delta Tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d436fd8-3f8b-4193-8db9-08f6804f79fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_data(df, country):\n",
    "    delta_path = delta_root + country\n",
    "\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(delta_path):\n",
    "        os.makedirs(delta_path)\n",
    "        \n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e149b-dbac-4d5b-9e0b-dbcee401d81b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Great! We've just created functions that will **extract** the data from our generated CSV files, **transform** them into Delta Tables with the currency standardized, then **load** them into a new directory.\n",
    "\n",
    "You guessed it! We have just created an **ETL pipeline!** \n",
    "\n",
    "After declaring our country list and our currency conversion rates, we can run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1df26-63c3-4c65-852f-6ad6159dfbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "COUNTRY_LIST = [\"czech\", \"germany\", \"swiss\"]\n",
    "CZK_TO_EUR_RATE = 25\n",
    "CHF_TO_EUR_RATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde60bbb-22de-4407-ab8f-3c34590e1e8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Hint:</b> As you can tell by the parameters to the create_csv.py functions in Section 2, we can synthetically generate data for as many stores in as many European countries as we want! Feel free to experiment, so long as the countries are declared in the cell above <b>and the countries that are already there remain.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509607bd-a86f-4252-9c6e-354709521775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for country in COUNTRY_LIST:\n",
    "    # Load data from the DBs\n",
    "    df = load_data(spark, country, data_path)\n",
    "    df.show()\n",
    "    \n",
    "    # Clean the data\n",
    "    cleaned_df = clean_data(df, spark, country)\n",
    "    cleaned_df.printSchema()\n",
    "    \n",
    "    # Write the cleaned data back to the Delta Table\n",
    "    write_data(cleaned_df, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca77167-5306-4515-aa1c-1178a42f593a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we'll confirm the Delta Tables were create correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61cd90-dfe5-42cd-ad23-9fcc25739f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for country in COUNTRY_LIST:\n",
    "    # List files in a directory\n",
    "    selected_country_path = delta_root + country\n",
    "    files = os.listdir(selected_country_path)\n",
    "    print(\"Table:\", country)\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            full_path = os.path.join(selected_country_path, file)\n",
    "            print(\"Saved in:\", full_path)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318187ca-e5ee-4846-b0a9-293efe1f59d9",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e027a2b-d412-4621-b26f-00e1ddebeda8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this exercise, you learned to perform the basics of data engineering - all within a single notebook! \n",
    "\n",
    "**HPE AI Essentials** makes this possible by natively supporting and including the most widely used open-source data tools and frameworks and making them readily available out-of-the-box, such that you spent this time performing invaluable data preperation for upcoming exercises instead of hours installing and connecting them all!\n",
    "\n",
    "In the next exercise, you will learn how to use EzPresto on HPE AI Essentials to prepare these datasets for visualization and modelling. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
