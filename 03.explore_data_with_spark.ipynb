{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801896dd-7519-4344-8cac-2a9e6ce22a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e1680-7e91-45c9-9059-be73aea14741",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> This exercise requires the completion of <a href=\"./02.query_with_ezpresto.ipynb\" <b>Exercise 2:</b> Connect and Query Data Sources with EzPresto</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bac2dc-3599-441e-ab23-d952d84d1e24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 3:** Explore Retail Data with Apache Spark\n",
    "\n",
    "This exercise will introduce **Apache Spark on HPE AI Essentials**. We'll leverage Spark's powerful distributed processing capabilities to analyze and fix the sales information.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Set up a Spark session for interacting with data.\n",
    "- Generate sample sales data for different countries and currencies.\n",
    "- Explore techniques for data loading, transformation, and analysis using Spark SQL and DataFrames.\n",
    "- Create Delta Tables and perform version control.\n",
    "\n",
    "Feel free to modify and extend the code examples to suit your specific data analysis needs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf107c4-4241-4b59-968e-c0cd1ea13f8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Prerequisites:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9744912-6776-4d76-a074-770fe2be4750",
   "metadata": {
    "tags": []
   },
   "source": [
    "As instructed in the [Introductory notebook](./00.introduction.ipynb), ensure that you have run `pip install -r requirements.txt` in a Terminal window, located in the same working directory, prior to running this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34b061-c46d-4864-ad93-36367fd76695",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Make sure you selected <b>PySpark</b> for your notebook kernel - check the top right corner!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbeea39-73eb-4ed7-9ab8-c6bd3c5aa8c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Create Spark Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366089e-a3b4-4c9e-ba53-fcbad54f0034",
   "metadata": {
    "tags": []
   },
   "source": [
    "Think about the most recent Excel spreadsheet you edited. It probably had tens or even hundreds of rows across tens of columns. When you run an Excel command, such as a *SUM()* or a *VLOOKUP()*, you may have noticed that it took a far bit of time to process. Maybe, even the fans of your laptop sped up a bit as your computer worked to crunch the numbers. \n",
    "\n",
    "Now, scale that same command out to a spreadsheet with tens of **millions** of rows across **thousands** of columns. That is the Big Data that companies must work with on a daily basis, and no single PC is going to run any *VLOOKUP* command on data of that size.\n",
    "\n",
    "Instead of spreadsheets, the enterprise world is largely built upon **tables** in a variety of formats. To query these tables to retrieve certain data takes a **mammoth** amount of compute. It makes no sense to have a single **compute server** executing these queries - it would be far faster to parallelize queries across several computers. Enter **Apache Spark**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce431084-dbf6-46c0-9378-d31139a83d2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction to Apache Spark on HPE AI Essentials\n",
    "\n",
    "Apache Spark is a popular open-source big data framework that **distributes the computations** required to perform queries on large sets of data. This distribution, along with working with data in-memory rather than directly from storage disks, drastically brings down the time usually taken to query and index data. The combination of speed, versatility, and ease of use made Spark the go-to framework when working with big data. \n",
    "\n",
    "Apache Spark comes pre-installed with **HPE AI Essentials** and can leverage as much or as little of the compute available in a AIE cluster as a user desired. The core components of an Apache Spark deployment include:\n",
    "\n",
    "<img src=\"./images/exercise1/spark_archi.PNG\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "\n",
    "**Driver:** The driver program coordinates the execution of Spark jobs. It submits tasks to executors, schedules operations, and manages communication between various components.\n",
    "\n",
    "**Workers:** These are machines in the Spark cluster that manage executors. Each worker runs one or more executors. When running Spark on a HPE AI Essentials deployment, Spark Workers are Kubernetes pods distributed among worker nodes of the AIE cluster, allowing them to scale across multiple machines as required. \n",
    "\n",
    "**Executors:** Executors reside on worker nodes and carry out the actual computations assigned by the driver program. They partition and distribute the workload across machines in the cluster.\n",
    "\n",
    "**JVM:**  Spark utilizes the Java Virtual Machine (JVM) on each worker node to execute executors.\n",
    "\n",
    "On **HPE AI Essentials**, you will use Apache Spark to analyze large datasets at high speed with a unified platform for batch processing, streaming, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be59379-6d08-426b-b759-80942d845b76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Spark Interactive Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07e337-f157-4cc2-9160-6f7cbaa69aaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's begin using Spark! Here, you use HPE AI Essentials' native integration of **Apache Livy** to create and manage an interactive Spark session. Livy is an open-source REST service that enables remote and interactive analytics on Apache Spark clusters. It provides a way to interact with Spark clusters programmatically using a REST API, allowing you to submit Spark jobs, run interactive queries, and manage Sparksessions from web applications without the need for a specific Spark client. As a result, multiple AIE users can interact with your Spark cluster concurrently and reliably!\n",
    "\n",
    "First, let's connect to the Livy endpoint and create a new Spark interactive session. The Spark interactive\n",
    "session is particularly useful for exploratory data analysis, prototyping, and iterative development. It allows you to\n",
    "interactively work with large datasets, perform transformations, apply analytical operations, and build ML models using\n",
    "Spark's distributed computing capabilities. \n",
    "\n",
    "To communicate with Livy and manage your sessions you use Sparkmagic, an open-source tool that provides a Jupyter kernel\n",
    "extension. Sparkmagic integrates with Livy, to provide the underlying communication layer between the Jupyter kernel and\n",
    "the Spark cluster.\n",
    "\n",
    "**Execute the cell below**, then:\n",
    "\n",
    "1. Select the `Add Endpoint` tab.\n",
    "1. Select `Single Sign-on` and ensure there is a Livy address in the `Address` field. \n",
    "1. Click `Add Endpoint`.\n",
    "1. Select the `Create Session` tab.\n",
    "1. Provide a name (e.g. `retail-demo`).\n",
    "1. Select `python` under the Language field.\n",
    "1. Click `Create Session` (right side).\n",
    "\n",
    "The session will take a few minutes for your session to initialize. \n",
    "\n",
    "Once ready, the Manage Sessions pane will activate, displaying\n",
    "your session ID. When the session state turns to idle, you're all set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b30ae-c564-42d3-8872-3559afa39b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf2946-32f2-40b6-88cc-0af78fed48f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's check the status of the session.\n",
    "\n",
    "1. Navigate back to the AIE dashboard.\n",
    "1. In the sidebar navigation menu, select `Spark Interactive Sessions`.\n",
    "\n",
    "![image.png](./images/exercise1/menu.PNG)\n",
    "\n",
    "3. Here, you can check the status of your session. It will take 2-3 minutes to start. When the `State` says `Idle`, the session is ready. \n",
    "\n",
    "![image.png](./images/exercise1/session.PNG)\n",
    "\n",
    "4. Scroll back up to the Notebook cell of the session (%manage_spark command). Confirm under the `Manage Sessions` tab that the session should now be visible as `Idle` too. \n",
    "\n",
    "![image.png](./images/exercise1/session2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25baec78-bb1b-4449-bbf7-98d6760ad9a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Set your <b>Username</b>, your <b>Domain</b> and the name of your <b>Presto connection</b> (catalog) here !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8788a-205f-42ff-9222-7c18efd8c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME=\"\"\n",
    "DOMAIN=\"\"\n",
    "CATALOG=f\"retail{USERNAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79c95e-d669-4575-adba-c2ce7553a120",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, let's import the required libraries for working with Spark in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08ddd1-9cb2-4cf3-a568-d64f384ad7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68571ddd-0bbb-4112-8d86-f431d3b5d0be",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can now instantiate the Spark session. We'll add delta extensions to the configuration to be able to interact with the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63045c7a-21a9-443a-8188-697576a0b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailDataPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "delta_path = f\"file:///mounts/shared-volume/shared/{USERNAME}/delta-tables/\"\n",
    "os.makedirs(delta_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61a38f-58a1-46c4-9a6c-f3c51ee944f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Create Delta Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e45a8-7146-420c-b766-4949479dcd6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we will create Delta Tables from our SQL database that we can query using EzPresto. Delta Tables are a type of table that can be created in Delta Lake, which is an extension of Apache Parquet file format.\n",
    "\n",
    "### Define an ETL Pipeline to create Delta Tables \n",
    "\n",
    "First, let's define some functions that will:\n",
    "\n",
    "1. Load the data in from a SQL database and and save it as a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44180fb8-24d9-431d-ba8d-e9ddf91196e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tables to extract\n",
    "tables = [\n",
    "    \"source_catalog\",\n",
    "    \"source_customers\",\n",
    "    \"source_orders\",\n",
    "    \"source_order_products\",\n",
    "    \"source_stock\"\n",
    "]\n",
    "\n",
    "SCHEMA=\"public\"\n",
    "\n",
    "def extract_and_save_table(table_name):\n",
    "    \"\"\"Extract a single table from Presto and save to Delta\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "        \n",
    "        # Presto connection configuration\n",
    "        uri = f\"jdbc:presto://ezpresto.{DOMAIN}:443/{CATALOG}/{SCHEMA}\"\n",
    "        query = f\"SELECT * FROM {CATALOG}.{SCHEMA}.{table_name}\"\n",
    "        \n",
    "        # Read from Presto\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"driver\", \"com.facebook.presto.jdbc.PrestoDriver\") \\\n",
    "            .option(\"url\", uri) \\\n",
    "            .option(\"SSL\", \"true\") \\\n",
    "            .option(\"IgnoreSSLChecks\", \"true\") \\\n",
    "            .option(\"query\", query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Write to Delta format\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"{delta_path}{table_name}\")\n",
    "            \n",
    "        print(f\"Successfully saved {table_name} to Delta format\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Process all tables\n",
    "for table in tables:\n",
    "    success = extract_and_save_table(table)\n",
    "    if not success:\n",
    "        print(f\"Failed to process table {table}\")\n",
    "\n",
    "print(\"All tables processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c3240-a78b-4f1b-a62f-72576edf8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which tables exist\n",
    "existing_tables = []\n",
    "for table in tables:\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(f\"{delta_path}{table}\")\n",
    "        existing_tables.append(table)\n",
    "        print(f\"Found Delta table: {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {table}: {str(e)}\")\n",
    "\n",
    "if not existing_tables:\n",
    "    print(\"\\nERROR: No Delta tables found. Please confirm:\")\n",
    "    print(f\"1. The path is correct: {delta_path}\")\n",
    "    print(\"2. The tables were created successfully\")\n",
    "    print(\"3. You have read permissions to the location\")\n",
    "else:\n",
    "    print(\"\\nTables available:\", existing_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c9954-ac26-41a4-abd3-18f7b04b00ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Clean the data, in this case by checking the spelling and dropping the `null` values. Then we save it back to the same Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518a59b-ce54-4efa-9549-d6719864a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, when, lit\n",
    "\n",
    "# Clean product names and categories\n",
    "cleaned_catalog = spark.read.format(\"delta\").load(f\"{delta_path}source_catalog\") \\\n",
    "    .withColumn(\"product_name\", trim(col(\"product_name\"))) \\\n",
    "    .withColumn(\"product_category\", \n",
    "        when(col(\"product_category\") == \"Toyz\", \"Toys\")\n",
    "        .when(col(\"product_category\") == \"Clothng\", \"Clothing\")\n",
    "        .when(col(\"product_category\") == \"Eletronics\", \"Electronics\")\n",
    "        .otherwise(col(\"product_category\"))) \\\n",
    "    .filter(col(\"product_id\").isNotNull()) \\\n",
    "    .filter(col(\"price_cents\") > 0)  # Remove negative prices\n",
    "\n",
    "cleaned_catalog.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86245d-8aaf-4016-a28e-184aab52c91c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's compare the result, but first we need to define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bee54e-230f-43f8-9ca3-1aa39105f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_table(table_name):\n",
    "    delta_table = f\"{delta_path}{table_name}\"\n",
    "    \n",
    "    # Read current version (latest)\n",
    "    df_current = spark.read.format(\"delta\").load(delta_table)\n",
    "    \n",
    "    # Find the latest version number\n",
    "    latest_version = DeltaTable.forPath(spark, delta_table).history(1).collect()[0].version\n",
    "    \n",
    "    # Read previous version (latest - 1)\n",
    "    df_previous = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version - 1).load(delta_table)\n",
    "    \n",
    "    # Show data\n",
    "    print(f\"Current Version of {table_name}:\")\n",
    "    df_current.show()\n",
    "    \n",
    "    print(f\"Previous Version {table_name}:\")\n",
    "    df_previous.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cdb6d-a057-4f0e-b5e2-748343ee7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table(\"source_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c826182-adfa-4ba4-9f24-d869b91c4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean customer data\n",
    "cleaned_customers = spark.read.format(\"delta\").load(f\"{delta_path}source_customers\") \\\n",
    "    .withColumn(\"customer_name\", trim(col(\"customer_name\"))) \\\n",
    "    .withColumn(\"customer_surname\", trim(col(\"customer_surname\"))) \\\n",
    "    .withColumn(\"customer_email\",\n",
    "        when(\n",
    "            (col(\"customer_email\").contains(\"@\")) & \n",
    "            (col(\"customer_email\").contains(\".\")),\n",
    "            col(\"customer_email\")\n",
    "        ).otherwise(lit(None))) \\\n",
    "    .filter(col(\"customer_id\").isNotNull())\n",
    "\n",
    "cleaned_customers.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a80554-57a4-4e1a-a192-376335e6ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table(\"source_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5aeda-e5e8-4caf-b00f-0a8ac4684ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Clean stock data\n",
    "cleaned_stock = spark.read.format(\"delta\").load(f\"{delta_path}source_stock\") \\\n",
    "    .filter(col(\"product_quantity\") > 0) \\\n",
    "    .filter(col(\"entry_date\") <= lit(current_date())) \\\n",
    "    .filter(col(\"product_id\").isNotNull()) \\\n",
    "    .filter(col(\"purchase_price_cents\") > 0)\n",
    "\n",
    "cleaned_stock.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8623ab-7e51-451e-88b3-8247b9f5b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table(\"source_stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba173c-e25a-4dbe-8189-83d409a60f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean orders data\n",
    "cleaned_orders = spark.read.format(\"delta\").load(f\"{delta_path}source_orders\") \\\n",
    "    .filter(col(\"order_date\") <= current_date()) \\\n",
    "    .filter(col(\"customer_id\").isNotNull()) \\\n",
    "    .withColumn(\"order_status\",\n",
    "        when(col(\"order_status\").isin([\"completed\", \"pending\", \"cancelled\", \"shipped\"]),\n",
    "            col(\"order_status\")\n",
    "        ).otherwise(lit(\"pending\")))\n",
    "\n",
    "cleaned_orders.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabf6e5-fe1b-470b-b075-cf543d0a897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table(\"source_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8f5aa-a8f4-4920-9ce0-8c271ba0777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean order products data\n",
    "cleaned_order_products = spark.read.format(\"delta\").load(f\"{delta_path}source_order_products\") \\\n",
    "    .filter(col(\"product_quantity\") > 0) \\\n",
    "    .filter(col(\"order_id\").isNotNull()) \\\n",
    "    .filter(col(\"product_id\").isNotNull())\n",
    "\n",
    "cleaned_order_products.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{delta_path}source_order_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d98a4a-9184-4473-8e04-3a7caf9001e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table(\"source_order_products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e149b-dbac-4d5b-9e0b-dbcee401d81b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Great! We've just created functions that will **extract** the data from our SQL tables, **transform** them to get rid of inconsistencies, then **load** them as Delta Table into a new directory.\n",
    "\n",
    "You guessed it! We have just created an **ETL pipeline!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70002a-d392-432c-ae72-5412f6586516",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Connecting the Data Source**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9716e-6b89-4659-b5e3-ade0f8d1b830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect Delta Tables as Data Source using Apache Hive.\n",
    "\n",
    "Let's take those Delta Tables you just and make them available to other applications in AI Essentials by connecting them as an **Data Source**. Apache Hive gives an SQL-like interface to query data stored in various databases and file systems, like the delta tables you created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f44ba-5e8f-4034-bc6c-7430b6bad567",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Navigate back to the AI Essentials dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Engineering` > `Data Sources`.\n",
    "1. Under the `Structured Data` tab, click `Add New Data Source`.\n",
    "1. Under **Hive**, click `Create Connection`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab4ca8-ddad-48e7-a4e7-2de7cc8540c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Name**: `deltavince`\n",
    "- **Hive Metastore:** `discovery`\n",
    "- **Data Dir:** `file:/data/shared/vince/delta-tables/`\n",
    "- **File Type:** `PARQUET`\n",
    "5. Click `Connect`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41cd7d8-be8d-45a0-8e0f-4c6a7ce33891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dt_connection_details():\n",
    "    \"\"\"Display database connection details without connecting\"\"\"\n",
    "    connection_details = {\n",
    "        \"Name\": f\"delta{USERNAME}\",\n",
    "        \"Hive Metastore\": \"deltadiscovery\",\n",
    "        \"Data Dir\": f\"{delta_path}\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDelta Table Connection Details:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    max_key_length = max(len(key) for key in connection_details.keys())\n",
    "    \n",
    "    for key, value in connection_details.items():\n",
    "        print(f\"{key.title():>{max_key_length}} : {value}\")\n",
    "    \n",
    "    print(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "display_dt_connection_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c201ed0-38a1-41cc-bad2-f1ff04d571e9",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad8851-745e-49f5-97a0-15d0a2e65cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this exercise, you learned to perform the basics of data engineering - all within a single notebook! \n",
    "\n",
    "**HPE AI Essentials** makes this possible by natively supporting and including the most widely used open-source data tools and frameworks and making them readily available out-of-the-box, such that you spent this time performing invaluable data preperation for upcoming exercises instead of hours installing and connecting them all!\n",
    "\n",
    "In the next exercise, you will learn how to use this data to build your first AI Agent\n",
    "\n",
    "Next: <a href=\"./04.create_AI_agent.ipynb\" style=\"color: black\"><b style=\"color: #01a982;\">Exercise 4:</b> Creating an AI Agent to analyze the data</a>\n",
    "\n",
    "Previous: <a href=\"./02.query_with_ezpresto.ipynb\" style=\"color: black\"><b style=\"color: #01a982;\">Exercise 2:</b> Connect and Query Data Sources with EzPresto</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b98f0-a764-419d-abea-43798fb27efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
