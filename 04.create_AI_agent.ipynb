{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be01229-0819-4586-8d10-32c2a0513ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925c1f9-322a-4053-8a64-7c5f91ca3c00",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> This exercise requires the completion of <a href=\"./03.explore_data_with_spark.ipynb\" <b>Exercise 3:</b> Explore Retail Data with Apache Spark</a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806b008-9c4f-42f5-838a-5b42b0d5188e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 4:** Creating an AI Agent to analyze the data.\n",
    "\n",
    "In this exercise, you'll explore how to harness the power of HPE Private Cloud AI’s **NVIDIA Inference Microservices (NIM)**, featuring Meta's **Llama 3.1 8b Instruct**, to create your very own **AI-powered Data Analyst Agent**. This agent will interact with your prepared data and help you analyze, summarize, and derive insights—all with natural language.\n",
    "\n",
    "HPE PCAI provides scalable, containerized access to state-of-the-art models like Llama 3.1, enabling low-latency, high-throughput inferencing—perfect for building intelligent agents that can reason over structured and unstructured data.\n",
    "\n",
    "Your journey in this exercise will include:\n",
    "- Integrating your previously prepared datasets with the inference workflow.\n",
    "- Configuring your AI agent so that it leverages Llama 3.1 8b via NVIDIA Inference Microservices.\n",
    "- Crafting prompts and building logic for your AI agent to act like a data analyst.\n",
    "- Interacting with your AI agent using natural language within a Jupyter notebook.\n",
    "\n",
    "By the end of this exercise, you’ll be able to prototype a lightweight, intelligent AI assistant that can query, explain, and generate insights—turning raw data into valuable knowledge with just a few prompts.\n",
    "\n",
    "Let’s get started and build your first Data Analyst AI Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf552cc0-f949-4d48-9637-d5f987603046",
   "metadata": {},
   "source": [
    "## **1. Agent Configuration**\n",
    "\n",
    "This section covers the configuration of the agent, including:  \n",
    "* Defining the data context that the agent will interact with  \n",
    "* Setting up the routine the agent will follow as a system prompt (embedding the data context)  \n",
    "* Establishing the list of tools available for the agent to complete its tasks  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f77b6b-690e-4b2c-a3ac-5b217c27f684",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Set your <b>Username</b>, your <b>Domain</b> and the name of your <b>Presto connection</b> (catalog) here !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa58ce2-9ae7-4047-b1bb-958fa169aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME=\"\"\n",
    "DOMAIN=\"\"\n",
    "CATALOG=f\"delta{USERNAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a772a-0693-4250-9194-efeba5560dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Import Librairies\n",
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "import json\n",
    "import inspect\n",
    "from pandas import DataFrame\n",
    "SCHEMA=\"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78926158-0aac-4cef-a095-4832f83c7cc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "This function will retrieve and refresh the NVIDIA JWT authentication token from a secure file path, as the token expires every 30 minutes and must be updated regularly to maintain API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd55a4-cf53-41c5-964e-b5efaa2b0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read JWT Token\n",
    "def get_nvidia_auth_token():\n",
    "    %update_token\n",
    "    token_path = Path(\"/etc/secrets/ezua/.auth_token\")\n",
    "    if token_path.exists():\n",
    "        with open(token_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    raise ValueError(\"NVIDIA auth token not found at /etc/secrets/ezua/.auth_token\")\n",
    "\n",
    "nvidia_api_key = get_nvidia_auth_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cd8ad-3e27-4730-99ee-0d0120b6fd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "Initialize an NVIDIA LLM client with specific parameters (like model, URL, and API key) and assign it to `Settings.llm` for use throughout the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e024707-9008-461e-8a6f-f5a38eb561a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Set your <b>base_url</b> here !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8fde2-45de-42d3-9468-903e4081898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NVIDIA NIM Setup\n",
    "llm = NVIDIA(\n",
    "    base_url=\"https://llama-3-1-8b-6efc4543-predictor-ezai-services.hpepcai-ingress.pcai.hpecic.net/v1\",\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    api_key=nvidia_api_key,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470d2f2-463c-4d09-ae44-bf3b8c73e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import presto\n",
    "from pandas import DataFrame\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e6a1c-dd02-4391-9b17-df7df5547e03",
   "metadata": {
    "tags": []
   },
   "source": [
    "This function will establish and return a Presto connection using specified host, port, catalog, and schema settings, with HTTPS and SSL verification disabled `IgnoreSSLChecks=true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35795f-c65e-4740-9585-1e0d9465e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presto_connection():\n",
    "    return presto.connect(\n",
    "        host=f\"ezpresto.{DOMAIN}\",\n",
    "        port=443,\n",
    "        catalog=CATALOG,\n",
    "        schema=SCHEMA,\n",
    "        protocol='https',\n",
    "        requests_kwargs={\n",
    "            'verify': False\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc089c-8d8f-49d7-8be6-2d2bec3a9be2",
   "metadata": {
    "tags": []
   },
   "source": [
    "This function will query the Delta table schema from Presto, fetch column metadata for a specific schema, convert the results into a JSON-formatted DataFrame, and handle connection cleanup and errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641dfe4-1b73-41e8-ab8d-e928d77a1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Delta Table Schema Query (fixed connection handling)\n",
    "def query_delta_dictionary():\n",
    "    query = f'''\n",
    "    SELECT \n",
    "        table_schema as \"DatabaseName\",\n",
    "        table_name as \"TableName\", \n",
    "        column_name as \"ColumnName\",\n",
    "        data_type as \"ColumnType\"\n",
    "    FROM {CATALOG}.information_schema.columns\n",
    "    WHERE table_schema NOT IN ('information_schema', 'sys')\n",
    "      AND table_schema = '{SCHEMA}'\n",
    "    '''\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_presto_connection()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        table_dictionary = DataFrame(\n",
    "            results, \n",
    "            columns=[\"DatabaseName\", \"TableName\", \"ColumnName\", \"ColumnType\"]\n",
    "        )\n",
    "        return json.dumps(table_dictionary.to_json())\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Connection failed: {str(e)}\"})\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2269d7f-e989-457a-aa03-4ff273ee5d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define a detailed system prompt that guides the LLM to act as a data analyst, using Delta table metadata and Presto SQL to answer business questions with optimized read-only queries and clear, user-friendly responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a9da2-2aef-4299-a631-d33eff1bcc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. System Prompt Setup\n",
    "db_dictionary = query_delta_dictionary()\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an advanced data analyst for a retailer company, specializing in analyzing data from our Delta Lake tables accessed via Presto. Your primary responsibility is to assist users by answering business-related questions using SQL queries. Follow these steps:\n",
    "\n",
    "1. Understanding User Requests\n",
    "   - Users provide business questions in plain English.\n",
    "   - Extract relevant data points needed to construct a meaningful response.\n",
    "\n",
    "2. Generating SQL Queries\n",
    "   - Construct an optimized Presto SQL query to retrieve the necessary data from Delta tables.\n",
    "   - The query must be a **single-line string** without carriage returns or line breaks.\n",
    "   - Ensure the query uses proper catalog.schema.table references (format: {CATALOG}.{SCHEMA}.table_name)\n",
    "   - The metadata of available tables and columns is in this json structure: \n",
    "     {db_dictionary}\n",
    "   - Apply appropriate filtering, grouping, and ordering to enhance performance.\n",
    "   - Presto-specific considerations:\n",
    "     * Use `DATE()` for date casting instead of `::date`\n",
    "     * String concatenation uses `||` not `+`\n",
    "     * For approximate counts, consider `approx_distinct()` \n",
    "   - Don't display the SQL queries unless specifically asked\n",
    "\n",
    "3. Executing the Query\n",
    "   - Run the SQL query on our Presto system and retrieve the results efficiently.\n",
    "\n",
    "4. Responding to the User\n",
    "   - Convert the query results into a **concise, insightful, and plain-English response**.\n",
    "   - Present the information in a clear, structured, and user-friendly manner.\n",
    "   - For large results, consider summarizing trends instead of listing all data points.\n",
    "\n",
    "You have access to these tools:\n",
    "- `query_delta_database`: For executing Presto SQL queries on Delta tables\n",
    "- `query_delta_dictionary`: For fetching metadata about tables and columns\n",
    "\n",
    "Always use `query_delta_database` when the user asks for data stored in our Delta tables.\n",
    "Important: Never suggest queries that would modify data - we only allow read operations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e77f9-a96f-4fed-939a-d9d90f805cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "This function will execute a sanitized Presto SQL query on Delta tables—automatically adding catalog and schema if missing—then return the results as JSON, handling connections and errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24762011-6171-4861-87d6-5515899d5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Query Delta Tables (fixed connection handling)\n",
    "def query_delta_database(sql_statement):\n",
    "    try:\n",
    "        query_statement = sql_statement.strip().replace('\\n', ' ')\n",
    "        \n",
    "        # Auto-add catalog.schema prefix if missing\n",
    "        if 'FROM ' in query_statement and '.' not in query_statement.split('FROM ')[1].split()[0]:\n",
    "            table_ref = query_statement.split('FROM ')[1].split()[0]\n",
    "            query_statement = query_statement.replace(\n",
    "                f'FROM {table_ref}', \n",
    "                f'FROM {CATALOG}.{SCHEMA}.{table_ref}'\n",
    "            )\n",
    "        \n",
    "        conn = None\n",
    "        try:\n",
    "            conn = get_presto_connection()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query_statement)\n",
    "            \n",
    "            if cursor.description:\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                data = cursor.fetchall()\n",
    "                df = DataFrame(data, columns=columns)\n",
    "                return json.dumps(df.to_dict(orient='records'))\n",
    "            else:\n",
    "                return json.dumps({\"message\": \"Query executed successfully\"})\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6096e8a2-ab39-4986-b1a8-e0903215fa86",
   "metadata": {
    "tags": []
   },
   "source": [
    "This function will send the system prompt you just created and user query as messages to the LLM, then return its chat-based response as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631787d-99e0-47ab-a482-cda280941afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Agent Conversation Function\n",
    "def run_agent_conversation(user_query):\n",
    "    from llama_index.core.llms import ChatMessage\n",
    "    \n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=system_prompt),\n",
    "        ChatMessage(role=\"user\", content=user_query)\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    return str(response)  # Changed from response.content to str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b1e61-1346-4579-8c4b-0a2e1caa9b23",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we invoke the agent with a natural language question to generate and execute a SQL-based analysis, then print the response.\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede4a71-1c22-4303-a38c-7c5c97bc172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Example Usage (with proper string termination)\n",
    "response = run_agent_conversation(\"What are the top 5 selling products by revenue?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd8478-1b5d-48ab-936e-cedf6f55cab5",
   "metadata": {},
   "source": [
    "## 2. Agent Runtime\n",
    "This section covers the code executed while the agent is in action, including:\n",
    "* Preparing the tools for use by the agent\n",
    "* The agent's runtime function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61d57f-12be-43f5-8459-937b20ec1dc3",
   "metadata": {},
   "source": [
    "This function will convert a Python function's signature and docstring into an NVIDIA-compatible tool schema by extracting parameter types, requirements, and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54a832-be6b-493d-b450-7b81f32d3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Callable\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "def function_to_schema(func: Callable) -> Dict[str, Any]:\n",
    "    \"\"\"Convert a Python function to a tool schema compatible with NVIDIA LLM\n",
    "    \n",
    "    Args:\n",
    "        func: The Python function to convert\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the function schema in NVIDIA-compatible format\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(func)\n",
    "    docstring = inspect.getdoc(func) or \"\"\n",
    "    \n",
    "    # Extract parameter information\n",
    "    parameters = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},\n",
    "        \"required\": []\n",
    "    }\n",
    "    \n",
    "    for name, param in sig.parameters.items():\n",
    "        if name == \"self\":\n",
    "            continue\n",
    "            \n",
    "        param_type = \"string\"  # default type\n",
    "        if param.annotation != inspect.Parameter.empty:\n",
    "            if param.annotation == str:\n",
    "                param_type = \"string\"\n",
    "            elif param.annotation == int:\n",
    "                param_type = \"integer\"\n",
    "            elif param.annotation == float:\n",
    "                param_type = \"number\"\n",
    "            elif param.annotation == bool:\n",
    "                param_type = \"boolean\"\n",
    "        \n",
    "        parameters[\"properties\"][name] = {\n",
    "            \"type\": param_type,\n",
    "            \"description\": \"\"  # Can be enhanced with parameter-specific docs\n",
    "        }\n",
    "        \n",
    "        if param.default == inspect.Parameter.empty:\n",
    "            parameters[\"required\"].append(name)\n",
    "    \n",
    "    return {\n",
    "        \"name\": func.__name__,\n",
    "        \"description\": docstring,\n",
    "        \"parameters\": parameters\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705c718-d767-4b7b-9172-194646741aa6",
   "metadata": {},
   "source": [
    "First, we need to prepare the agent tools by creating tool schemas, mapping functions to their names, and defining a mechanism to execute tool calls with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa1776-d549-4551-9aaa-8db5b33a3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Prepare Tools for Agent\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "tools = [query_delta_database]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "tools_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "def execute_tool_call(tool_call, tools_map):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"Assistant: {name}({args})\")\n",
    "\n",
    "    # call corresponding function with provided arguments\n",
    "    return tools_map[name](**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d8800-7ca5-496e-9ecd-04b05990ceda",
   "metadata": {},
   "source": [
    "Then, we convert a dictionary message into a LlamaIndex `ChatMessage` object, including optional additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc60047-b1c7-4ce2-b15c-e846a09316ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chat_message(message: Dict[str, Any]) -> ChatMessage:\n",
    "    \"\"\"Convert dictionary message to LlamaIndex ChatMessage\"\"\"\n",
    "    return ChatMessage(\n",
    "        role=message[\"role\"],\n",
    "        content=message[\"content\"],\n",
    "        additional_kwargs=message.get(\"additional_kwargs\", {})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e367c-6a17-40be-8750-e5940de428b9",
   "metadata": {},
   "source": [
    "Finally, we handle a full conversation turn by streaming and displaying tokenized responses from the LLM, processing tool calls if needed, and appending the results to the message history for further interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96f65c-3b6b-4984-81d1-b499d92ce970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "def run_full_turn(system_message: str, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    chat_messages = [convert_to_chat_message(msg) for msg in messages]\n",
    "    \n",
    "    while True:\n",
    "        # Initialize streaming\n",
    "        full_response = []\n",
    "        response_buffer = \"\"\n",
    "        out = display(Markdown(\"\"), display_id=True)\n",
    "        \n",
    "        # Get streaming response with token awareness\n",
    "        response_stream = llm.stream_chat(\n",
    "            chat_messages,\n",
    "            max_tokens=4096,  # Adjust based on your model's limits\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Process stream with token-aware chunking\n",
    "        for chunk in response_stream:\n",
    "            content = chunk.delta\n",
    "            if content:\n",
    "                response_buffer += content\n",
    "                full_response.append(content)\n",
    "                \n",
    "                # Display when we hit natural breaks or every 20 tokens\n",
    "                if len(response_buffer.split()) >= 20 or content.endswith(('\\n', '.', '!', '?')):\n",
    "                    out.update(Markdown(\"\".join(full_response)))\n",
    "                    response_buffer = \"\"\n",
    "                    time.sleep(0.05)  # Natural reading speed\n",
    "        \n",
    "        # Final update to ensure complete display\n",
    "        out.update(Markdown(\"\".join(full_response)))\n",
    "        \n",
    "        # Store complete response\n",
    "        response_dict = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\".join(full_response),\n",
    "            \"additional_kwargs\": getattr(response_stream, \"additional_kwargs\", {})\n",
    "        }\n",
    "        messages.append(response_dict)\n",
    "        \n",
    "        # Handle tool calls (unchanged)\n",
    "        additional_kwargs = response_dict.get(\"additional_kwargs\", {})\n",
    "        if \"tool_calls\" in additional_kwargs:\n",
    "            for tool_call in additional_kwargs[\"tool_calls\"]:\n",
    "                result = execute_tool_call(tool_call, tools_map)\n",
    "                result_message = {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": result,\n",
    "                    \"tool_call_id\": tool_call.get(\"id\", \"\"),\n",
    "                    \"name\": tool_call[\"function\"][\"name\"]\n",
    "                }\n",
    "                messages.append(result_message)\n",
    "                chat_messages.append(convert_to_chat_message(result_message))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc0457-9e29-4746-908a-496419536425",
   "metadata": {},
   "source": [
    "## 3. Running the Agent\n",
    "\n",
    "Congratulations! You've created an AI agent!\n",
    "\n",
    "Now, let's try it out!\n",
    "\n",
    "### Sample Questions:\n",
    "1. What are our top-selling products by revenue and quantity sold?\n",
    "2. Who are our top 10 customers by total spend and order frequency?\n",
    "3. Which products have the lowest stock levels relative to their sales velocity?\n",
    "4. Which product categories generate the highest profit margins?\n",
    "5. What is our order fulfillment rate and average time to fulfill orders?\n",
    "6. How has our customer base grown over time?\n",
    "7. What are the seasonal trends in our product categories?\n",
    "8. What products are frequently purchased together?\n",
    "9. What percentage of customers make repeat purchases?\n",
    "10. Which customer segments are most profitable when considering acquisition cost and lifetime value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed733b2f-9fd1-440f-83b3-509291f44141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated imports\n",
    "from typing import AsyncIterator, Iterator\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 10. Prepare Tools for Agent (unchanged)\n",
    "tools = [query_delta_database]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "tools_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "# Modified agent interaction with streaming\n",
    "def run_agent_interaction():\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nUser (type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        messages = run_full_turn(system_prompt, messages)\n",
    "        \n",
    "        # Display any tool results\n",
    "        for msg in reversed(messages):\n",
    "            if msg.get(\"role\") == \"tool\" and \"content\" in msg:\n",
    "                print(f\"\\n[Database Result]: {msg['content']}\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_agent_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfac0-f760-45ed-9c7f-ecc7d86535a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Conclusion**\n",
    "\n",
    "Great job building your first **AI-powered Data Analyst Agent** with **HPE Private Cloud AI** and **NVIDIA Inference Microservices** (NIM) featuring Meta’s **Llama 3.1 8b Instruct**!\n",
    "\n",
    "In this exercise, you successfully:\n",
    "- Integrated your datasets with the AI inference pipeline\n",
    "- Configured and deployed a responsive, containerized LLM-powered agent\n",
    "- Designed smart prompting strategies for data analysis and summarization\n",
    "- Interacted with the agent in natural language via Jupyter notebooks\n",
    "- Transformed complex datasets into clear, actionable insights with ease\n",
    "\n",
    "By leveraging PCAI’s scalable infrastructure and the reasoning power of Llama 3.1, you've prototyped a modern, intelligent assistant capable of bridging the gap between data and decisions.\n",
    "\n",
    "From prompt to insight, you’ve created a data analyst that works at the speed of thought.\n",
    "\n",
    "Next: <a href=\"./05.cleanup.ipynb\" style=\"color: black\"><b style=\"color: #01a982;\">Cleanup</b></a>\n",
    "\n",
    "Previous: <a href=\"./03.explore_data_with_spark.ipynb\" style=\"color: black\"><b style=\"color: #01a982;\">Exercise 3:</b> Explore Retail Data with Apache Spark</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
