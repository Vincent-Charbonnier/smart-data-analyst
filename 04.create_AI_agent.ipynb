{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be01229-0819-4586-8d10-32c2a0513ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925c1f9-322a-4053-8a64-7c5f91ca3c00",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> This exercise requires the completion of <a href=\"./03.explore_data_with_spark\" <b>Exercise 3:</b> Explore Retail Data with Apache Spark</a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806b008-9c4f-42f5-838a-5b42b0d5188e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 4:** Creating an AI Agent to analyze the data.\n",
    "\n",
    "In this exercise, you'll explore how to harness the power of HPE Private Cloud AI’s **NVIDIA Inference Microservices (NIM)**, featuring Meta's **Llama 3.1 8b Instruct**, to create your very own **AI-powered Data Analyst Agent**. This agent will interact with your prepared data and help you analyze, summarize, and derive insights—all with natural language.\n",
    "\n",
    "HPE PCAI provides scalable, containerized access to state-of-the-art models like Llama 3.1, enabling low-latency, high-throughput inferencing—perfect for building intelligent agents that can reason over structured and unstructured data.\n",
    "\n",
    "Your journey in this exercise will include:\n",
    "- Integrating your previously prepared datasets with the inference workflow.\n",
    "- Configuring your AI agent so that it leverages Llama 3.1 8b via NVIDIA Inference Microservices.\n",
    "- Crafting prompts and building logic for your AI agent to act like a data analyst.\n",
    "- Interacting with your AI agent using natural language within a Jupyter notebook.\n",
    "\n",
    "By the end of this exercise, you’ll be able to prototype a lightweight, intelligent AI assistant that can query, explain, and generate insights—turning raw data into valuable knowledge with just a few prompts.\n",
    "\n",
    "Let’s get started and build your first Data Analyst AI Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf552cc0-f949-4d48-9637-d5f987603046",
   "metadata": {},
   "source": [
    "## **1. Agent Configuration**\n",
    "\n",
    "This section covers the configuration of the agent, including:  \n",
    "* Defining the data context that the agent will interact with  \n",
    "* Setting up the routine the agent will follow as a system prompt (embedding the data context)  \n",
    "* Establishing the list of tools available for the agent to complete its tasks  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f77b6b-690e-4b2c-a3ac-5b217c27f684",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Set your <b>Username</b>, your <b>Domain</b> and the name of your <b>Presto connection</b> (catalog) here !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa58ce2-9ae7-4047-b1bb-958fa169aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME=\"\"\n",
    "DOMAIN=\"\"\n",
    "CATALOG=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a772a-0693-4250-9194-efeba5560dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Import Librairies\n",
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "import json\n",
    "import inspect\n",
    "from pandas import DataFrame\n",
    "SCHEMA=\"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78926158-0aac-4cef-a095-4832f83c7cc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "We start by defining a function to retrieve and refresh the NVIDIA JWT authentication token from a secure file path, as the token expires every 30 minutes and must be updated regularly to maintain API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd55a4-cf53-41c5-964e-b5efaa2b0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read JWT Token\n",
    "def get_nvidia_auth_token():\n",
    "    %update_token\n",
    "    token_path = Path(\"/etc/secrets/ezua/.auth_token\")\n",
    "    if token_path.exists():\n",
    "        with open(token_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    raise ValueError(\"NVIDIA auth token not found at /etc/secrets/ezua/.auth_token\")\n",
    "\n",
    "nvidia_api_key = get_nvidia_auth_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cd8ad-3e27-4730-99ee-0d0120b6fd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "We start by defining a function to retrieve and refresh the NVIDIA JWT authentication token from a secure file path, as the token expires every 30 minutes and must be updated regularly to maintain API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8fde2-45de-42d3-9468-903e4081898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NVIDIA NIM Setup\n",
    "llm = NVIDIA(\n",
    "    base_url=\"https://llama-3-1-8b-0b33052f-predictor-ezai-services.hpepcai.ezmeral.demo.local/v1\",\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    api_key=nvidia_api_key,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470d2f2-463c-4d09-ae44-bf3b8c73e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import presto\n",
    "from pandas import DataFrame\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35795f-c65e-4740-9585-1e0d9465e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Presto Connection\n",
    "def get_presto_connection():\n",
    "    return presto.connect(\n",
    "        host=f\"ezpresto.{DOMAIN}\",\n",
    "        port=443,\n",
    "        catalog=CATALOG,\n",
    "        schema=SCHEMA,\n",
    "        protocol='https'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641dfe4-1b73-41e8-ab8d-e928d77a1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Delta Table Schema Query (fixed connection handling)\n",
    "def query_delta_dictionary():\n",
    "    query = f'''\n",
    "    SELECT \n",
    "        table_schema as \"DatabaseName\",\n",
    "        table_name as \"TableName\", \n",
    "        column_name as \"ColumnName\",\n",
    "        data_type as \"ColumnType\"\n",
    "    FROM {CATALOG}.information_schema.columns\n",
    "    WHERE table_schema NOT IN ('information_schema', 'sys')\n",
    "      AND table_schema = '{SCHEMA}'\n",
    "    '''\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_presto_connection()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        table_dictionary = DataFrame(results, \n",
    "                                   columns=[\"DatabaseName\", \"TableName\", \"ColumnName\", \"ColumnType\"])\n",
    "        return json.dumps(table_dictionary.to_json())\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a9da2-2aef-4299-a631-d33eff1bcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. System Prompt Setup\n",
    "db_dictionary = query_delta_dictionary()\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an advanced data analyst for a retailer company, specializing in analyzing data from our Delta Lake tables accessed via Presto. Your primary responsibility is to assist users by answering business-related questions using SQL queries. Follow these steps:\n",
    "\n",
    "1. Understanding User Requests\n",
    "   - Users provide business questions in plain English.\n",
    "   - Extract relevant data points needed to construct a meaningful response.\n",
    "\n",
    "2. Generating SQL Queries\n",
    "   - Construct an optimized Presto SQL query to retrieve the necessary data from Delta tables.\n",
    "   - The query must be a **single-line string** without carriage returns or line breaks.\n",
    "   - Ensure the query uses proper catalog.schema.table references (format: {CATALOG}.{SCHEMA}.table_name)\n",
    "   - The metadata of available tables and columns is in this json structure: \n",
    "     {db_dictionary}\n",
    "   - Apply appropriate filtering, grouping, and ordering to enhance performance.\n",
    "   - Presto-specific considerations:\n",
    "     * Use `DATE()` for date casting instead of `::date`\n",
    "     * String concatenation uses `||` not `+`\n",
    "     * For approximate counts, consider `approx_distinct()` \n",
    "   - Don't display the SQL queries unless specifically asked\n",
    "\n",
    "3. Executing the Query\n",
    "   - Run the SQL query on our Presto system and retrieve the results efficiently.\n",
    "\n",
    "4. Responding to the User\n",
    "   - Convert the query results into a **concise, insightful, and plain-English response**.\n",
    "   - Present the information in a clear, structured, and user-friendly manner.\n",
    "   - For large results, consider summarizing trends instead of listing all data points.\n",
    "\n",
    "You have access to these tools:\n",
    "- `query_delta_database`: For executing Presto SQL queries on Delta tables\n",
    "- `query_delta_dictionary`: For fetching metadata about tables and columns\n",
    "\n",
    "Always use `query_delta_database` when the user asks for data stored in our Delta tables.\n",
    "Important: Never suggest queries that would modify data - we only allow read operations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24762011-6171-4861-87d6-5515899d5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Query Delta Tables (fixed connection handling)\n",
    "def query_delta_database(sql_statement):\n",
    "    try:\n",
    "        query_statement = sql_statement.strip().replace('\\n', ' ')\n",
    "        \n",
    "        if 'FROM ' in query_statement and '.' not in query_statement.split('FROM ')[1].split()[0]:\n",
    "            table_ref = query_statement.split('FROM ')[1].split()[0]\n",
    "            query_statement = query_statement.replace(\n",
    "                f'FROM {table_ref}', \n",
    "                f'FROM {CATALOG}.{SCHEMA}.{table_ref}'\n",
    "            )\n",
    "        \n",
    "        conn = None\n",
    "        try:\n",
    "            conn = get_presto_connection()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query_statement)\n",
    "            \n",
    "            if cursor.description:\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                data = cursor.fetchall()\n",
    "                df = DataFrame(data, columns=columns)\n",
    "                return json.dumps(df.to_dict(orient='records'))\n",
    "            else:\n",
    "                return json.dumps({\"message\": \"Query executed successfully\"})\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631787d-99e0-47ab-a482-cda280941afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Agent Conversation Function\n",
    "def run_agent_conversation(user_query):\n",
    "    from llama_index.core.llms import ChatMessage\n",
    "    \n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=system_prompt),\n",
    "        ChatMessage(role=\"user\", content=user_query)\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    return str(response)  # Changed from response.content to str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede4a71-1c22-4303-a38c-7c5c97bc172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Example Usage (with proper string termination)\n",
    "response = run_agent_conversation(\"What are the top 5 selling products by revenue?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd8478-1b5d-48ab-936e-cedf6f55cab5",
   "metadata": {},
   "source": [
    "## Agent Runtime\n",
    "This section covers the code executed while the agent is in action, including:\n",
    "* Preparing the tools for use by the agent\n",
    "* The agent's runtime function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54a832-be6b-493d-b450-7b81f32d3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Callable\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "def function_to_schema(func: Callable) -> Dict[str, Any]:\n",
    "    \"\"\"Convert a Python function to a tool schema compatible with NVIDIA LLM\n",
    "    \n",
    "    Args:\n",
    "        func: The Python function to convert\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the function schema in NVIDIA-compatible format\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(func)\n",
    "    docstring = inspect.getdoc(func) or \"\"\n",
    "    \n",
    "    # Extract parameter information\n",
    "    parameters = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},\n",
    "        \"required\": []\n",
    "    }\n",
    "    \n",
    "    for name, param in sig.parameters.items():\n",
    "        if name == \"self\":\n",
    "            continue\n",
    "            \n",
    "        param_type = \"string\"  # default type\n",
    "        if param.annotation != inspect.Parameter.empty:\n",
    "            if param.annotation == str:\n",
    "                param_type = \"string\"\n",
    "            elif param.annotation == int:\n",
    "                param_type = \"integer\"\n",
    "            elif param.annotation == float:\n",
    "                param_type = \"number\"\n",
    "            elif param.annotation == bool:\n",
    "                param_type = \"boolean\"\n",
    "        \n",
    "        parameters[\"properties\"][name] = {\n",
    "            \"type\": param_type,\n",
    "            \"description\": \"\"  # Can be enhanced with parameter-specific docs\n",
    "        }\n",
    "        \n",
    "        if param.default == inspect.Parameter.empty:\n",
    "            parameters[\"required\"].append(name)\n",
    "    \n",
    "    return {\n",
    "        \"name\": func.__name__,\n",
    "        \"description\": docstring,\n",
    "        \"parameters\": parameters\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5642a-f01b-4462-ab54-b23187c998d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Prepare Tools for Agent\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "tools = [query_postgres_database]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "tools_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "def execute_tool_call(tool_call, tools_map):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"Assistant: {name}({args})\")\n",
    "\n",
    "    # call corresponding function with provided arguments\n",
    "    return tools_map[name](**args)\n",
    "\n",
    "def convert_to_chat_message(message: Dict[str, Any]) -> ChatMessage:\n",
    "    \"\"\"Convert dictionary message to LlamaIndex ChatMessage\"\"\"\n",
    "    return ChatMessage(\n",
    "        role=message[\"role\"],\n",
    "        content=message[\"content\"],\n",
    "        additional_kwargs=message.get(\"additional_kwargs\", {})\n",
    "    )\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "def run_full_turn(system_message: str, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    chat_messages = [convert_to_chat_message(msg) for msg in messages]\n",
    "    \n",
    "    while True:\n",
    "        # Initialize streaming\n",
    "        full_response = []\n",
    "        response_buffer = \"\"\n",
    "        out = display(Markdown(\"\"), display_id=True)\n",
    "        \n",
    "        # Get streaming response with token awareness\n",
    "        response_stream = llm.stream_chat(\n",
    "            chat_messages,\n",
    "            max_tokens=4096,  # Adjust based on your model's limits\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Process stream with token-aware chunking\n",
    "        for chunk in response_stream:\n",
    "            content = chunk.delta\n",
    "            if content:\n",
    "                response_buffer += content\n",
    "                full_response.append(content)\n",
    "                \n",
    "                # Display when we hit natural breaks or every 20 tokens\n",
    "                if len(response_buffer.split()) >= 20 or content.endswith(('\\n', '.', '!', '?')):\n",
    "                    out.update(Markdown(\"\".join(full_response)))\n",
    "                    response_buffer = \"\"\n",
    "                    time.sleep(0.05)  # Natural reading speed\n",
    "        \n",
    "        # Final update to ensure complete display\n",
    "        out.update(Markdown(\"\".join(full_response)))\n",
    "        \n",
    "        # Store complete response\n",
    "        response_dict = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\".join(full_response),\n",
    "            \"additional_kwargs\": getattr(response_stream, \"additional_kwargs\", {})\n",
    "        }\n",
    "        messages.append(response_dict)\n",
    "        \n",
    "        # Handle tool calls (unchanged)\n",
    "        additional_kwargs = response_dict.get(\"additional_kwargs\", {})\n",
    "        if \"tool_calls\" in additional_kwargs:\n",
    "            for tool_call in additional_kwargs[\"tool_calls\"]:\n",
    "                result = execute_tool_call(tool_call, tools_map)\n",
    "                result_message = {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": result,\n",
    "                    \"tool_call_id\": tool_call.get(\"id\", \"\"),\n",
    "                    \"name\": tool_call[\"function\"][\"name\"]\n",
    "                }\n",
    "                messages.append(result_message)\n",
    "                chat_messages.append(convert_to_chat_message(result_message))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc0457-9e29-4746-908a-496419536425",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "### Sample Questions:\n",
    "1. What are our top-selling products by revenue and quantity sold?\n",
    "2. Who are our top 10 customers by total spend and order frequency?\n",
    "3. Which products have the lowest stock levels relative to their sales velocity?\n",
    "4. Which product categories generate the highest profit margins?\n",
    "5. What is our order fulfillment rate and average time to fulfill orders?\n",
    "6. How has our customer base grown over time?\n",
    "7. What are the seasonal trends in our product categories?\n",
    "8. What products are frequently purchased together?\n",
    "9. What percentage of customers make repeat purchases?\n",
    "10. Which customer segments are most profitable when considering acquisition cost and lifetime value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed733b2f-9fd1-440f-83b3-509291f44141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated imports\n",
    "from typing import AsyncIterator, Iterator\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 10. Prepare Tools for Agent (unchanged)\n",
    "tools = [query_postgres_database]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "tools_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "# Modified agent interaction with streaming\n",
    "def run_agent_interaction():\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nUser (type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        messages = run_full_turn(system_prompt, messages)\n",
    "        \n",
    "        # Display any tool results\n",
    "        for msg in reversed(messages):\n",
    "            if msg.get(\"role\") == \"tool\" and \"content\" in msg:\n",
    "                print(f\"\\n[Database Result]: {msg['content']}\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_agent_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f71c84-02cf-4d3e-9f4f-c47e6ac2dbb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Connecting a Data Source in Unified Analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0fe25-896d-4c44-a64b-2ae98df81012",
   "metadata": {
    "tags": []
   },
   "source": [
    "**HPE Ezmeral Unified Analytics** allows users to connect multiple types of internal and external data sources - from SQL servers to Snowflake, Terradata and Oracle databases - and make the files, objects and tables within them available to any tool or application running on Unified Analytics.\n",
    "\n",
    "In this section, you will learn how to make a data connection using the Delta Tables you created in Exercise 1. \n",
    "\n",
    "### Connect Delta Tables as Data Source using Apache Hive.\n",
    "\n",
    "Let's take those Delta Tables you created in Exercise 1 and make them available to other applications in Unified Analytics by connecting them as an **Data Source**. Apache Hive gives an SQL-like interface to query data stored in various databases and file systems, like the delta tables you created in Exercise 1. This will allow you to use EzPresto to turn them into datasets later in the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d9b1-1b1e-4170-8590-6410c8ce5c22",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Engineering` > `Data Sources`.\n",
    "1. Under the `Structured Data` tab, click `Add New Data Source`.\n",
    "1. Under **Hive**, click `Create Connection`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115054b-7f09-4e94-a658-805ab59cbd41",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Name**: `retail`\n",
    "- **Hive Metastore:** `discovery`\n",
    "- **Data Dir:** `file:/data/shared/retail-data/delta-tables`\n",
    "- **File Type:** `PARQUET`\n",
    "5. Click `Connect`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eef0ef-596b-49d8-aaea-0fb356b214d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/connect-dl.png\" alt=\"Drawing\" style=\"width: 25%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81c4e6-e737-4a72-be42-b2797b4f0864",
   "metadata": {
    "tags": []
   },
   "source": [
    "6. Under the `Structured Data` tab, you will now see your connected data source. \n",
    "\n",
    "### Viewing and Querying Data from Data Sources\n",
    "\n",
    "Now that our Delta Tables are available via a Data Source, we can leverage the native data tools within Unified Analytics to run queries and create datasets.\n",
    "\n",
    "1. Click on the three dots in the top right hand corner of the newly created data source.\n",
    "1. Click `Change to public access`. In the dialog box that appears, click `Proceed`.\n",
    "1. Next, select `Query using Data Catalog`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f721c4a-07a1-4c2f-b26a-8f0b9a0da9a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/title.png\" alt=\"Drawing\" style=\"width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03eaf7-2b56-487b-9853-01edbdfcaedd",
   "metadata": {
    "tags": []
   },
   "source": [
    "4. Under `Connected Data Sources`, look for the `retail` group.\n",
    "5. Under the `retail` group, check the `default` box. \n",
    "6. Select the datasets for all three countries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48139022-3815-40ea-bb82-a0ab84b1fc0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/datacatalog.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355dcac6-8f33-4c6d-813c-357c0afb1f69",
   "metadata": {
    "tags": []
   },
   "source": [
    "7. Click `Selected Datasets` in the top left corner.\n",
    "8. Click `Query Editor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a74bc-bb74-4e46-9f63-1cebc4990d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, you are introduced to the the **Unified Analytics Query Editor** where you can directly query data sources from specific datasets and data tables - all from within the Unified Analytics user interface! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de14a6-4cad-4d43-adf0-d6d5452c3429",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/QueryEditor.png\" alt=\"Drawing\" style=\"width: 75%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f9fe6-6057-4d7b-ab9b-a940f6b3f8fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "9. Next, we're going to run an SQL query which will combine the data from our three tables (czech, germany, and swiss) in the retail.default schema. This will merge all columns from the czech and germany tables, and select specific columns from the swiss table whilst also applying a transformation to the country column. We'll also limit our final result set to 1000 rows. And all in a nice UI!\n",
    "\n",
    "    Paste the following SQL Query into the `SQL Query` field and \n",
    "    click `Run`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13bcea-7bb7-4f4f-a216-c0a42f4f6d1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "```sql\n",
    "SELECT * FROM retail.default.czech UNION ALL SELECT * FROM retail.default.germany UNION ALL ( SELECT PRODUCTID , PRODUCT , TYPE , UNITPRICE , UNIT , QTY , TOTALSALES , CURRENCY , STORE , (CASE WHEN (country = 'Swiss') THEN 'Switzerland' ELSE country END) COUNTRY , YEAR FROM retail.default.swiss ) LIMIT 1000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d258af-ff6e-47b6-9fcb-adb98593bb69",
   "metadata": {
    "tags": []
   },
   "source": [
    "10. Expand the resulting query to visually validate it.\n",
    "11. Under `Actions` (top-left corner of the table), click `Save as View`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d945a-e833-4e5a-8d65-dd00989aa680",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/query-results.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cdd3d-f65a-4cec-a360-b52e6b93b377",
   "metadata": {
    "tags": []
   },
   "source": [
    "12. Name the View `retail`. \n",
    "13. We'll want to save the schema of this new table as a Custom Schema. Under `Schema`, select `+ Add new schema` and name it `retailschema`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785fbea-7da1-498a-abcd-75a10f07472b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/save-as-view.png\" alt=\"Drawing\" style=\"width: 45%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a892513-16c2-4467-b14a-afa866c6b591",
   "metadata": {
    "tags": []
   },
   "source": [
    "You have now saved the dataset resulting from your SQL query as an **Asset**. Assets are made available to other applications via EzPresto, as you will explore in Exercise 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93394c5a-e892-4908-a14f-77cabd545b96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Cached Assets in HPE Ezmeral Unified Analytics**\n",
    "\n",
    "To view our saved Assets, including the one that we just created:\n",
    "\n",
    "1. In Unified Analytics, under the sidebar navigation menu, select `Data Engineering` > `Cached Assets`. \n",
    "1. You should see an asset with Name `retail`. \n",
    "1. Click the three dots associated with the `retail` asset.\n",
    "\n",
    "<img src=\"./images/exercise2/cachedasset.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "\n",
    "4. Click `View Columns`. \n",
    "5. Validate there are eleven fields. \n",
    "\n",
    "Should you ever wish to run futher queries on a Cached Asset in the future, check the box next to an asset and click `Query Editor`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc431cae-33d3-4f9f-bedb-d61b4b43b166",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Jupyter Magic Commands on HPE Ezmeral Unified Analytics**\n",
    "\n",
    "Jupyter Notebooks Magic functions, also known as magic commands or magics, are commands that you can execute within a code cell.   \n",
    "Magics are not code of any language, but are shortcuts that extends the capabilities of a notebook. \n",
    "\n",
    "There are two types of magic commands - **Line** and **Cell** magic commands:\n",
    "\n",
    "**Line magic** commands do not require a cell body and start with a single % character.  \n",
    "**Cell magic** commands start with %% and require additional lines of input (a cell body). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05b8ca-96c8-4232-b1c2-b66b3d197a7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HPE Ezmeral Magic Commands \n",
    "**HPE Ezmeral Unified Analytics Software** supports both Line and Cell magic commands and includes custom commands that allow for users to interact with other tools native to Unified Analytics directly within notebooks.\n",
    "\n",
    "We can check out the full list of custom HPE magic commands by running `%command`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82bf08-370c-4d5f-b866-e34c1c28623e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The %commands command lists the magic commands and SDKs that are customized by Hewlett Packard Enterprise and are available in this notebook.\n",
    "%commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d19dd-815b-43ca-92bb-b2bf74ce12a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Updating the Cached JWT Token\n",
    "\n",
    "In a Jupyter notebook, a JWT token (JSON Web Token) is a compact and URL-safe means of representing authentication information to be transferred between other notebook servers or external applications. It is commonly used for securely authenticating and authorizing users within Jupyter environments, allowing them to access resources and execute code while ensuring their identity and permissions are properly validated.\n",
    "\n",
    "When working in Jupyter notebooks for long durations, particularly when making calls to other applications, the JWT token can expire and result in an error when attempting to make calls. This is particularly relevant for working on a Jupyter notebook within **HPE Ezmeral Unified Analytics**, which provides users to leverage a plethora of external tools within the notebook (Such as Spark, Livy and Presto).\n",
    "\n",
    "**If you encounter a JWT token expiration error while running cells in a Jupyter notebook**, you can resolve it by running the `%update_token` magic command.  \n",
    "This function updates the JWT in environment variables and any other locations where the token is utilized. \n",
    "  \n",
    "Ideally, it is good practice to refresh the token prior to making external connections. Some examples relevant to the Smart Retail Experience demo include:\n",
    "\n",
    "- Authentication when establishing a connection with PrestoDB.\n",
    "- Authentication with local s3 minio object storage.  \n",
    "- Authentication with KServe external API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7b6e8-d5a4-4244-9edf-9d9425355d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b59107-9c22-430e-b49e-a41e29245916",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Directly interacting with connected SQL databases using the SQL Magic Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c053e-e835-4527-b02c-769fc26b65c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using the `%sql` magic command, you can directly query SQL databases from Data Sources you have connected to **HPE Ezmeral Unified Analytics Software** from within Jupyter notebook cells! When you run the notebook cell containing `%sql` and your SQL query, the magic command sends the query to the database, runs the query, and retrieves the result.\n",
    "\n",
    "This is made possible by the native integration of EzPresto into Unified Analytics. **However, the Data Source must be made publicly available.**\n",
    "\n",
    "To change the access of a Data Source from `private` to `public`:\n",
    "\n",
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Engineering` > `Data Sources`.\n",
    "1. Under the three dots in the top corner of the Data Source of interest, click `Change to public access`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341b8e2-56f7-415f-b88a-cfa64c6b32f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> Wait until a confirmation message appears stating that the source is publicly available.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c2cd9-c591-433e-9150-ef4d83edf3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's try the `%sql` magic command to interact with our Delta Tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f552d93-de1e-4af8-9c2c-5d0ac2c960cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql select * from retail.retail.czech limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fc7b3-da34-43bd-8a6c-5c23ddbb377b",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also save the output of our command as a Python variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cece6e3-372a-414f-accb-8dd82b4841ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = %sql select * from retail.retail.czech limit 10\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfac0-f760-45ed-9c7f-ecc7d86535a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Conclusion**\n",
    "\n",
    "In this exercise, you learned how **EzPresto** on **HPE Ezmeral Unified Analytics** makes connecting internal and external data sources to your applications, such this notebook hosted on Unified Analytics, a snap. You learned how to leverage the data engineering tools available within the Unified Analytics interface, including the Query Editor, to create datasets from your data sources that could then be shared and quered using HPE Magic Commands inside Unified Analytics-hosted Jupyter Notebooks.\n",
    "\n",
    "In the next exercise, you will learn how to use visualize the datasets you have created using **Apache Superset**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
