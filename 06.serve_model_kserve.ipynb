{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995e629d-b140-4654-8000-c5d2d2982027",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c186f6-e95f-463b-980c-1b8c83503285",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 6:** Serving your model with KServe\n",
    "\n",
    "By this stage, you have a model that is saved in a Model Regsitry as a 'Production' model. Now, it's time to make this model available to every self-serve checkout in every one of our retail stores.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Be briefly introduced to containers, Kubernetes, Kubeflow and KServe.\n",
    "- Create an InferenceService configuration.\n",
    "- Serve the model through an InferenceService using KServe.\n",
    "- Learn how to manage Kubeflow Endpoints.\n",
    "\n",
    "By the end of this exercise, you will have learned how to serve a model at scale using KServe and Kubeflow on **HPE Ezmeral Unified Analytics**.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b852c8-2216-4bf9-bee9-432aa4c7987c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Prerequsites**\n",
    "\n",
    "As instructed in the [Introductory notebook](./00.introduction.ipynb), ensure that you have run `pip install -r requirements.txt` in a Terminal window, located in the same working directory, prior to running this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82a37e-7723-46fa-b22f-ac40fe013c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> This exercise requires the completion of Exercises 5: Tracking, Registering and Inferencing Models in MLflow.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7535e-9c43-4dc5-9efb-6e7c4e6d64b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Introduction to Kubeflow and KServe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ba67d-c7bc-4ba3-9868-ee779aaa04c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you were just looking to serve your model locally, such as to a single notebook, script or application, you would now be in a position to simply use the MLflow URI to get a \"copy\" of it each time. \n",
    "\n",
    "You can probably guess that whilst this may work for a one or a few instances, it is hardly a **scalable** solution. What if we want to deploy this model such that it can be remotely inferenced by every self-serve checkout in every retail store across multiple countries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5fb20-44cf-4b6a-8ec9-adc11fa41124",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A quick word about Kubernetes and containers\n",
    "\n",
    "This problem of scaling applications (such as a model inferencing server) based on user demand is one that has plagued infrastructure managers since the birth of the modern computer. How do I deploy an application onto infrastructure such that it is available to 100 users during the evening and 100,000 users during the day? Making it available.\n",
    "\n",
    "This problem brought about the idea of the **container**, whereby a lightweight instance of an application could be easily spun up and powered down near-instantaneously. By deploying applications in containers, we could better manage the allocation of our compute infrastructure resources at any given time. \n",
    "\n",
    "However, containerization brought about another problem: when there are hundreds of thousands or even **millions** of containers, who (or *what*) is going to spin up and power down these containers based on demand? Enter **Kubernetes**. \n",
    "\n",
    "**Kubernetes**, also known as K8s, is an open-source system designed to automate the deployment, scaling, and management of containerized applications. Kubernetes is like a conductor for a container orchestra*, ensuring everything runs smoothly and efficiently on top of several resource notes (compute and storage servers). Kubernetes groups containers, which are self-contained units of software, into rapidly replicatable logical units for easier management and discovery.\n",
    "\n",
    "**A fitting analogy, seeing as Kubernetes is, by definition, a container orchestration platform!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97116932-8e48-48a4-9643-7760e0da1d3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What is KServe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8443e-6b38-4237-882b-55ac3c4fae1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "KServe is a framework for deploying and serving machine learning (ML) models in production on Kubernetes. It simplifies the process of serving models by providing a Kubernetes Custom Resource Definition (CRD) that lets you easily define **how** your models should be served.\n",
    "\n",
    "KServe is **incredibly powerful** for model inferencing, as it can scale your model serving instances up or down based on real-time traffic. This allows for what is known as \"scale-to-zero\" functionality on CPUs and GPUs for efficient resource utilization.\n",
    "\n",
    "KServe is not a standalone platform but instead core add-on component of **Kubeflow**, specifically addressing the model serving aspect of the ML pipeline. Kubeflow is an open-source platform focused on machine learning operations (MLOps) on Kubernetes. It offers a collection of tools that cover the entire ML lifecycle, from model building, training, and deployment to monitoring and management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a5512-b8a3-4c07-9a2c-9e557a629f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wait, Kubeflow sounds familiar..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51ba28-74f1-46f4-bb4c-5ceea61107ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "... to another machine learning platform we've already been introduced to in MLflow. \n",
    "\n",
    "Kubeflow and MLflow serve different purposes in the machine learning workflow:\n",
    "\n",
    "* **Kubeflow** is an open-source platform designed to facilitate the end-to-end **orchestration and management** of machine learning workflows on **Kubernetes**, providing capabilities for model training, **deployment**, **serving**, and **monitoring**.\n",
    "\n",
    "* **MLflow**, on the other hand, is a light **platform-agnostic** open-source tool that excels at **experiment tracking**, **version control** for models through a Model Registry, and facilitates **collaboration** among data scientists by keeping everything organized and reproducible.  \n",
    "\n",
    "The latest versions of Kubeflow and MLflow come natively installed with **HPE Ezmeral Unified Analytics**, which sits on top of a Kubernetes distribution - taking away all of the pain of deploying, connecting and managing these applications on top of Kubernetes yourself. \n",
    "\n",
    "Today, data scientists and engineers leverage both Kubeflow and MLflow to address distinct needs within the machine learning lifecycle. Together with Unified Analytics, they provide the complete MLOps solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490016b4-5cb1-4699-9c00-ecfc8a19cd23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Declaring Variables and Importing Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa7970-549d-4f24-a048-5ebdf1e3cb9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's re-declare the variables related to our MLflow experiement such that we can access them in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c129f625-a3e9-4b4b-a95e-ee05fbe6ba0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment variables for MLflow\n",
    "experiment_name = \"retail-experiment\"\n",
    "model_name = \"produce-detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9589dae-75c1-49fd-8ea8-72dc42c2c429",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll import the necessary libraries. \n",
    "\n",
    "Ignore any warnings that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150f8bf7-ed2c-4c87-934c-9c79e6a63a93",
   "metadata": {
    "papermill": {
     "duration": 7.613981,
     "end_time": "2023-03-13T20:40:28.950010",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.336029",
     "status": "completed"
    },
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 10:23:56.252755: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-07 10:23:56.256468: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-07 10:23:56.268033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741343036.286593    2456 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741343036.292198    2456 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-07 10:23:56.312229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from kubernetes import client \n",
    "from kubernetes.client import V1EnvVar\n",
    "from kubernetes.client.models import V1ObjectMeta\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import utils\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1TFServingSpec\n",
    "import urllib3\n",
    "import mlflow\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f91cdf-3363-4e18-8a00-4b47067899ca",
   "metadata": {
    "papermill": {
     "duration": 0.074875,
     "end_time": "2023-03-13T21:22:22.585971",
     "exception": false,
     "start_time": "2023-03-13T21:22:22.511096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **3. Model Serving with KServe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bdadb-cf16-4796-a2ad-0037c4cdd238",
   "metadata": {
    "tags": []
   },
   "source": [
    "It's time to leverage the power of KServe to make our produce detection model available wherever we may need it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f9206-fe7f-4eca-8e10-c362a5555039",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Model details from MLflow\n",
    "\n",
    "First, let's connect this notebook to MLflow and get the URI for the 'Production' version of our model. We confirmed this URI was accessible in our Testing section of <a href=\"./05.working_in_mlflow.ipynb\" style=\"color: black\">Exercise 5</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879fc0fa-91f8-4f05-abb3-e43c55a2e9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token successfully refreshed.\n"
     ]
    }
   ],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaad2cc0-5394-47cb-9f66-a748090a2049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Storage Path in S3 (as shown in MLflow): s3://mlflow.ddpcai/14/dcb9bf579a2449e88cb47e697b81af39/artifacts/tf_serving_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2456/2264835038.py:5: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest_versions = client.get_latest_versions(name=model_name, stages=[\"Production\"])\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the MlflowClient\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get the latest model version in Production\n",
    "latest_versions = client.get_latest_versions(name=model_name, stages=[\"Production\"])\n",
    "latest_version = latest_versions[0]\n",
    "\n",
    "# Get the model uri\n",
    "model_uri = latest_version.source.replace(\"model\", \"tf_serving_model\")\n",
    "\n",
    "print(\"Model Storage Path in S3 (as shown in MLflow): \" + str(model_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86510ed0-768f-42ec-95f4-4fec04238247",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create the InferenceService config file (YAML)\n",
    "\n",
    "Next, we'll create the configuration file (YAML) that tells KServe to serve our model as an **InferenceService**. An InferenceService is a Kubernetes Custom Resource (CR) provided by KServe. In the context of KServe, an InferenceService represents a scalable and load-balanced service that hosts one or more machine learning models for real-time inference or prediction.\n",
    "\n",
    "A YAML file for defining an InferenceService in KServe typically specifies the configuration for deploying and serving a machine learning model. Like most KServe YAMLs, our InferenceService YAML will require:\n",
    "\n",
    "- `apiVersion` and `kind` to specify the Kubernetes API version and kind of resource being defined, respectively.\n",
    "- `metadata` to provide metadata, such as the name of the InferenceService.\n",
    "- `spec` to define the specifications for the InferenceService, including the `predictor` and `transformer`.\n",
    "- `predictor` section specifies the details of the model to be served, such as the model's location (`modelUri`), runtime version (`runtimeVersion`), and resource requirements (`resources`).\n",
    "- `transformer` section specifies any pre-processing or post-processing steps required before or after making predictions. In this example, it includes a container image (`image`) and environment variables (`env`).\n",
    "\n",
    "For our YAML file, we set up the necessary resources in Kubernetes to deploy an InferenceService with KServe, including secrets for authentication with the Unified Analytics internal S3 storage that MLflow uses to store models in the Model Registry, service account authentication details, and the configuration for the InferenceService itself (where we parse the model URI and declare Tensorflow as the `predictor`). \n",
    "\n",
    "We'll declare the secret and service name parameters that we will parse into the YAML text, then create a YAML file and store it in the local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c184d0-e715-4266-9e0d-974aa86d66ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set paramentes\n",
    "isvc_name = experiment_name\n",
    "secret_name = 's3-proxy-kserve-secret'\n",
    "sa_name = 's3-proxy-kserve-sa'\n",
    "\n",
    "#Set name of YAML file\n",
    "yaml_name = './model-kserve.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ffa909-99c2-461f-99d7-ce26bbe4ed58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create YAML configuration file\n",
    "with open(yaml_name, 'w') as file:\n",
    "    text = f\"\"\"---\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: \"{secret_name}\"\n",
    "  annotations:\n",
    "    serving.kserve.io/s3-cabundle: \"\"\n",
    "    serving.kserve.io/s3-endpoint: \"local-s3-service.ezdata-system.svc.cluster.local:30000/\"\n",
    "    serving.kserve.io/s3-useanoncredential: \"false\"\n",
    "    serving.kserve.io/s3-usehttps: \"0\"\n",
    "    serving.kserve.io/s3-verifyssl: \"0\"\n",
    "stringData:\n",
    "  AWS_ACCESS_KEY_ID: \"{os.environ['AUTH_TOKEN']}\"\n",
    "  AWS_SECRET_ACCESS_KEY: \"s3\"\n",
    "type: Opaque\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: \"{sa_name}\"\n",
    "secrets:\n",
    "  - name: \"access-token\"\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: \"{isvc_name}\"\n",
    "  annotations:\n",
    "    \"sidecar.istio.io/inject\": \"false\"\n",
    "spec:\n",
    "  predictor:\n",
    "    tensorflow:\n",
    "      storageUri: \"{model_uri}\"\n",
    "    serviceAccountName: \"{sa_name}\"\n",
    "\"\"\"\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65a202-da63-4237-b97f-a9950b852aa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we run the Kubernetes command `apply` to deploy our InferenceService!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57361cd2-819f-4901-a800-ea72c8a30ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/s3-proxy-kserve-secret created\n",
      "serviceaccount/s3-proxy-kserve-sa created\n",
      "inferenceservice.serving.kserve.io/retail-experiment created\n"
     ]
    }
   ],
   "source": [
    "# Create the container\n",
    "!kubectl apply -f {yaml_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071c5d4-b707-4e83-9aec-b67a373e2224",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can confirm that it's up, running and ready to inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce17d8fd-2f82-43ee-825e-2c33cfcdee9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               READY    PREV    LATEST    URL\n",
      "retail-experiment  Unknown\n",
      "NAME               READY      PREV    LATEST  URL\n",
      "retail-experiment  Unknown       0       100\n",
      "NAME               READY      PREV    LATEST  URL\n",
      "retail-experiment  Unknown       0       100\n",
      "NAME               READY      PREV    LATEST  URL\n",
      "retail-experiment  Unknown       0       100\n",
      "NAME               READY      PREV    LATEST  URL\n",
      "retail-experiment  True          0       100  https://retail-experiment-predictor-admin-901d042c.hpepcai.ezmeral.demo.local\n",
      "\n",
      "InferenceService retail-experiment is ready.\n"
     ]
    }
   ],
   "source": [
    "# Wait until the ISvc is ready\n",
    "KServe_client = KServeClient()\n",
    "KServe_client.wait_isvc_ready(isvc_name, watch=True, timeout_seconds=120)\n",
    "print(f\"\\nInferenceService {isvc_name} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a530e1d-1ad4-4a9e-b899-f658a3746323",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4. Managing Kubeflow Endpoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb00d4-dc0a-4787-849e-71e4dab53751",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can observe the status of our InferenceService through the **Endpoints** pane in Kubeflow.\n",
    "\n",
    "To access the Endpoints pane:\n",
    "\n",
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Science` > `Model Serving`.\n",
    "1. The **Kubeflow Endpoints** pane will open in a new tab.\n",
    "\n",
    "Here, we will see the complete list of applications (in our case, an InferenceService instance) currently being served using KServe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5095b51-0f7f-4591-b3d6-82b04a165272",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise6/kserve1.png\" alt=\"Drawing\" style=\"width: 90%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb024006-4c79-4b20-af12-11ce92f320fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Let's explore the InferenceService further.\n",
    "\n",
    "4. Click on the `retail-experiment` Endpoint.\n",
    "\n",
    "Here, we can see details about our InferenceService, including the **serving endpoints**, the MLflow URI for the model and our choice of predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78455700-e1bb-42ff-a17e-fd1182f6bb09",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise6/kserve2.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e499a06-6594-493f-a726-b81c7f8d3999",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tip:</b> Should you ever encounter issues with an InferenceService, you can diagnose the issue via the logs located under the `Logs` tab.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61e46c-cc4e-4691-8c4a-4b5c66150fd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `URL internal` link is what can be used to inference the link from any application within **HPE Ezmeral Unified Analytics**, including notebooks and custom applications hosted within Unified Analytics. For applications hosted outside of our Unified Analytics cluster that have the right authentication, you would use the `URL external` link. \n",
    "\n",
    "5. Copy the `URL internal` link to the clipboard and paste it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a8ba023-4a1a-4ece-896b-efe47cb76f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "internal_url = \"http://retail-experiment-predictor.admin-9dbd466f.svc.cluster.local\" #paste the URL internal endpoint for your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a8fec-becf-4bda-a5db-b4076333da45",
   "metadata": {
    "papermill": {
     "duration": 0.074875,
     "end_time": "2023-03-13T21:22:22.585971",
     "exception": false,
     "start_time": "2023-03-13T21:22:22.511096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **5. Inferencing our Model using an Endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2ef76-6906-40c6-8cfe-8a0847fa025d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Your produce recognition model is scalably served and is accessible via an endpoint URL. Let's learn how we use this endpoint to send the model an image of a fruit or vegetable, and for it to send back what it detects*.\n",
    "\n",
    "**Calling an model to make a detection or prediction is known as **inferencing** a model.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95cb9b7-1b9a-434d-8c7b-4d21ff41827d",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we'll build out the full **endpoint URL** using the `URL internal` from our InferenceService."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a37d6bb-deec-4cac-907d-8942c107524b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token successfully refreshed.\n"
     ]
    }
   ],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7298806c-5d0f-4746-8267-67e7791c3e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving URL: http://retail-experiment-predictor.admin-9dbd466f.svc.cluster.local/v1/models/retail-experiment:predict\n"
     ]
    }
   ],
   "source": [
    "# Build the Serving URL\n",
    "serving_url = internal_url + \"/v1/models/\" + isvc_name + \":predict\"\n",
    "\n",
    "print(\"Serving URL: \" + serving_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a306316-8590-4167-b3d9-228a91ad63c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll define some functions. Similar to previous exercises, we'll prepare any image that we want to infer the model on. We'll also define a function that will convert our preprocessed image into a JSON `REST` package that we can `POST` to the endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33dccaa-6807-420b-a661-5cb19418255a",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_image(location):\n",
    "    # Load the image\n",
    "    if \"http\" in location:\n",
    "        response = requests.get(location)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "        img=load_img(location,target_size=(224,224,3))\n",
    "    \n",
    "    img = img.resize((224, 224))\n",
    "    print(type(img))\n",
    "    img_array = img_to_array(img)\n",
    "    print(img_array.shape)\n",
    "    print(type(img_array))\n",
    "    img_array = img_array / 255.0\n",
    "    print(img_array.shape)\n",
    "    print(type(img_array))\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    print(img_array.shape)\n",
    "    print(type(img_array))\n",
    "\n",
    "    return img_array\n",
    "\n",
    "def format_data(data):\n",
    "    # Convert the NumPy array to a list\n",
    "    data_list = data.tolist()\n",
    "    \n",
    "    # Format the list as a JSON string\n",
    "    data_formatted = json.dumps(data_list)\n",
    "    \n",
    "    # Create a JSON request string with the formatted data\n",
    "    json_request = '{{ \"instances\" : {} }}'.format(data_formatted)\n",
    "    \n",
    "    return json_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeac32f-c210-4df2-9d61-9c2171529b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we're ready to inference our model with a supplied image. \n",
    "\n",
    "For the final time, **go out** onto Google Images and find a **new** image with a fruit or vegetable in it. Replace the `image_url` variable with the link to your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240cfb5f-485f-4512-81a7-5039f5e8c45c",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "(224, 224, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(224, 224, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 224, 224, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='retail-experiment-predictor.admin-9dbd466f.svc.cluster.local', port=80): Max retries exceeded with url: /v1/models/retail-experiment:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f542ec00690>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1303\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1349\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1058\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:996\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f542ec00690>: Failed to establish a new connection: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:802\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    800\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 802\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/retry.py:594\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    596\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='retail-experiment-predictor.admin-9dbd466f.svc.cluster.local', port=80): Max retries exceeded with url: /v1/models/retail-experiment:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f542ec00690>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m headers \u001b[38;5;241m=\u001b[39m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUTH_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Make the POST request\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserving_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest successfully made.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='retail-experiment-predictor.admin-9dbd466f.svc.cluster.local', port=80): Max retries exceeded with url: /v1/models/retail-experiment:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f542ec00690>: Failed to establish a new connection: [Errno -2] Name or service not known'))"
     ]
    }
   ],
   "source": [
    "# Define your labels\n",
    "labels = {'apple': 0, 'banana': 1, 'carrot': 2, 'cucumber': 3, 'lemon': 4, 'orange': 5}\n",
    "labels = dict((v, k) for k, v in labels.items())\n",
    "\n",
    "online_url = \"\"\n",
    "local_url = os.getcwd() + \"/images/test_image.jpg\"\n",
    "\n",
    "if online_url:\n",
    "    image_url = online_url\n",
    "else:\n",
    "    image_url = local_url\n",
    "\n",
    "preprocessed_image = preprocess_image(image_url)\n",
    "\n",
    "json_request = format_data(preprocessed_image)\n",
    "\n",
    "headers = headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(serving_url, data=json_request, headers=headers, verify=False)\n",
    "print(response)\n",
    "print(\"Request successfully made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7361c8-8ac5-45af-8339-0a08cfd818ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's see what we got back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f837d-ab95-4b9a-bdd6-ead2c9684f15",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Print the raw response content\n",
    "print(\"Raw Response Content:\")\n",
    "print(response.content.decode('utf-8'))\n",
    "\n",
    "# Decode the JSON response\n",
    "if response.headers.get(\"Content-Type\") == \"application/json\":\n",
    "    response_data = response.json()\n",
    "    predictions = response_data['predictions']\n",
    "\n",
    "    formatted_predictions = [[round(pred * 100, 2) for pred in prediction] for prediction in predictions]\n",
    "\n",
    "    print(\"\\nTranslated Predictions:\")\n",
    "    for label, prob in zip(labels, formatted_predictions[0]):\n",
    "        print(f\"- {label}: \\t{prob}%\")\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label_index = np.argmax(formatted_predictions)\n",
    "    predicted_label = labels[predicted_label_index]\n",
    "\n",
    "    print(\"\\nPredicted class label:\", predicted_label, \"with\", formatted_predictions[0][predicted_label_index], \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0aff05-4880-4198-b4d9-e6ae90c4016c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Did the model correctly guess what was in your image? If so, great! If not... still great! We've successfully validated that our model is being served with an endpoint using KServe!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33620aa3-f065-4c09-9362-749c1274fcee",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0c656-2af4-4225-aaff-8252d5c68c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this exercise, you have learned the underlying theory behind scaling the serving of a model - including containers, Kubernetes, Kubeflow and KServe. You took the latest version of your produce detection model from the MLflow Model Repository, created the configuration YAML for a KServe InferenceService, and deployed it on the Kubernetes cluster that powers Unified Analytics.\n",
    "\n",
    "Now, we have an endpoint URL that we can call to inference our model from any application within **HPE Ezmeral Unified Analytics**!\n",
    "\n",
    "In the next exercise, we will leverage this endpoint to make **real-time** detections of produce within a custom self-checkout application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df4177-8730-4e85-b3d2-c5848d7229d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1abf5e-b445-4bdb-a33c-b7e2e3668fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ba00e9-c983-43c6-b40c-fbfd17973d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bentoml\n",
      "  Downloading bentoml-1.4.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting a2wsgi>=1.10.7 (from bentoml)\n",
      "  Downloading a2wsgi-1.10.8-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohttp (from bentoml)\n",
      "  Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiosqlite>=0.20.0 (from bentoml)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (24.2.0)\n",
      "Collecting cattrs<23.2.0,>=22.1.0 (from bentoml)\n",
      "  Downloading cattrs-23.1.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting click-option-group (from bentoml)\n",
      "  Downloading click_option_group-0.5.6-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (2.2.1)\n",
      "Collecting fs (from bentoml)\n",
      "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.11/site-packages (from bentoml) (0.27.2)\n",
      "Collecting httpx-ws>=0.6.0 (from bentoml)\n",
      "  Downloading httpx_ws-0.7.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from bentoml) (3.1.4)\n",
      "Collecting kantoku>=0.18.1 (from bentoml)\n",
      "  Downloading kantoku-0.18.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from bentoml) (1.26.4)\n",
      "Collecting nvidia-ml-py (from bentoml)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: opentelemetry-api~=1.20 in /opt/conda/lib/python3.11/site-packages (from bentoml) (1.27.0)\n",
      "Collecting opentelemetry-instrumentation-aiohttp-client~=0.41b0 (from bentoml)\n",
      "  Downloading opentelemetry_instrumentation_aiohttp_client-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi~=0.41b0 (from bentoml)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation~=0.41b0 (from bentoml)\n",
      "  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.20 in /opt/conda/lib/python3.11/site-packages (from bentoml) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions~=0.41b0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (0.48b0)\n",
      "Collecting opentelemetry-util-http~=0.41b0 (from bentoml)\n",
      "  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (24.1)\n",
      "Collecting pathspec (from bentoml)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pip-requirements-parser>=31.2.0 (from bentoml)\n",
      "  Downloading pip_requirements_parser-32.0.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.10.0 in /home/admin/.local/lib/python3.11/site-packages (from bentoml) (0.20.0)\n",
      "Requirement already satisfied: psutil in /home/admin/.local/lib/python3.11/site-packages (from bentoml) (5.9.8)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.11/site-packages (from bentoml) (2.9.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from bentoml) (2.9.0)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from bentoml) (1.0.1)\n",
      "Requirement already satisfied: python-json-logger in /opt/conda/lib/python3.11/site-packages (from bentoml) (2.0.7)\n",
      "Collecting python-multipart (from bentoml)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml>=5.0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (6.0.2)\n",
      "Collecting questionary>=2.0.1 (from bentoml)\n",
      "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: rich>=11.2.0 in /home/admin/.local/lib/python3.11/site-packages (from bentoml) (13.9.4)\n",
      "Collecting schema (from bentoml)\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Collecting simple-di>=0.1.4 (from bentoml)\n",
      "  Downloading simple_di-0.1.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: starlette>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (0.41.0)\n",
      "Collecting tomli-w (from bentoml)\n",
      "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.22.0 in /home/admin/.local/lib/python3.11/site-packages (from bentoml) (0.30.6)\n",
      "Requirement already satisfied: watchfiles>=0.15.0 in /opt/conda/lib/python3.11/site-packages (from bentoml) (0.24.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.11/site-packages (from aiosqlite>=0.20.0->bentoml) (4.12.2)\n",
      "Requirement already satisfied: anyio>=4 in /opt/conda/lib/python3.11/site-packages (from httpx-ws>=0.6.0->bentoml) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore>=1.0.4 in /opt/conda/lib/python3.11/site-packages (from httpx-ws>=0.6.0->bentoml) (1.0.6)\n",
      "Collecting wsproto (from httpx-ws>=0.6.0->bentoml)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx->bentoml) (2024.8.30)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx->bentoml) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx->bentoml) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore>=1.0.4->httpx-ws>=0.6.0->bentoml) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2>=3.0.1->bentoml) (3.0.2)\n",
      "Requirement already satisfied: pyzmq>=17.0 in /opt/conda/lib/python3.11/site-packages (from kantoku>=0.18.1->bentoml) (26.2.0)\n",
      "Requirement already satisfied: tornado>=5.0.2 in /opt/conda/lib/python3.11/site-packages (from kantoku>=0.18.1->bentoml) (6.4.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api~=1.20->bentoml) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api~=1.20->bentoml) (8.4.0)\n",
      "Collecting opentelemetry-semantic-conventions~=0.41b0 (from bentoml)\n",
      "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.41b0->bentoml) (1.16.0)\n",
      "Collecting opentelemetry-api~=1.20 (from bentoml)\n",
      "  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi~=0.41b0->bentoml)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-sdk~=1.20 (from bentoml)\n",
      "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from pip-requirements-parser>=31.2.0->bentoml) (3.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3->bentoml) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.11/site-packages (from pydantic<3->bentoml) (2.23.4)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from questionary>=2.0.1->bentoml) (3.0.48)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/admin/.local/lib/python3.11/site-packages (from rich>=11.2.0->bentoml) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.2.0->bentoml) (2.18.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->bentoml)\n",
      "  Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->bentoml)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->bentoml)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->bentoml)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->bentoml)\n",
      "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->bentoml)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting appdirs~=1.4.3 (from fs->bentoml)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from fs->bentoml) (75.1.0)\n",
      "Requirement already satisfied: six~=1.10 in /opt/conda/lib/python3.11/site-packages (from fs->bentoml) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api~=1.20->bentoml) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/admin/.local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->bentoml) (0.1.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt_toolkit<4.0,>=2.0->questionary>=2.0.1->bentoml) (0.2.13)\n",
      "Downloading bentoml-1.4.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading a2wsgi-1.10.8-py3-none-any.whl (17 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading cattrs-23.1.2-py3-none-any.whl (50 kB)\n",
      "Downloading httpx_ws-0.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading kantoku-0.18.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
      "Downloading opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_instrumentation_aiohttp_client-0.51b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
      "Downloading pip_requirements_parser-32.0.1-py3-none-any.whl (35 kB)\n",
      "Downloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Downloading simple_di-0.1.5-py3-none-any.whl (9.8 kB)\n",
      "Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click_option_group-0.5.6-py3-none-any.whl (12 kB)\n",
      "Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
      "Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
      "Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: schema, nvidia-ml-py, appdirs, wsproto, tomli-w, simple-di, python-multipart, propcache, pip-requirements-parser, pathspec, opentelemetry-util-http, multidict, kantoku, fs, frozenlist, click-option-group, cattrs, asgiref, aiosqlite, aiohappyeyeballs, a2wsgi, yarl, questionary, opentelemetry-api, aiosignal, opentelemetry-semantic-conventions, httpx-ws, aiohttp, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-aiohttp-client, bentoml\n",
      "\u001b[33m  WARNING: The scripts circus-plugin, circus-top, circusctl, circusd and circusd-stats are installed in '/home/admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts opentelemetry-bootstrap and opentelemetry-instrument are installed in '/home/admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script bentoml is installed in '/home/admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed a2wsgi-1.10.8 aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 aiosqlite-0.21.0 appdirs-1.4.4 asgiref-3.8.1 bentoml-1.4.3 cattrs-23.1.2 click-option-group-0.5.6 frozenlist-1.5.0 fs-2.4.16 httpx-ws-0.7.1 kantoku-0.18.1 multidict-6.1.0 nvidia-ml-py-12.570.86 opentelemetry-api-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-aiohttp-client-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 pathspec-0.12.1 pip-requirements-parser-32.0.1 propcache-0.3.0 python-multipart-0.0.20 questionary-2.1.0 schema-0.7.7 simple-di-0.1.5 tomli-w-1.2.0 wsproto-1.2.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bentoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19c970f9-7aa7-490c-9ba9-faa07f2f33ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265d295687834d9691a1bdea2540ba10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 10:47:34.097494: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/admin/.local/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module '_bentoml_impl.frameworks.mlflow' has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mpyfunc\u001b[38;5;241m.\u001b[39mload_model(mlflow_model_path)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save it as a BentoML model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m bento_model \u001b[38;5;241m=\u001b[39m \u001b[43mbentoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretail-detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, mlflow_model_path)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBento model saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbento_model\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/bentoml/_internal/utils/lazy_loader.py:72\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load()\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '_bentoml_impl.frameworks.mlflow' has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import bentoml\n",
    "\n",
    "# Define your MLflow model path (local or S3)\n",
    "mlflow_model_path = \"s3://mlflow.ddpcai/14/dcb9bf579a2449e88cb47e697b81af39/artifacts/model\"\n",
    "\n",
    "# Load the MLflow model\n",
    "model = mlflow.pyfunc.load_model(mlflow_model_path)\n",
    "\n",
    "# Save it as a BentoML model\n",
    "bento_model = bentoml.mlflow.save_model(\"retail-detection\", mlflow_model_path)\n",
    "\n",
    "print(f\"Bento model saved at: {bento_model.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10fcf3d7-bf38-4b8f-abe9-665a9129a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab90e9d4-04d2-40b6-a0d9-efa5eb0daabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b87ac3bdfd4e5583b0eb7c1b80839d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the default model signature for Tensorflow ({'__call__': {'batchable': False}}) for model \"retail-detection\".\n",
      "Assets written to: /tmp/tmpcwis4w3tbentoml_model_retail-detection/assets\n",
      "HTTP Request: POST https://t.bentoml.com \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bento model saved at: /home/admin/bentoml/models/retail-detection/4cr5fbh3ighh2wvm/\n"
     ]
    }
   ],
   "source": [
    "import mlflow.tensorflow\n",
    "import bentoml\n",
    "\n",
    "# Load the MLflow model as a TensorFlow model instead of PyFunc\n",
    "model = mlflow.tensorflow.load_model(mlflow_model_path)\n",
    "\n",
    "# Now save it as a BentoML model\n",
    "bento_model = bentoml.tensorflow.save_model(\"retail-detection\", model)\n",
    "\n",
    "print(f\"Bento model saved at: {bento_model.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "893ca6e0-4e3e-4f18-b2e4-a24c267919ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf bentoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908102b-979a-4530-81dd-31e222fe61ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-cuda-full:ezaf-fy23-q2",
   "experiment": {
    "id": "new",
    "name": "jk-fruit-demo"
   },
   "experiment_name": "jk-fruit-demo",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "random",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "additionalMetricNames": [],
     "goal": 1,
     "objectiveMetricName": "stage",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "50",
       "min": "1",
       "step": "1"
      },
      "name": "param_epoch",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "list": [
        "32",
        "64"
       ]
      },
      "name": "param_batch_size",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "1",
       "step": "1"
      },
      "name": "param_patience",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": false,
   "pipeline_description": "fruit-veg",
   "pipeline_name": "fruit-veg",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true",
    "label:add-user-s3-secret:true"
   ],
   "storage_class_name": "dataplatform",
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/mnt/shared/",
     "name": "kubeflow-pipeline",
     "size": 1,
     "size_type": "Gi",
     "snapshot": false,
     "snapshot_name": "",
     "type": "pvc"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2679.282234,
   "end_time": "2023-03-13T21:24:51.692744",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-13T20:40:12.410510",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
