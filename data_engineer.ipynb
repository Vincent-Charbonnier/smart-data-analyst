{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82cee11-17f2-4ed5-baee-705a88300c45",
   "metadata": {},
   "source": [
    "## Agent Configuration  \n",
    "\n",
    "This section covers the configuration of the agent, including:  \n",
    "* Defining the data context that the agent will interact with  \n",
    "* Setting up the routine the agent will follow as a system prompt (embedding the data context)  \n",
    "* Establishing the list of tools available for the agent to complete its tasks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5c1752-981f-44ce-b140-9044912d3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_namespace_from_service_account():\n",
    "    \"\"\"\n",
    "    Reads the Kubernetes namespace from the service account mount point.\n",
    "    Returns 'default' if not running in a Kubernetes pod or if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    namespace_file = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n",
    "    try:\n",
    "        with open(namespace_file, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except IOError:\n",
    "        return 'default'\n",
    "\n",
    "# Set NAMESPACE variable\n",
    "NAMESPACE = get_namespace_from_service_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7d3743-22e6-4b47-9844-6172563a43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (same as above)\n",
    "NAMESPACE = get_namespace_from_service_account()\n",
    "PG_SERVICE_NAME = \"vince-retail-postgres\"\n",
    "PG_DATABASE_NAME = \"vince-retail\"\n",
    "POSTGRES_PASSWORD = \"postgres\"\n",
    "\n",
    "def get_db_connection(retries=3, delay=5):\n",
    "    \"\"\"Get connection with retries\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                host=f\"{PG_SERVICE_NAME}.{NAMESPACE}.svc.cluster.local\",\n",
    "                database=PG_DATABASE_NAME,\n",
    "                user=\"postgres\",\n",
    "                password=POSTGRES_PASSWORD,\n",
    "                port=\"5432\",\n",
    "                connect_timeout=5\n",
    "            )\n",
    "            return conn\n",
    "        except psycopg2.OperationalError as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            print(f\"Connection failed (attempt {attempt + 1}), retrying...\")\n",
    "            time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6365318b-52d9-4fc4-82f4-3b0a2cd2bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_type\" in Model has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "HTTP Request: GET https://llama-3-1-8b-6efc4543-predictor-ezai-services.hpepcai-ingress.pcai.hpecic.net/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Read JWT Token\n",
    "def get_nvidia_auth_token():\n",
    "    token_path = Path(\"/etc/secrets/ezua/.auth_token\")\n",
    "    if token_path.exists():\n",
    "        with open(token_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    raise ValueError(\"NVIDIA auth token not found at /etc/secrets/ezua/.auth_token\")\n",
    "\n",
    "nvidia_api_key = get_nvidia_auth_token()\n",
    "\n",
    "# 2. NVIDIA NIM Setup\n",
    "llm = NVIDIA(\n",
    "    base_url=\"https://llama-3-1-8b-6efc4543-predictor-ezai-services.hpepcai-ingress.pcai.hpecic.net/v1\",\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    api_key=nvidia_api_key,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "Settings.llm = llm\n",
    "\n",
    "embed_model = NVIDIAEmbedding(\n",
    "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "    base_url=\"https://embedding-v5-8da38070-predictor-ezai-services.hpepcai-ingress.pcai.hpecic.net/v1\",\n",
    "    api_key=nvidia_api_key,\n",
    "    truncate=\"END\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 3. System Prompt Configuration\n",
    "DATA_ENGINEER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a senior data engineer specializing in retail data cleaning and preparation for analytics. \n",
    "Your task is to clean and transform the vince-retail database which contains tables:\n",
    "- source_catalog (product information)\n",
    "- source_customers (customer information)\n",
    "- source_stock (inventory data)\n",
    "- source_orders (order headers)\n",
    "- source_order_products (order line items)\n",
    "\n",
    "Key data quality issues to address:\n",
    "1. Missing values (NULLs)\n",
    "2. Inconsistent categories/spelling (e.g., \"Toyz\" vs \"Toys\")\n",
    "3. Invalid dates (future dates or very old dates)\n",
    "4. Negative quantities\n",
    "5. Invalid emails\n",
    "6. Duplicate records\n",
    "7. Referential integrity issues\n",
    "\n",
    "Approach:\n",
    "1. First analyze the data to identify issues\n",
    "2. Propose cleaning strategies based on data type and business context\n",
    "3. Execute cleaning only after user confirmation\n",
    "4. Document all changes made\n",
    "\n",
    "Available operations:\n",
    "- Handle missing values (drop, impute, flag)\n",
    "- Standardize categorical values\n",
    "- Validate and correct dates\n",
    "- Fix numeric anomalies\n",
    "- Validate email formats\n",
    "- Deduplicate records\n",
    "- Enforce referential integrity\n",
    "\n",
    "Always:\n",
    "- Explain your reasoning\n",
    "- Show samples before/after changes\n",
    "- Preserve raw data (create cleaned views/tables)\n",
    "- Consider business impact of changes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0983834-aafc-4f22-b6c7-d2cdb3b0be33",
   "metadata": {},
   "source": [
    "## Agent Runtime\n",
    "This section covers the code executed while the agent is in action, including:\n",
    "* Preparing the tools for use by the agent\n",
    "* The agent's runtime function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073d593b-209e-4ffb-bb28-bf12279682c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProactiveDataEngineerAgent:\n",
    "    def __init__(self, db_tools):\n",
    "        self.tools = db_tools\n",
    "        self.cleaning_log = []\n",
    "        self.auto_approve = False  # Set to True for fully automatic mode\n",
    "        \n",
    "    def analyze_and_clean(self, table: str = None, auto_approve: bool = None):\n",
    "        \"\"\"Comprehensive analysis and automated cleaning with optional approval\"\"\"\n",
    "        if auto_approve is not None:\n",
    "            self.auto_approve = auto_approve\n",
    "            \n",
    "        print(f\"\\n{'='*40}\\nStarting Data Quality Assessment\\n{'='*40}\")\n",
    "        \n",
    "        # 1. Perform analysis\n",
    "        analysis = self.tools.get_data_issues_report()\n",
    "        if table:\n",
    "            analysis = {table: analysis.get(table, {})}\n",
    "        \n",
    "        # 2. Generate cleaning plan\n",
    "        cleaning_plan = self._generate_cleaning_plan(analysis)\n",
    "        \n",
    "        # 3. Execute cleaning with optional approval\n",
    "        results = self._execute_cleaning_plan(cleaning_plan)\n",
    "        \n",
    "        # 4. Generate report\n",
    "        self._generate_cleaning_report(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_cleaning_plan(self, analysis: Dict) -> List[Dict]:\n",
    "        \"\"\"Convert analysis into executable cleaning operations\"\"\"\n",
    "        cleaning_plan = []\n",
    "        \n",
    "        for table, issues in analysis.items():\n",
    "            print(f\"\\nAnalyzing {table}...\")\n",
    "            \n",
    "            # Handle null values\n",
    "            for col, null_count in [(k,v) for k,v in issues.items() if '_nulls' in k]:\n",
    "                col_name = col.replace('_nulls', '')\n",
    "                if null_count > 0:\n",
    "                    op = self._create_null_handling_operation(table, col_name, null_count)\n",
    "                    cleaning_plan.append(op)\n",
    "            \n",
    "            # Handle negative quantities\n",
    "            if 'negative_quantities' in issues and issues['negative_quantities'] > 0:\n",
    "                op = self._create_quantity_fix_operation(table, issues['negative_quantities'])\n",
    "                cleaning_plan.append(op)\n",
    "            \n",
    "            # Handle date issues\n",
    "            for col, future_count in [(k,v) for k,v in issues.items() if '_future_dates' in k]:\n",
    "                col_name = col.replace('_future_dates', '')\n",
    "                if future_count > 0:\n",
    "                    op = self._create_date_fix_operation(table, col_name, future_count)\n",
    "                    cleaning_plan.append(op)\n",
    "        \n",
    "        return cleaning_plan\n",
    "    \n",
    "    def _create_null_handling_operation(self, table: str, column: str, count: int) -> Dict:\n",
    "        \"\"\"Generate operation for handling null values\"\"\"\n",
    "        schema = self.tools.get_table_schema(table)\n",
    "        col_type = next((c['type'] for c in schema['columns'] if c['name'] == column), 'text')\n",
    "        \n",
    "        # Smart default values based on column type\n",
    "        if col_type in ['integer', 'bigint', 'numeric']:\n",
    "            default = 0\n",
    "            method = \"Zero-fill\"\n",
    "        elif col_type == 'boolean':\n",
    "            default = False\n",
    "            method = \"False-fill\"\n",
    "        elif 'date' in col_type.lower():\n",
    "            default = '1970-01-01'\n",
    "            method = \"Epoch-date-fill\"\n",
    "        else:\n",
    "            default = 'Unknown'\n",
    "            method = \"Unknown-fill\"\n",
    "        \n",
    "        return {\n",
    "            'id': f\"null_fix_{table}_{column}\",\n",
    "            'table': table,\n",
    "            'column': column,\n",
    "            'type': 'fill_nulls',\n",
    "            'value': default,\n",
    "            'description': f\"{method} for {count} null values in {table}.{column}\",\n",
    "            'severity': 'high' if count > 10 else 'medium'\n",
    "        }\n",
    "    \n",
    "    def _create_quantity_fix_operation(self, table: str, count: int) -> Dict:\n",
    "        \"\"\"Generate operation for fixing negative quantities\"\"\"\n",
    "        return {\n",
    "            'id': f\"quantity_fix_{table}\",\n",
    "            'table': table,\n",
    "            'column': 'product_quantity',\n",
    "            'type': 'fix_numeric_anomalies',\n",
    "            'description': f\"Correct {count} negative quantities in {table}\",\n",
    "            'action': 'abs',\n",
    "            'severity': 'high'\n",
    "        }\n",
    "    \n",
    "    def _create_date_fix_operation(self, table: str, column: str, count: int) -> Dict:\n",
    "        \"\"\"Generate operation for fixing invalid dates\"\"\"\n",
    "        return {\n",
    "            'id': f\"date_fix_{table}_{column}\",\n",
    "            'table': table,\n",
    "            'column': column,\n",
    "            'type': 'correct_dates',\n",
    "            'description': f\"Correct {count} invalid future dates in {table}.{column}\",\n",
    "            'correct_value': datetime.now().date(),\n",
    "            'severity': 'high'\n",
    "        }\n",
    "    \n",
    "    def _execute_cleaning_plan(self, plan: List[Dict]) -> Dict:\n",
    "        \"\"\"Execute cleaning operations with optional approval\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for op in sorted(plan, key=lambda x: x['severity'], reverse=True):\n",
    "            print(f\"\\nOperation: {op['description']}\")\n",
    "            print(f\"Type: {op['type']}\")\n",
    "            print(f\"Severity: {op['severity'].upper()}\")\n",
    "            \n",
    "            if not self.auto_approve:\n",
    "                confirm = input(\"Execute this operation? (y/n/a for all): \").lower()\n",
    "                if confirm == 'a':\n",
    "                    self.auto_approve = True\n",
    "                elif confirm != 'y':\n",
    "                    results[op['id']] = {'status': 'skipped'}\n",
    "                    continue\n",
    "            \n",
    "            try:\n",
    "                result = self.tools.clean_data([op])\n",
    "                results[op['id']] = result[op['id']]\n",
    "                self.cleaning_log.append({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'operation': op,\n",
    "                    'result': result\n",
    "                })\n",
    "                print(f\"Result: {result[op['id']]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing operation: {str(e)}\")\n",
    "                results[op['id']] = {'status': 'error', 'message': str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_cleaning_report(self, results: Dict):\n",
    "        \"\"\"Generate summary report of cleaning operations\"\"\"\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Data Cleaning Report\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        stats = {\n",
    "            'executed': 0,\n",
    "            'skipped': 0,\n",
    "            'errors': 0,\n",
    "            'rows_affected': 0\n",
    "        }\n",
    "        \n",
    "        for op_id, result in results.items():\n",
    "            if result.get('status') == 'success':\n",
    "                stats['executed'] += 1\n",
    "                stats['rows_affected'] += result.get('rows_affected', 0)\n",
    "            elif result.get('status') == 'skipped':\n",
    "                stats['skipped'] += 1\n",
    "            else:\n",
    "                stats['errors'] += 1\n",
    "        \n",
    "        print(f\"\\nOperations Executed: {stats['executed']}\")\n",
    "        print(f\"Operations Skipped: {stats['skipped']}\")\n",
    "        print(f\"Operations Failed: {stats['errors']}\")\n",
    "        print(f\"Total Rows Affected: {stats['rows_affected']}\")\n",
    "        \n",
    "        if stats['errors'] > 0:\n",
    "            print(\"\\nFailed Operations:\")\n",
    "            for op_id, result in results.items():\n",
    "                if result.get('status') not in ('success', 'skipped'):\n",
    "                    print(f\"- {op_id}: {result.get('message', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc41a824-4061-49ba-9a76-c74de4e1e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "class DataEngineerTools:\n",
    "    \"\"\"Tools for the data engineer agent to interact with the database using Polars\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conn = get_db_connection()\n",
    "    \n",
    "    def get_table_schema(self, table_name: str) -> Dict:\n",
    "        \"\"\"Get schema information for a table\"\"\"\n",
    "        with self.conn.cursor() as cursor:\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT column_name, data_type, is_nullable \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_name = %s\n",
    "            \"\"\", (table_name,))\n",
    "            return {\n",
    "                \"columns\": [\n",
    "                    {\"name\": row[0], \"type\": row[1], \"nullable\": row[2] == 'YES'} \n",
    "                    for row in cursor.fetchall()\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    def sample_table_data(self, table_name: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Get sample data from a table\"\"\"\n",
    "        with self.conn.cursor() as cursor:\n",
    "            cursor.execute(f\"SELECT * FROM {table_name} LIMIT %s\", (limit,))\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "    \n",
    "    def execute_query(self, query: str) -> pl.DataFrame:\n",
    "        \"\"\"Execute a SQL query and return results as Polars DataFrame\"\"\"\n",
    "        with self.conn.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            return pl.DataFrame(data, schema=columns)\n",
    "    \n",
    "    def get_data_issues_report(self) -> Dict:\n",
    "        \"\"\"Generate a data quality report using Polars\"\"\"\n",
    "        report = {}\n",
    "        tables = ['source_catalog', 'source_customers', 'source_stock', \n",
    "                 'source_orders', 'source_order_products']\n",
    "        \n",
    "        for table in tables:\n",
    "            report[table] = {}\n",
    "            schema = self.get_table_schema(table)\n",
    "            \n",
    "            # Count nulls per column\n",
    "            for col in schema['columns']:\n",
    "                null_count = self.execute_query(\n",
    "                    f\"SELECT COUNT(*) FROM {table} WHERE {col['name']} IS NULL\"\n",
    "                ).get_column(\"count\")[0]\n",
    "                report[table][f\"{col['name']}_nulls\"] = null_count\n",
    "                \n",
    "            # Check for negative quantities where applicable\n",
    "            if 'quantity' in [c['name'] for c in schema['columns']]:\n",
    "                neg_count = self.execute_query(\n",
    "                    f\"SELECT COUNT(*) FROM {table} WHERE product_quantity < 0\"\n",
    "                ).get_column(\"count\")[0]\n",
    "                report[table]['negative_quantities'] = neg_count\n",
    "                \n",
    "            # Check date validity for date columns\n",
    "            date_cols = [c['name'] for c in schema['columns'] if 'date' in c['name'].lower()]\n",
    "            for col in date_cols:\n",
    "                future_dates = self.execute_query(\n",
    "                    f\"\"\"SELECT COUNT(*) FROM {table} \n",
    "                    WHERE {col} > CURRENT_DATE + INTERVAL '1 year'\"\"\"\n",
    "                ).get_column(\"count\")[0]\n",
    "                report[table][f\"{col}_future_dates\"] = future_dates\n",
    "                \n",
    "        return report\n",
    "    \n",
    "    def clean_data(self, operations: List[Dict]) -> Dict:\n",
    "        \"\"\"Execute cleaning operations\"\"\"\n",
    "        results = {}\n",
    "        with self.conn.cursor() as cursor:\n",
    "            for op in operations:\n",
    "                try:\n",
    "                    if op['type'] == 'fill_nulls':\n",
    "                        cursor.execute(f\"\"\"\n",
    "                            UPDATE {op['table']} \n",
    "                            SET {op['column']} = %s \n",
    "                            WHERE {op['column']} IS NULL\n",
    "                        \"\"\", (op['value'],))\n",
    "                        results[op['id']] = {\n",
    "                            \"status\": \"success\", \n",
    "                            \"rows_affected\": cursor.rowcount\n",
    "                        }\n",
    "                        \n",
    "                    elif op['type'] == 'fix_numeric_anomalies':\n",
    "                        if op['action'] == 'abs':\n",
    "                            cursor.execute(f\"\"\"\n",
    "                                UPDATE {op['table']} \n",
    "                                SET {op['column']} = ABS({op['column']})\n",
    "                                WHERE {op['column']} < 0\n",
    "                            \"\"\")\n",
    "                        elif op['action'] == 'set':\n",
    "                            cursor.execute(f\"\"\"\n",
    "                                UPDATE {op['table']} \n",
    "                                SET {op['column']} = %s \n",
    "                                WHERE {op['column']} < 0\n",
    "                            \"\"\", (op['value'],))\n",
    "                        results[op['id']] = {\n",
    "                            \"status\": \"success\", \n",
    "                            \"rows_affected\": cursor.rowcount\n",
    "                        }\n",
    "                    \n",
    "                    elif op['type'] == 'standardize_category':\n",
    "                        for wrong, correct in op['mappings'].items():\n",
    "                            cursor.execute(f\"\"\"\n",
    "                                UPDATE {op['table']} \n",
    "                                SET {op['column']} = %s \n",
    "                                WHERE {op['column']} = %s\n",
    "                            \"\"\", (correct, wrong))\n",
    "                        results[op['id']] = {\n",
    "                            \"status\": \"success\", \n",
    "                            \"rows_affected\": cursor.rowcount\n",
    "                        }\n",
    "                    \n",
    "                    elif op['type'] == 'remove_duplicates':\n",
    "                        cursor.execute(f\"\"\"\n",
    "                            CREATE TEMP TABLE temp_{op['table']} AS \n",
    "                            SELECT DISTINCT ON ({','.join(op['key_columns'])}) * \n",
    "                            FROM {op['table']}\n",
    "                        \"\"\")\n",
    "                        cursor.execute(f\"TRUNCATE {op['table']}\")\n",
    "                        cursor.execute(f\"\"\"\n",
    "                            INSERT INTO {op['table']} \n",
    "                            SELECT * FROM temp_{op['table']}\n",
    "                        \"\"\")\n",
    "                        cursor.execute(f\"DROP TABLE temp_{op['table']}\")\n",
    "                        results[op['id']] = {\"status\": \"success\"}\n",
    "                        \n",
    "                    elif op['type'] == 'correct_dates':\n",
    "                        cursor.execute(f\"\"\"\n",
    "                            UPDATE {op['table']} \n",
    "                            SET {op['column']} = %s \n",
    "                            WHERE {op['column']} > CURRENT_DATE + INTERVAL '1 year'\n",
    "                                OR {op['column']} < DATE '1900-01-01'\n",
    "                        \"\"\", (op['correct_value'],))\n",
    "                        results[op['id']] = {\n",
    "                            \"status\": \"success\", \n",
    "                            \"rows_affected\": cursor.rowcount\n",
    "                        }\n",
    "                        \n",
    "                    elif op['type'] == 'validate_emails':\n",
    "                        cursor.execute(f\"\"\"\n",
    "                            UPDATE {op['table']} \n",
    "                            SET {op['column']} = %s \n",
    "                            WHERE {op['column']} IS NOT NULL\n",
    "                            AND {op['column']} !~ '^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$'\n",
    "                        \"\"\", (op['default_value'],))\n",
    "                        results[op['id']] = {\n",
    "                            \"status\": \"success\", \n",
    "                            \"rows_affected\": cursor.rowcount\n",
    "                        }\n",
    "                        \n",
    "                    self.conn.commit()\n",
    "                except Exception as e:\n",
    "                    self.conn.rollback()\n",
    "                    results[op['id']] = {\"status\": \"error\", \"message\": str(e)}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12660a-7794-4b2f-8397-a4196d9c31c3",
   "metadata": {},
   "source": [
    "## Running the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7e428d-bf41-43fd-bc84-1d79e214381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Proactive Data Engineer Agent\n",
      "    ----------------------------\n",
      "    This agent will:\n",
      "    1. Analyze your data for quality issues\n",
      "    2. Propose automated fixes\n",
      "    3. Execute fixes with your approval (unless in auto mode)\n",
      "    4. Generate detailed reports\n",
      "    \n",
      "    Available commands:\n",
      "    - analyze [table]: Analyze specific table or all tables\n",
      "    - auto on/off: Toggle auto-approval mode\n",
      "    - history: Show cleaning history\n",
      "    - exit: End session\n",
      "    \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Command:  analyze\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting Data Quality Assessment\n",
      "========================================\n",
      "\n",
      "Analyzing source_catalog...\n",
      "\n",
      "Analyzing source_customers...\n",
      "\n",
      "Analyzing source_stock...\n",
      "\n",
      "Analyzing source_orders...\n",
      "\n",
      "Analyzing source_order_products...\n",
      "\n",
      "Operation: Zero-fill for 3 null values in source_catalog.price_cents\n",
      "Type: fill_nulls\n",
      "Severity: MEDIUM\n",
      "Result: {'status': 'success', 'rows_affected': 3}\n",
      "\n",
      "Operation: Unknown-fill for 6 null values in source_catalog.product_name\n",
      "Type: fill_nulls\n",
      "Severity: MEDIUM\n",
      "Result: {'status': 'success', 'rows_affected': 6}\n",
      "\n",
      "Operation: Unknown-fill for 5 null values in source_catalog.product_category\n",
      "Type: fill_nulls\n",
      "Severity: MEDIUM\n",
      "Result: {'status': 'success', 'rows_affected': 5}\n",
      "\n",
      "Operation: Unknown-fill for 5 null values in source_customers.customer_surname\n",
      "Type: fill_nulls\n",
      "Severity: MEDIUM\n",
      "Result: {'status': 'success', 'rows_affected': 5}\n",
      "\n",
      "Operation: Unknown-fill for 4 null values in source_customers.customer_email\n",
      "Type: fill_nulls\n",
      "Severity: MEDIUM\n",
      "Result: {'status': 'success', 'rows_affected': 4}\n",
      "\n",
      "Operation: Zero-fill for 6 null values in source_stock.purchase_price_cents\n",
      "Type: fill_nulls\n",
      "Severity: MEDIUM\n",
      "Result: {'status': 'success', 'rows_affected': 6}\n",
      "\n",
      "Operation: Epoch-date-fill for 9 null values in source_stock.entry_date\n",
      "Type: fill_nulls\n",
      "Severity: MEDIUM\n",
      "Result: {'status': 'success', 'rows_affected': 9}\n",
      "\n",
      "Operation: Unknown-fill for 13 null values in source_customers.customer_name\n",
      "Type: fill_nulls\n",
      "Severity: HIGH\n",
      "Result: {'status': 'success', 'rows_affected': 13}\n",
      "\n",
      "Operation: Zero-fill for 161 null values in source_stock.product_quantity\n",
      "Type: fill_nulls\n",
      "Severity: HIGH\n",
      "Result: {'status': 'success', 'rows_affected': 161}\n",
      "\n",
      "Operation: Zero-fill for 12 null values in source_orders.customer_id\n",
      "Type: fill_nulls\n",
      "Severity: HIGH\n",
      "Result: {'status': 'error', 'message': 'insert or update on table \"source_orders\" violates foreign key constraint \"source_orders_customer_id_fkey\"\\nDETAIL:  Key (customer_id)=(0) is not present in table \"source_customers\".\\n'}\n",
      "\n",
      "Operation: Epoch-date-fill for 11 null values in source_orders.order_date\n",
      "Type: fill_nulls\n",
      "Severity: HIGH\n",
      "Result: {'status': 'success', 'rows_affected': 11}\n",
      "\n",
      "Operation: Correct 3 invalid future dates in source_orders.order_date\n",
      "Type: correct_dates\n",
      "Severity: HIGH\n",
      "Result: {'status': 'success', 'rows_affected': 3}\n",
      "\n",
      "Operation: Zero-fill for 587 null values in source_order_products.product_quantity\n",
      "Type: fill_nulls\n",
      "Severity: HIGH\n",
      "Result: {'status': 'success', 'rows_affected': 587}\n",
      "\n",
      "========================================\n",
      "Data Cleaning Report\n",
      "========================================\n",
      "\n",
      "Operations Executed: 12\n",
      "Operations Skipped: 0\n",
      "Operations Failed: 1\n",
      "Total Rows Affected: 813\n",
      "\n",
      "Failed Operations:\n",
      "- null_fix_source_orders_customer_id: insert or update on table \"source_orders\" violates foreign key constraint \"source_orders_customer_id_fkey\"\n",
      "DETAIL:  Key (customer_id)=(0) is not present in table \"source_customers\".\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Command:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session ended. Final cleaning summary:\n",
      "- Zero-fill for 161 null values in source_stock.product_quantity\n",
      "  Rows affected: N/A\n",
      "- Zero-fill for 12 null values in source_orders.customer_id\n",
      "  Rows affected: N/A\n",
      "- Epoch-date-fill for 11 null values in source_orders.order_date\n",
      "  Rows affected: N/A\n",
      "- Correct 3 invalid future dates in source_orders.order_date\n",
      "  Rows affected: N/A\n",
      "- Zero-fill for 587 null values in source_order_products.product_quantity\n",
      "  Rows affected: N/A\n"
     ]
    }
   ],
   "source": [
    "def run_proactive_data_engineer(auto_mode=True):\n",
    "    \"\"\"Run the proactive data engineer with optional auto-approval\"\"\"\n",
    "    tools = DataEngineerTools()\n",
    "    agent = ProactiveDataEngineerAgent(tools)\n",
    "    agent.auto_approve = auto_mode\n",
    "    \n",
    "    print(\"\"\"\n",
    "    Proactive Data Engineer Agent\n",
    "    ----------------------------\n",
    "    This agent will:\n",
    "    1. Analyze your data for quality issues\n",
    "    2. Propose automated fixes\n",
    "    3. Execute fixes with your approval (unless in auto mode)\n",
    "    4. Generate detailed reports\n",
    "    \n",
    "    Available commands:\n",
    "    - analyze [table]: Analyze specific table or all tables\n",
    "    - auto on/off: Toggle auto-approval mode\n",
    "    - history: Show cleaning history\n",
    "    - exit: End session\n",
    "    \"\"\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            nvidia_api_key = get_nvidia_auth_token()\n",
    "            command = input(\"\\nCommand: \").strip().lower()\n",
    "            \n",
    "            if command in ['exit', 'quit']:\n",
    "                break\n",
    "            elif command.startswith('analyze'):\n",
    "                table = command.split()[1] if len(command.split()) > 1 else None\n",
    "                agent.analyze_and_clean(table)\n",
    "            elif command == 'auto on':\n",
    "                agent.auto_approve = True\n",
    "                print(\"Auto-approval mode enabled\")\n",
    "            elif command == 'auto off':\n",
    "                agent.auto_approve = False\n",
    "                print(\"Auto-approval mode disabled\")\n",
    "            elif command == 'history':\n",
    "                print(\"\\nCleaning History:\")\n",
    "                for log in agent.cleaning_log:\n",
    "                    print(f\"{log['timestamp']}: {log['operation']['description']}\")\n",
    "                    print(f\"  Result: {log['result']}\")\n",
    "            elif command == 'help':\n",
    "                print(\"Commands: analyze [table], auto on/off, history, exit\")\n",
    "            else:\n",
    "                print(\"Invalid command. Type 'help' for options\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nSession ended. Final cleaning summary:\")\n",
    "    for log in agent.cleaning_log[-5:]:  # Show last 5 operations\n",
    "        print(f\"- {log['operation']['description']}\")\n",
    "        print(f\"  Rows affected: {log['result'].get('rows_affected', 'N/A')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_proactive_data_engineer(auto_mode=True)  # Set to True for fully automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b4760-313f-422a-9cab-4c91bc1ca295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
