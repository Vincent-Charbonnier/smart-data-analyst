{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdced2-2f27-46db-ae08-4f85ff96215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70a296-ccee-4d63-81b1-f5968ba9fc73",
   "metadata": {},
   "source": [
    "## Prepare - Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58873766-eb5e-4345-bc90-74579db9c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_namespace_from_service_account():\n",
    "    \"\"\"\n",
    "    Reads the Kubernetes namespace from the service account mount point.\n",
    "    Returns 'default' if not running in a Kubernetes pod or if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    namespace_file = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n",
    "    try:\n",
    "        with open(namespace_file, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except IOError:\n",
    "        return 'default'\n",
    "\n",
    "# Global configuration\n",
    "NAMESPACE = get_namespace_from_service_account()\n",
    "POSTGRES_PASSWORD = \"postgres\"\n",
    "PG_SERVICE_NAME = \"vince-retail-postgres\"\n",
    "PG_DATABASE_NAME = \"vince-retail\"\n",
    "\n",
    "def resource_exists(resource_type, name, namespace):\n",
    "    \"\"\"Check if a Kubernetes resource exists\"\"\"\n",
    "    cmd = f\"kubectl get {resource_type} {name} -n {namespace} --ignore-not-found --no-headers\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return bool(result.stdout.strip())\n",
    "\n",
    "def deploy_postgresql():\n",
    "    \"\"\"Deploy PostgreSQL with proper permission handling and PVC management\"\"\"\n",
    "    # Generate unique PVC name\n",
    "    pvc_name = f\"postgres-pvc-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Delete any existing deployment to start fresh (if we have permissions)\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                f\"kubectl delete deployment -n {NAMESPACE} {PG_SERVICE_NAME} --ignore-not-found\",\n",
    "                shell=True, check=True\n",
    "            )\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Warning: Could not delete existing deployment (may not have permissions): {e}\")\n",
    "\n",
    "        # 2. Create PVC only if it doesn't exist\n",
    "        if not resource_exists(\"pvc\", pvc_name, NAMESPACE):\n",
    "            try:\n",
    "                pvc = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: {pvc_name}\n",
    "  namespace: {NAMESPACE}\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "                \"\"\"\n",
    "                subprocess.run(f\"echo '{pvc}' | kubectl apply -f -\", shell=True, check=True)\n",
    "                print(f\"Created new PVC: {pvc_name}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error creating PVC with unique name, trying default name: {e}\")\n",
    "                # Try with a default name if the timestamp-based one fails\n",
    "                pvc_name = \"postgres-pvc\"\n",
    "                if not resource_exists(\"pvc\", pvc_name, NAMESPACE):\n",
    "                    pvc = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: {pvc_name}\n",
    "  namespace: {NAMESPACE}\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "                    \"\"\"\n",
    "                    subprocess.run(f\"echo '{pvc}' | kubectl apply -f -\", shell=True, check=True)\n",
    "        else:\n",
    "            print(f\"Using existing PVC: {pvc_name}\")\n",
    "\n",
    "        # 3. Create Deployment with simplified permission handling\n",
    "        deployment = f\"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: {PG_SERVICE_NAME}\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {PG_SERVICE_NAME}\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {PG_SERVICE_NAME}\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: {PG_SERVICE_NAME}\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "    spec:\n",
    "      securityContext:\n",
    "        fsGroup: 999\n",
    "        runAsUser: 999\n",
    "        runAsGroup: 999\n",
    "      containers:\n",
    "      - name: postgres\n",
    "        image: postgres:13\n",
    "        env:\n",
    "        - name: POSTGRES_PASSWORD\n",
    "          value: \"{POSTGRES_PASSWORD}\"\n",
    "        - name: POSTGRES_DB\n",
    "          value: \"{PG_DATABASE_NAME}\"\n",
    "        - name: PGDATA\n",
    "          value: \"/var/lib/postgresql/data/pgdata\"\n",
    "        ports:\n",
    "        - containerPort: 5432\n",
    "        volumeMounts:\n",
    "        - name: postgres-data\n",
    "          mountPath: /var/lib/postgresql/data\n",
    "          subPath: postgres\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"1\"\n",
    "        readinessProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - pg_isready\n",
    "            - -U\n",
    "            - postgres\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "        livenessProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - pg_isready\n",
    "            - -U\n",
    "            - postgres\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "      volumes:\n",
    "      - name: postgres-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: {pvc_name}\n",
    "        \"\"\"\n",
    "        \n",
    "        subprocess.run(f\"echo '{deployment}' | kubectl apply -f -\", shell=True, check=True)\n",
    "        \n",
    "        # 4. Create Service if it doesn't exist\n",
    "        if not resource_exists(\"service\", PG_SERVICE_NAME, NAMESPACE):\n",
    "            service = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: {PG_SERVICE_NAME}\n",
    "  namespace: {NAMESPACE}\n",
    "spec:\n",
    "  selector:\n",
    "    app: {PG_SERVICE_NAME}\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 5432\n",
    "      targetPort: 5432\n",
    "  type: ClusterIP\n",
    "            \"\"\"\n",
    "            subprocess.run(f\"echo '{service}' | kubectl apply -f -\", shell=True, check=True)\n",
    "        \n",
    "        print(\"Waiting for PostgreSQL to initialize...\")\n",
    "        time.sleep(30)\n",
    "        \n",
    "        # Check status\n",
    "        pod_name = get_pod_name(NAMESPACE, PG_SERVICE_NAME)\n",
    "        if pod_name:\n",
    "            print(f\"\\nPostgreSQL pod: {pod_name}\")\n",
    "            print(\"\\nPod status:\")\n",
    "            subprocess.run(f\"kubectl get pod -n {NAMESPACE} {pod_name}\", shell=True)\n",
    "            \n",
    "            print(\"\\nPod logs:\")\n",
    "            subprocess.run(f\"kubectl logs -n {NAMESPACE} {pod_name}\", shell=True)\n",
    "        else:\n",
    "            print(\"Could not find PostgreSQL pod\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deploying PostgreSQL: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_pod_name(namespace, service_name):\n",
    "    \"\"\"Get the PostgreSQL pod name\"\"\"\n",
    "    cmd = f\"kubectl get pods -n {namespace} -l app={service_name} -o jsonpath='{{.items[0].metadata.name}}'\"\n",
    "    try:\n",
    "        return subprocess.run(\n",
    "            cmd, shell=True, check=True, capture_output=True, text=True\n",
    "        ).stdout.strip(\"'\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "\n",
    "# Run the deployment\n",
    "deploy_postgresql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be980a-8278-4276-8fda-bf1b93f16d02",
   "metadata": {},
   "source": [
    "## Prepare - Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79268c11-3d15-49ec-95dd-6e488dceccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration (same as above)\n",
    "NAMESPACE = get_namespace_from_service_account()\n",
    "PG_SERVICE_NAME = \"vince-retail-postgres\"\n",
    "PG_DATABASE_NAME = \"vince-retail\"\n",
    "POSTGRES_PASSWORD = \"postgres\"\n",
    "\n",
    "def get_db_connection(retries=3, delay=5):\n",
    "    \"\"\"Get connection with retries\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                host=f\"{PG_SERVICE_NAME}.{NAMESPACE}.svc.cluster.local\",\n",
    "                database=PG_DATABASE_NAME,\n",
    "                user=\"postgres\",\n",
    "                password=POSTGRES_PASSWORD,\n",
    "                port=\"5432\",\n",
    "                connect_timeout=5\n",
    "            )\n",
    "            return conn\n",
    "        except psycopg2.OperationalError as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            print(f\"Connection failed (attempt {attempt + 1}), retrying...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "def initialize_database():\n",
    "    \"\"\"Create tables and load sample data\"\"\"\n",
    "    try:\n",
    "        print(\"Initializing database...\")\n",
    "        \n",
    "        # 1. Create tables\n",
    "        print(\"Creating tables...\")\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS source_catalog (\n",
    "            product_id INT PRIMARY KEY,\n",
    "            product_name VARCHAR(100),\n",
    "            product_category VARCHAR(50),\n",
    "            price_cents INT\n",
    "        );''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS source_customers (\n",
    "            customer_id INT PRIMARY KEY,\n",
    "            customer_name VARCHAR(50),\n",
    "            customer_surname VARCHAR(50),\n",
    "            customer_email VARCHAR(100)\n",
    "        );''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS source_stock (\n",
    "            entry_id INT PRIMARY KEY,\n",
    "            product_id INT REFERENCES source_catalog(product_id),\n",
    "            product_quantity INT,\n",
    "            purchase_price_cents INT,\n",
    "            entry_date DATE\n",
    "        );''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS source_orders (\n",
    "            order_id INT PRIMARY KEY,\n",
    "            customer_id INT REFERENCES source_customers(customer_id),\n",
    "            order_status VARCHAR(20),\n",
    "            order_date DATE\n",
    "        );''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS source_order_products (\n",
    "            transaction_id INT PRIMARY KEY,\n",
    "            order_id INT REFERENCES source_orders(order_id),\n",
    "            product_id INT REFERENCES source_catalog(product_id),\n",
    "            product_quantity INT\n",
    "        );''')\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Tables created successfully\")\n",
    "        \n",
    "        # 2. Load sample data\n",
    "        print(\"Loading sample data...\")\n",
    "        data = generate_sample_data()\n",
    "        \n",
    "        # Load products\n",
    "        for p in data[\"products\"]:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO source_catalog VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (product_id) DO NOTHING\n",
    "            ''', (p[\"product_id\"], p[\"product_name\"], p[\"product_category\"], p[\"price_cents\"]))\n",
    "        \n",
    "        # Load customers\n",
    "        for c in data[\"customers\"]:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO source_customers VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (customer_id) DO NOTHING\n",
    "            ''', (c[\"customer_id\"], c[\"customer_name\"], c[\"customer_surname\"], c[\"customer_email\"]))\n",
    "        \n",
    "        # Load stock\n",
    "        for s in data[\"stock\"]:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO source_stock VALUES (%s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (entry_id) DO NOTHING\n",
    "            ''', (s[\"entry_id\"], s[\"product_id\"], s[\"product_quantity\"], s[\"purchase_price_cents\"], s[\"entry_date\"]))\n",
    "        \n",
    "        # Load orders\n",
    "        for o in data[\"orders\"]:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO source_orders VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (order_id) DO NOTHING\n",
    "            ''', (o[\"order_id\"], o[\"customer_id\"], o[\"order_status\"], o[\"order_date\"]))\n",
    "        \n",
    "        # Load order products\n",
    "        for op in data[\"order_products\"]:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO source_order_products VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (transaction_id) DO NOTHING\n",
    "            ''', (op[\"transaction_id\"], op[\"order_id\"], op[\"product_id\"], op[\"product_quantity\"]))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Sample data loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error during initialization: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Generate sample data function (same as before)\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate realistic sample data\"\"\"\n",
    "    categories = [\"Toys\", \"Clothing\", \"Books\", \"Electronics\", \"Home Decor\"]\n",
    "    first_names = [\"Emma\", \"Liam\", \"Olivia\", \"Noah\", \"Ava\"]\n",
    "    last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\"]\n",
    "    statuses = [\"completed\", \"pending\", \"cancelled\", \"shipped\"]\n",
    "    \n",
    "    # Products\n",
    "    products = [{\n",
    "        \"product_id\": i,\n",
    "        \"product_name\": f\"{random.choice(categories)[:-1]} {i}\",\n",
    "        \"product_category\": random.choice(categories),\n",
    "        \"price_cents\": random.randint(500, 10000)\n",
    "    } for i in range(1, 101)]\n",
    "    \n",
    "    # Customers\n",
    "    customers = [{\n",
    "        \"customer_id\": i,\n",
    "        \"customer_name\": random.choice(first_names),\n",
    "        \"customer_surname\": random.choice(last_names),\n",
    "        \"customer_email\": f\"{random.choice(first_names).lower()}.{random.choice(last_names).lower()}@example.com\"\n",
    "    } for i in range(1, 51)]\n",
    "    \n",
    "    # Stock\n",
    "    stock = []\n",
    "    entry_id = 1\n",
    "    for product in products:\n",
    "        for _ in range(random.randint(1, 3)):\n",
    "            stock.append({\n",
    "                \"entry_id\": entry_id,\n",
    "                \"product_id\": product[\"product_id\"],\n",
    "                \"product_quantity\": random.randint(10, 100),\n",
    "                \"purchase_price_cents\": int(product[\"price_cents\"] * 0.7),\n",
    "                \"entry_date\": (datetime.now() - timedelta(days=random.randint(1, 365))).strftime(\"%Y-%m-%d\")\n",
    "            })\n",
    "            entry_id += 1\n",
    "    \n",
    "    # Orders\n",
    "    orders = [{\n",
    "        \"order_id\": i,\n",
    "        \"customer_id\": random.randint(1, 50),\n",
    "        \"order_status\": random.choice(statuses),\n",
    "        \"order_date\": (datetime.now() - timedelta(days=random.randint(1, 90))).strftime(\"%Y-%m-%d\")\n",
    "    } for i in range(1, 201)]\n",
    "    \n",
    "    # Order Products\n",
    "    order_products = []\n",
    "    transaction_id = 1\n",
    "    for order in orders:\n",
    "        for _ in range(random.randint(1, 5)):\n",
    "            product = random.choice(products)\n",
    "            order_products.append({\n",
    "                \"transaction_id\": transaction_id,\n",
    "                \"order_id\": order[\"order_id\"],\n",
    "                \"product_id\": product[\"product_id\"],\n",
    "                \"product_quantity\": random.randint(1, 3)\n",
    "            })\n",
    "            transaction_id += 1\n",
    "    \n",
    "    return {\n",
    "        \"products\": products,\n",
    "        \"customers\": customers,\n",
    "        \"stock\": stock,\n",
    "        \"orders\": orders,\n",
    "        \"order_products\": order_products\n",
    "    }\n",
    "\n",
    "# Run initialization separately\n",
    "initialize_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a77620-ce06-4854-b6f8-a2fce5cdea9b",
   "metadata": {},
   "source": [
    "## Verifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab96455-0cb4-4d75-8b75-b44400f0ba47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def verify_data():\n",
    "    \"\"\"Verify data exists by querying each table\"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(\"\\nVerifying data...\")\n",
    "        \n",
    "        # Check products\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM source_catalog\")\n",
    "        product_count = cursor.fetchone()[0]\n",
    "        print(f\"Products: {product_count} records\")\n",
    "        \n",
    "        # Check customers\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM source_customers\")\n",
    "        customer_count = cursor.fetchone()[0]\n",
    "        print(f\"Customers: {customer_count} records\")\n",
    "        \n",
    "        # Check stock\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM source_stock\")\n",
    "        stock_count = cursor.fetchone()[0]\n",
    "        print(f\"Stock entries: {stock_count} records\")\n",
    "        \n",
    "        # Check orders\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM source_orders\")\n",
    "        order_count = cursor.fetchone()[0]\n",
    "        print(f\"Orders: {order_count} records\")\n",
    "        \n",
    "        # Check order products\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM source_order_products\")\n",
    "        transaction_count = cursor.fetchone()[0]\n",
    "        print(f\"Order products: {transaction_count} records\")\n",
    "        \n",
    "        # Sample some data\n",
    "        print(\"\\nSample product:\")\n",
    "        cursor.execute(\"SELECT * FROM source_catalog LIMIT 1\")\n",
    "        print(cursor.fetchone())\n",
    "        \n",
    "        print(\"\\nSample customer:\")\n",
    "        cursor.execute(\"SELECT * FROM source_customers LIMIT 1\")\n",
    "        print(cursor.fetchone())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying data: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Run verification\n",
    "verify_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c808b-d180-4254-ac91-e2343caa560a",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727233f5-77fd-4760-90da-08c31c522baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyArrow\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_star_schema_matplotlib():\n",
    "    \"\"\"Draw star schema using matplotlib (no Graphviz required)\"\"\"\n",
    "    try:\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        ax.set_xlim(0, 10)\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Table definitions\n",
    "        tables = {\n",
    "            'source_catalog': {\n",
    "                'position': (2, 7),\n",
    "                'columns': ['product_id (PK)', 'product_name', 'product_category', 'price_cents'],\n",
    "                'color': '#a6cee3'\n",
    "            },\n",
    "            'source_customers': {\n",
    "                'position': (2, 3),\n",
    "                'columns': ['customer_id (PK)', 'customer_name', 'customer_surname', 'customer_email'],\n",
    "                'color': '#b2df8a'\n",
    "            },\n",
    "            'source_stock': {\n",
    "                'position': (5, 8),\n",
    "                'columns': ['entry_id (PK)', 'product_id (FK)', 'product_quantity', 'purchase_price_cents', 'entry_date'],\n",
    "                'color': '#fb9a99'\n",
    "            },\n",
    "            'source_orders': {\n",
    "                'position': (5, 5),\n",
    "                'columns': ['order_id (PK)', 'customer_id (FK)', 'order_status', 'order_date'],\n",
    "                'color': '#fdbf6f'\n",
    "            },\n",
    "            'source_order_products': {\n",
    "                'position': (8, 6),\n",
    "                'columns': ['transaction_id (PK)', 'order_id (FK)', 'product_id (FK)', 'product_quantity'],\n",
    "                'color': '#cab2d6'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Draw tables\n",
    "        for table, props in tables.items():\n",
    "            x, y = props['position']\n",
    "            \n",
    "            # Draw table box\n",
    "            table_box = Rectangle((x, y), 2, 0.3 + 0.2 * len(props['columns']), \n",
    "                                facecolor=props['color'], edgecolor='black')\n",
    "            ax.add_patch(table_box)\n",
    "            \n",
    "            # Add table name\n",
    "            ax.text(x + 1, y + 0.2 + 0.2 * len(props['columns']), table, \n",
    "                   ha='center', va='center', weight='bold')\n",
    "            \n",
    "            # Add columns\n",
    "            for i, col in enumerate(props['columns']):\n",
    "                ax.text(x + 0.1, y + 0.15 + 0.2 * (len(props['columns']) - i - 1), \n",
    "                       col, ha='left', va='center', fontsize=9)\n",
    "        \n",
    "        # Draw relationships\n",
    "        relationships = [\n",
    "            ('source_catalog', 'source_stock', '1:N'),\n",
    "            ('source_catalog', 'source_order_products', '1:N'),\n",
    "            ('source_customers', 'source_orders', '1:N'),\n",
    "            ('source_orders', 'source_order_products', '1:N')\n",
    "        ]\n",
    "        \n",
    "        for src, dst, label in relationships:\n",
    "            src_x = tables[src]['position'][0] + 2\n",
    "            src_y = tables[src]['position'][1] + (0.3 + 0.2 * len(tables[src]['columns'])) / 2\n",
    "            dst_x = tables[dst]['position'][0]\n",
    "            dst_y = tables[dst]['position'][1] + (0.3 + 0.2 * len(tables[dst]['columns'])) / 2\n",
    "            \n",
    "            arrow = FancyArrow(src_x, src_y, dst_x - src_x, dst_y - src_y, \n",
    "                              width=0.01, head_width=0.1, head_length=0.1, \n",
    "                              length_includes_head=True, color='black')\n",
    "            ax.add_patch(arrow)\n",
    "            \n",
    "            # Add relationship label\n",
    "            label_x = (src_x + dst_x) / 2\n",
    "            label_y = (src_y + dst_y) / 2\n",
    "            ax.text(label_x, label_y, label, ha='center', va='center', \n",
    "                   bbox=dict(facecolor='white', edgecolor='none', pad=1))\n",
    "        \n",
    "        plt.title('Vince Retail Database - Star Schema', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating diagram: {e}\")\n",
    "\n",
    "# Generate and display the diagram\n",
    "draw_star_schema_matplotlib()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35431717-376b-4c1c-ae09-7c484202ddcb",
   "metadata": {},
   "source": [
    "## Agent Configuration  \n",
    "\n",
    "This section covers the configuration of the agent, including:  \n",
    "* Defining the data context that the agent will interact with  \n",
    "* Setting up the routine the agent will follow as a system prompt (embedding the data context)  \n",
    "* Establishing the list of tools available for the agent to complete its tasks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab2171-4f8b-45c7-a0fb-088fed7a8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a7614-cdad-47ec-85a0-18ffb16f9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Import Librairies\n",
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "import json\n",
    "import inspect\n",
    "from pandas import DataFrame\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500522e9-c9b5-4d47-bde5-c1ef6af6c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read JWT Token\n",
    "def get_nvidia_auth_token():\n",
    "    token_path = Path(\"/etc/secrets/ezua/.auth_token\")\n",
    "    if token_path.exists():\n",
    "        with open(token_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    raise ValueError(\"NVIDIA auth token not found at /etc/secrets/ezua/.auth_token\")\n",
    "\n",
    "nvidia_api_key = get_nvidia_auth_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170204c0-8b8e-4626-ab3d-d93e98f8afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration\n",
    "NAMESPACE = get_namespace_from_service_account()\n",
    "PG_SERVICE_NAME = \"vince-retail-postgres\"\n",
    "PG_DATABASE_NAME = \"vince-retail\"\n",
    "POSTGRES_PASSWORD = \"postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d4123-8191-4392-9bb5-e6ffae7a453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NVIDIA NIM Setup\n",
    "llm = NVIDIA(\n",
    "    base_url=\"https://llama-3-1-8b-6efc4543-predictor-ezai-services.hpepcai-ingress.pcai.hpecic.net/v1\",\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    api_key=nvidia_api_key,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6360967-9ff9-4fd3-b4db-2c05cc8d6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PostgreSQL Connection\n",
    "def get_postgres_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=f\"{PG_SERVICE_NAME}.{NAMESPACE}.svc.cluster.local\",\n",
    "        database=PG_DATABASE_NAME,\n",
    "        user=\"postgres\",\n",
    "        password=POSTGRES_PASSWORD,\n",
    "        port=\"5432\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673a2aa-51af-4399-a42e-ed5f4d2b06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Database Schema Query\n",
    "def query_postgres_dictionary():\n",
    "    query = '''\n",
    "    SELECT \n",
    "        table_schema as \"DatabaseName\",\n",
    "        table_name as \"TableName\", \n",
    "        column_name as \"ColumnName\",\n",
    "        data_type as \"ColumnType\"\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema NOT IN ('pg_catalog', 'information_schema')\n",
    "    '''\n",
    "    with get_postgres_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        table_dictionary = DataFrame(results, \n",
    "                                   columns=[\"DatabaseName\", \"TableName\", \"ColumnName\", \"ColumnType\"])\n",
    "    return json.dumps(table_dictionary.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02eebf-932c-4b68-a53b-2dda83f6f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. System Prompt Setup\n",
    "db_dictionary = query_postgres_dictionary()\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an advanced data analyst for a retailer company, specializing in analyzing data from a PostgreSQL system. Your primary responsibility is to assist users by answering business-related questions using SQL queries on the PostgreSQL database. Follow these steps:\n",
    "\n",
    "1. Understanding User Requests\n",
    "   - Users provide business questions in plain English.\n",
    "   - Extract relevant data points needed to construct a meaningful response.\n",
    "\n",
    "2. Generating SQL Queries\n",
    "   - Construct an optimized PostgreSQL SQL query to retrieve the necessary data.\n",
    "   - The query must be a **single-line string** without carriage returns or line breaks.\n",
    "   - Ensure that the SQL query adheres to **PostgreSQL SQL syntax** and avoids unsupported keywords.\n",
    "   - The catalog of tables and columns to query is in the following json structure \n",
    "     {db_dictionary}\n",
    "   - Apply appropriate filtering, grouping, and ordering to enhance performance and accuracy.\n",
    "   - Don't display the SQL queries\n",
    "\n",
    "3. Executing the Query\n",
    "   - Run the SQL query on the PostgreSQL system and retrieve the results efficiently.\n",
    "\n",
    "4. Responding to the User\n",
    "   - Convert the query results into a **concise, insightful, and plain-English response**.\n",
    "   - Present the information in a clear, structured, and user-friendly manner.\n",
    "\n",
    "You have access to the following tools to answer user queries:\n",
    "- `query_postgres_database`: Use this to run SQL queries on the PostgreSQL database.\n",
    "- `query_postgres_dictionary`: Use this to fetch metadata about database tables.\n",
    "\n",
    "Always use `query_postgres_database` when the user asks for data stored in the database.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d27f1-1871-4ace-a1c9-610c5f6cdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Database Query Function\n",
    "def query_postgres_database(sql_statement):\n",
    "    try:\n",
    "        query_statement = sql_statement.strip().replace('\\n', ' ')\n",
    "        \n",
    "        with get_postgres_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query_statement)\n",
    "            \n",
    "            if cursor.description:\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                data = cursor.fetchall()\n",
    "                df = DataFrame(data, columns=columns)\n",
    "                return json.dumps(df.to_dict(orient='records'))\n",
    "            else:\n",
    "                conn.commit()\n",
    "                return json.dumps({\"rows_affected\": cursor.rowcount})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f33763-39c4-4a28-9c89-ace59429eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Agent Conversation Function\n",
    "def run_agent_conversation(user_query):\n",
    "    from llama_index.core.llms import ChatMessage\n",
    "    \n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=system_prompt),\n",
    "        ChatMessage(role=\"user\", content=user_query)\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    return str(response)  # Changed from response.content to str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5970b4-2e2e-4bdf-9fd2-d15d949ad29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Example Usage (with proper string termination)\n",
    "response = run_agent_conversation(\"What are the top 5 selling products by revenue?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c86a28-77ba-4ab2-96da-d3808522623d",
   "metadata": {},
   "source": [
    "## Agent Runtime\n",
    "This section covers the code executed while the agent is in action, including:\n",
    "* Preparing the tools for use by the agent\n",
    "* The agent's runtime function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b932dea-15a9-43a7-a87b-0e1bbed5acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Callable\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "def function_to_schema(func: Callable) -> Dict[str, Any]:\n",
    "    \"\"\"Convert a Python function to a tool schema compatible with NVIDIA LLM\n",
    "    \n",
    "    Args:\n",
    "        func: The Python function to convert\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the function schema in NVIDIA-compatible format\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(func)\n",
    "    docstring = inspect.getdoc(func) or \"\"\n",
    "    \n",
    "    # Extract parameter information\n",
    "    parameters = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},\n",
    "        \"required\": []\n",
    "    }\n",
    "    \n",
    "    for name, param in sig.parameters.items():\n",
    "        if name == \"self\":\n",
    "            continue\n",
    "            \n",
    "        param_type = \"string\"  # default type\n",
    "        if param.annotation != inspect.Parameter.empty:\n",
    "            if param.annotation == str:\n",
    "                param_type = \"string\"\n",
    "            elif param.annotation == int:\n",
    "                param_type = \"integer\"\n",
    "            elif param.annotation == float:\n",
    "                param_type = \"number\"\n",
    "            elif param.annotation == bool:\n",
    "                param_type = \"boolean\"\n",
    "        \n",
    "        parameters[\"properties\"][name] = {\n",
    "            \"type\": param_type,\n",
    "            \"description\": \"\"  # Can be enhanced with parameter-specific docs\n",
    "        }\n",
    "        \n",
    "        if param.default == inspect.Parameter.empty:\n",
    "            parameters[\"required\"].append(name)\n",
    "    \n",
    "    return {\n",
    "        \"name\": func.__name__,\n",
    "        \"description\": docstring,\n",
    "        \"parameters\": parameters\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917545b-d897-40ff-aae1-c9b4fd3adb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Prepare Tools for Agent\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "tools = [query_postgres_database]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "tools_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "def execute_tool_call(tool_call, tools_map):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"Assistant: {name}({args})\")\n",
    "\n",
    "    # call corresponding function with provided arguments\n",
    "    return tools_map[name](**args)\n",
    "\n",
    "def convert_to_chat_message(message: Dict[str, Any]) -> ChatMessage:\n",
    "    \"\"\"Convert dictionary message to LlamaIndex ChatMessage\"\"\"\n",
    "    return ChatMessage(\n",
    "        role=message[\"role\"],\n",
    "        content=message[\"content\"],\n",
    "        additional_kwargs=message.get(\"additional_kwargs\", {})\n",
    "    )\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "def run_full_turn(system_message: str, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    chat_messages = [convert_to_chat_message(msg) for msg in messages]\n",
    "    \n",
    "    while True:\n",
    "        # Initialize streaming\n",
    "        full_response = []\n",
    "        response_buffer = \"\"\n",
    "        out = display(Markdown(\"\"), display_id=True)\n",
    "        \n",
    "        # Get streaming response with token awareness\n",
    "        response_stream = llm.stream_chat(\n",
    "            chat_messages,\n",
    "            max_tokens=4096,  # Adjust based on your model's limits\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Process stream with token-aware chunking\n",
    "        for chunk in response_stream:\n",
    "            content = chunk.delta\n",
    "            if content:\n",
    "                response_buffer += content\n",
    "                full_response.append(content)\n",
    "                \n",
    "                # Display when we hit natural breaks or every 20 tokens\n",
    "                if len(response_buffer.split()) >= 20 or content.endswith(('\\n', '.', '!', '?')):\n",
    "                    out.update(Markdown(\"\".join(full_response)))\n",
    "                    response_buffer = \"\"\n",
    "                    time.sleep(0.05)  # Natural reading speed\n",
    "        \n",
    "        # Final update to ensure complete display\n",
    "        out.update(Markdown(\"\".join(full_response)))\n",
    "        \n",
    "        # Store complete response\n",
    "        response_dict = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\".join(full_response),\n",
    "            \"additional_kwargs\": getattr(response_stream, \"additional_kwargs\", {})\n",
    "        }\n",
    "        messages.append(response_dict)\n",
    "        \n",
    "        # Handle tool calls (unchanged)\n",
    "        additional_kwargs = response_dict.get(\"additional_kwargs\", {})\n",
    "        if \"tool_calls\" in additional_kwargs:\n",
    "            for tool_call in additional_kwargs[\"tool_calls\"]:\n",
    "                result = execute_tool_call(tool_call, tools_map)\n",
    "                result_message = {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": result,\n",
    "                    \"tool_call_id\": tool_call.get(\"id\", \"\"),\n",
    "                    \"name\": tool_call[\"function\"][\"name\"]\n",
    "                }\n",
    "                messages.append(result_message)\n",
    "                chat_messages.append(convert_to_chat_message(result_message))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f310e2-7e3b-42e8-bd52-13a8a57896c7",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "### Sample Questions:\n",
    "1. What are our top-selling products by revenue and quantity sold?\n",
    "2. Who are our top 10 customers by total spend and order frequency?\n",
    "3. Which products have the lowest stock levels relative to their sales velocity?\n",
    "4. Which product categories generate the highest profit margins?\n",
    "5. What is our order fulfillment rate and average time to fulfill orders?\n",
    "6. How has our customer base grown over time?\n",
    "7. What are the seasonal trends in our product categories?\n",
    "8. What products are frequently purchased together?\n",
    "9. What percentage of customers make repeat purchases?\n",
    "10. Which customer segments are most profitable when considering acquisition cost and lifetime value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e786c-fddd-45dc-93d9-e9d9bece1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated imports\n",
    "from typing import AsyncIterator, Iterator\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 10. Prepare Tools for Agent (unchanged)\n",
    "tools = [query_postgres_database]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "tools_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "# Modified agent interaction with streaming\n",
    "def run_agent_interaction():\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nUser (type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        messages = run_full_turn(system_prompt, messages)\n",
    "        \n",
    "        # Display any tool results\n",
    "        for msg in reversed(messages):\n",
    "            if msg.get(\"role\") == \"tool\" and \"content\" in msg:\n",
    "                print(f\"\\n[Database Result]: {msg['content']}\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_agent_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a302d7f-d098-421e-a434-0f4d3c52acf5",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc83a7-d466-4d30-b96c-8ea8cb87f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from time import sleep\n",
    "\n",
    "# Set NAMESPACE variable\n",
    "NAMESPACE = get_namespace_from_service_account()\n",
    "\n",
    "# Configuration - must match your deployment values\n",
    "PG_SERVICE_NAME = \"vince-retail-postgres\"\n",
    "PVC_PREFIX = \"postgres-pvc\"  # This should match the prefix used in deployment\n",
    "\n",
    "def clean_postgresql_deployment():\n",
    "    \"\"\"Clean up PostgreSQL deployment from Kubernetes\"\"\"\n",
    "    try:\n",
    "        print(\"Starting PostgreSQL cleanup...\")\n",
    "        \n",
    "        # 1. Delete the deployment\n",
    "        print(f\"Deleting deployment {PG_SERVICE_NAME}...\")\n",
    "        subprocess.run(\n",
    "            f\"kubectl delete deployment {PG_SERVICE_NAME} -n {NAMESPACE} --ignore-not-found\",\n",
    "            shell=True, check=True\n",
    "        )\n",
    "        \n",
    "        # 2. Delete the service\n",
    "        print(f\"Deleting service {PG_SERVICE_NAME}...\")\n",
    "        subprocess.run(\n",
    "            f\"kubectl delete service {PG_SERVICE_NAME} -n {NAMESPACE} --ignore-not-found\",\n",
    "            shell=True, check=True\n",
    "        )\n",
    "        \n",
    "        # 3. Wait for resources to be deleted\n",
    "        print(\"Waiting for resources to terminate...\")\n",
    "        sleep(10)  # Give Kubernetes time to clean up\n",
    "        \n",
    "        # 4. Verify deletion\n",
    "        print(\"Verifying deletion...\")\n",
    "        verify_deletion()\n",
    "        \n",
    "        print(\"\\nPostgreSQL deployment cleaned up successfully!\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during cleanup: {e.stderr.decode()}\")\n",
    "        raise\n",
    "\n",
    "def verify_deletion():\n",
    "    \"\"\"Verify that all resources have been deleted\"\"\"\n",
    "    try:\n",
    "        # Check deployment\n",
    "        result = subprocess.run(\n",
    "            f\"kubectl get deployment {PG_SERVICE_NAME} -n {NAMESPACE}\",\n",
    "            shell=True, capture_output=True, text=True\n",
    "        )\n",
    "        if \"NotFound\" not in result.stderr:\n",
    "            print(\"Warning: Deployment might still exist\")\n",
    "        \n",
    "        # Check service\n",
    "        result = subprocess.run(\n",
    "            f\"kubectl get service {PG_SERVICE_NAME} -n {NAMESPACE}\",\n",
    "            shell=True, capture_output=True, text=True\n",
    "        )\n",
    "        if \"NotFound\" not in result.stderr:\n",
    "            print(\"Warning: Service might still exist\")\n",
    "        \n",
    "        # Check pods\n",
    "        result = subprocess.run(\n",
    "            f\"kubectl get pods -n {NAMESPACE} -l app={PG_SERVICE_NAME}\",\n",
    "            shell=True, capture_output=True, text=True\n",
    "        )\n",
    "        if \"No resources found\" not in result.stdout:\n",
    "            print(\"Warning: Pods might still exist\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Verification error: {e}\")\n",
    "\n",
    "def delete_persistent_data():\n",
    "    \"\"\"Delete persistent volumes and claims\"\"\"\n",
    "    try:\n",
    "        print(\"\\nStarting persistent data cleanup...\")\n",
    "        \n",
    "        # 1. Find and delete all PVCs with the matching prefix\n",
    "        print(f\"Looking for PVCs with prefix '{PVC_PREFIX}'...\")\n",
    "        result = subprocess.run(\n",
    "            f\"kubectl get pvc -n {NAMESPACE} --no-headers -o custom-columns=':metadata.name' | grep '^{PVC_PREFIX}'\",\n",
    "            shell=True, capture_output=True, text=True\n",
    "        )\n",
    "        \n",
    "        pvcs = result.stdout.splitlines()\n",
    "        if pvcs:\n",
    "            print(f\"Found {len(pvcs)} PVC(s) to delete:\")\n",
    "            for pvc in pvcs:\n",
    "                print(f\"- {pvc}\")\n",
    "            print(\"\\nDeleting PVCs...\")\n",
    "            subprocess.run(\n",
    "                f\"kubectl delete pvc {' '.join(pvcs)} -n {NAMESPACE}\",\n",
    "                shell=True, check=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"No PVCs found matching the prefix\")\n",
    "        \n",
    "        # 2. Wait for PVCs to be deleted\n",
    "        print(\"Waiting for PVCs to be deleted...\")\n",
    "        sleep(10)\n",
    "        \n",
    "        # 3. Find and delete associated PVs (if not automatically deleted)\n",
    "        print(\"Checking for associated PVs...\")\n",
    "        result = subprocess.run(\n",
    "            f\"kubectl get pv --no-headers -o custom-columns=':metadata.name,:spec.claimRef.name' | grep {PVC_PREFIX} | awk '{{print $1}}'\",\n",
    "            shell=True, capture_output=True, text=True\n",
    "        )\n",
    "        \n",
    "        pvs = result.stdout.splitlines()\n",
    "        if pvs:\n",
    "            print(f\"Found {len(pvs)} PV(s) to delete:\")\n",
    "            for pv in pvs:\n",
    "                print(f\"- {pv}\")\n",
    "            print(\"\\nDeleting PVs...\")\n",
    "            subprocess.run(\n",
    "                f\"kubectl delete pv {' '.join(pvs)}\",\n",
    "                shell=True, check=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"No associated PVs found\")\n",
    "        \n",
    "        print(\"\\nPersistent data cleanup completed\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        if \"NotFound\" not in e.stderr.decode() and \"no matches found\" not in e.stderr.decode():\n",
    "            print(f\"Error during persistent data cleanup: {e.stderr.decode()}\")\n",
    "        else:\n",
    "            print(\"No persistent resources found to delete\")\n",
    "\n",
    "def full_cleanup():\n",
    "    \"\"\"Perform complete cleanup including persistent data\"\"\"\n",
    "    clean_postgresql_deployment()\n",
    "    delete_persistent_data()\n",
    "\n",
    "# Run the cleanup\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"PostgreSQL Kubernetes Cleanup Tool\")\n",
    "    print(f\"Namespace: {NAMESPACE}\")\n",
    "    print(f\"Resource: {PG_SERVICE_NAME}\")\n",
    "    print(f\"PVC Prefix: {PVC_PREFIX}*\")\n",
    "    \n",
    "    choice = input(\"\\nChoose cleanup option:\\n1. Basic cleanup (deployment/service)\\n2. Full cleanup (including persistent data)\\nEnter choice (1/2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        clean_postgresql_deployment()\n",
    "    elif choice == \"2\":\n",
    "        full_cleanup()\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae613a63-b596-43ab-b06c-13dc7314ed4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
